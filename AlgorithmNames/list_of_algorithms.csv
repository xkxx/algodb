3Dc,"3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 is a lossy data compression algorithm for normal maps invented and first implemented by ATI. It builds upon the earlier DXT5 algorithm and is an open standard. 3Dc is now implemented by both ATI and Nvidia.","[u'3D graphics software', u'Lossy compression algorithms', u'Open formats', u'Texture compression']","[u'ATI Technologies', u'Bump mapping', u'DXT5', u'Data compression', u'FourCC', u'Geometric', u'Lossy data compression', u'Normal mapping', u'Nvidia', u'Open standard', u'Texture mapping']"
A*,"In computer science, A* (pronounced as ""A star"" ( listen)) is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently traversable path between multiple points, called nodes. Noted for its performance and accuracy, it enjoys widespread use. However, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, although other work has found A* to be superior to other approaches.
Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first described the algorithm in 1968. It is an extension of Edsger Dijkstra's 1959 algorithm. A* achieves better performance by using heuristics to guide its search.","[u'All accuracy disputes', u'Articles with disputed statements from February 2014', u'Articles with example pseudocode', u'Combinatorial optimization', u'Game artificial intelligence', u'Graph algorithms', u'Routing algorithms', u'Search algorithms']","[u'A*', u'A* (disambiguation)', u'AI Magazine', u'A star', u'Admissible heuristic', u'Amortized time', u'Any-angle path planning', u'Artificial Intelligence: A Modern Approach', u'Association for Computing Machinery', u'Bertram Raphael', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u'Binary heap', u'Branch and bound', u'Branching factor', u'Breadth-first search', u'Computational complexity theory', u'Computer algorithm', u'Computer performance', u'Computer science', u'Consistent heuristic', u'D*', u'Depth-first search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dorothea Wagner', u'Edsger Dijkstra', u'Exponential time', u'Fibonacci heap', u'Fringe search', u'Goal node', u'Graph (data structure)', u'Graph traversal', u'Greedy algorithm', u'Hash table', u'Heuristic', u'Heuristic (computer science)', u'IDA*', u'Incremental heuristic search', u'Informed search algorithm', u'Institute of Electrical and Electronics Engineers', u'International Standard Book Number', u'Journal of the ACM', u'Jump point search', u'LIFO (computing)', u'Logarithm', u'Motion planning', u'Natural language processing', u'Nils Nilsson (researcher)', u'Node (graph theory)', u'Parsing', u'Pathfinding', u'Peter E. Hart', u'Peter Norvig', u'Peter Sanders (computer scientist)', u'Polynomial time', u'Princeton University', u'Priority queue', u'Pseudocode', u'Reduced cost', u'Robotics', u'Routing', u'SMA*', u'SRI International', u'Search algorithm', u'Shakey the Robot', u'Stochastic context-free grammar', u'Stuart J. Russell', u'Tree (data structure)']"
ALOPEX,"ALOPEX (an acronym from ""ALgorithms Of Pattern EXtraction"") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974.","[u'All stub articles', u'Artificial intelligence stubs', u'Artificial neural networks', u'Classification algorithms']","[u'Artificial intelligence', u'Backpropagation', u'Evangelia Micheli-Tzanakou', u'Machine learning']"
AdaBoost,"AdaBoost, short for ""Adaptive Boosting"", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems, however, it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (i.e., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner
While every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder to classify examples.","[u'Classification algorithms', u'Ensemble learning']","[u'Backfitting algorithm', u'Boosting (meta-algorithm)', u'Bootstrap aggregating', u'BrownBoost', u'CiteSeer', u'CoBoosting', u'Continuously differentiable function', u'Convex function', u'Convex set', u'Curse of dimensionality', u'Decision tree learning', u'Digital object identifier', u'Early stopping', u'Gradient boosting', u'Gradient descent', u'G\xf6del Prize', u'Haar-like features', u'Huber loss function', u'International Standard Book Number', u'LPBoost', u'Least squares', u'Linear programming', u'Logistic regression', u'Logit', u'LogitBoost', u'Machine learning', u'Meta-algorithm', u'Monotonic function', u'Neural network', u""Newton's method"", u'Newton-Raphson', u'Numerical instability', u'Outlier', u'Overfitting', u'Overfitting (machine learning)', u'Regression analysis', u'Robert Schapire', u'Smoothing spline', u'Support vector machine', u'Viola\u2013Jones object detection framework', u'Yoav Freund']"
Adaptive coding,"Adaptive coding refers to variants of entropy encoding methods of lossless data compression. They are particularly suited to streaming data, as they adapt to localized changes in the characteristics of the data, and don't require a first pass over the data to calculate a probability model. The cost paid for these advantages is that the encoder and decoder must be more complex to keep their states synchronized, and more computational power is needed to keep adapting the encoder/decoder state.
Almost all data compression methods involve the use of a model, a prediction of the composition of the data. When the data matches the prediction made by the model, the encoder can usually transmit the content of the data at a lower information cost, by making reference to the model. This general statement is a bit misleading as general data compression algorithm would include the popular LZW and LZ77 algorithms, which are hardly comparable to compression techniques typically called adaptive. Run length encoding and the typical JPEG compression with run length encoding and predefined Huffman codes do not transmit a model. A lot of other methods adapt their model to the current file and need to transmit it in addition to the encoded data, because both the encoder and the decoder need to use the model.
In adaptive coding, the encoder and decoder are instead equipped with a predefined meta-model about how they will alter their models in response to the actual content of the data, and otherwise start with a blank slate, meaning that no initial model needs to be transmitted. As the data is transmitted, both encoder and decoder adapt their models, so that unless the character of the data changes radically, the model becomes better-adapted to the data its handling and compresses it more efficiently approaching the efficiency of the static coding.

","[u'All articles lacking sources', u'Articles lacking sources from June 2009', u'Lossless compression algorithms']","[u'Beacon mode service', u'Binary Golay code', u'CCSDS 122.0-B-1', u'Cassini-Huygens', u'Command Loss Timer Reset', u'Concatenated error correction code', u'Consultative Committee for Space Data Systems', u'Data compression', u'Entropy encoding', u'GMSK', u'ICER', u'JPEG', u'JPEG 2000', u'K band', u'Ka band', u'Ku band', u'LZ77', u'LZW', u'Lossless data compression', u'Low-density parity-check code', u'Message Abstraction Layer', u'OQPSK', u'Performance Enhancing Proxy', u'Phase-shift keying', u'Proximity-1 Space Link Protocol', u'QPSK', u'Run length encoding', u'S band', u'Saturn', u'Service-oriented architecture', u'Solid-state drive', u'Space Communications Protocol Specifications', u'Spacecraft Monitoring & Control', u'Turbo code', u'X band']"
Adaptive replacement cache,"Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance than LRU (Least Recently Used) developed at the IBM Almaden Research Center. This is accomplished by keeping track of both Frequently Used and Recently Used pages plus a recent eviction history for both. In 2006, IBM was granted a patent for the adaptive replacement cache policy.

","[u'Memory management algorithms', u'Use dmy dates from August 2012', u'Virtual memory']","[u'Almaden Research Center', u'Cache algorithms', u'Clock with Adaptive Replacement', u'Dharmendra Modha', u'Nimrod Megiddo', u'Page replacement algorithm', u'PostgreSQL', u'Solaris Operating System', u'Sun Microsystems', u'ZFS']"
Addition-chain exponentiation,"In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. It works by creating the shortest addition chain that generates the desired exponent. Each exponentiation in the chain can be evaluated by multiplying two of the earlier exponentiation results. More generally, addition-chain exponentiation may also refer to exponentiation by non-minimal addition chains constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).
The shortest addition-chain algorithm requires no more multiplications than binary exponentiation and usually less. The first example of where it does better is for a15, where the binary method needs six multiplications but a shortest addition chain requires only five:
 (binary, 6 multiplications)
 (shortest addition chain, 5 multiplications).
On the other hand, the determination of a shortest addition chain is hard: no efficient optimal methods are currently known for arbitrary exponents, and the related problem of finding a shortest addition chain for a given set of exponents has been proven NP-complete. Even given a shortest chain, addition-chain exponentiation requires more memory than the binary method, because it must potentially store many previous exponents from the chain. So in practice, shortest addition-chain exponentiation is primarily used for small fixed exponents for which a shortest chain can be precomputed and is not too large.
There are also several methods to approximate a shortest addition chain, and which often require fewer multiplications than binary exponentiation; binary exponentiation itself is a suboptimal addition-chain algorithm. The optimal algorithm choice depends on the context (such as the relative cost of the multiplication and the number of times a given exponent is re-used).
The problem of finding the shortest addition chain cannot be solved by dynamic programming, because it does not satisfy the assumption of optimal substructure. That is, it is not sufficient to decompose the power into smaller powers, each of which is computed minimally, since the addition chains for the smaller powers may be related (to share computations). For example, in the shortest addition chain for a15 above, the subproblem for a6 must be computed as (a3)2 since a3 is re-used (as opposed to, say, a6 = a2(a2)2, which also requires three multiplies).","[u'Addition chains', u'Computer arithmetic algorithms', u'Exponentials']","[u'Addition-subtraction chain', u'Addition chain', u'Algorithm', u'Binary exponentiation', u'Computer science', u'Digital object identifier', u'Donald E. Knuth', u'Dynamic programming', u'Elliptic curve', u'Exponentiation', u'Integer', u'Mathematics', u'NP-complete', u'Negative number', u'Optimal substructure']"
Adler-32,"Adler-32 is a checksum algorithm which was invented by Mark Adler in 1995, and is a modification of the Fletcher checksum. Compared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). Adler-32 is more reliable than Fletcher-16, and slightly less reliable than Fletcher-32.",[u'Checksum algorithms'],"[u'16-bit', u'ASCII', u'Algorithm', u'Byte', u'CRC-32', u'CRC32', u'C (programming language)', u'Checksum', u'Composite number', u'Cyclic redundancy check', u'Endianness', u""Fletcher's checksum"", u'Fletcher-16', u'Fletcher-32', u'Hexadecimal', u'Hierarchical Data Format', u'List of hash functions', u'Mark Adler', u'Modular arithmetic', u'Prime number', u'Rolling checksum', u'Rsync', u'Stream Control Transmission Protocol', u'Word (data type)', u'Zlib']"
Aho–Corasick string matching algorithm,"In computer science, the Aho–Corasick algorithm is a string searching algorithm invented by Alfred V. Aho and Margaret J. Corasick. It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the ""dictionary"") within an input text. It matches all patterns simultaneously. The complexity of the algorithm is linear in the length of the patterns plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a, aa, aaa, aaaa and input string is aaaa).
Informally, the algorithm constructs a finite state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed pattern matches (e.g. a search for cat in a trie that does not contain cat, but contains cart, and thus would fail at the node prefixed by ca), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between pattern matches without the need for backtracking.
When the pattern dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries.
The Aho–Corasick string matching algorithm formed the basis of the original Unix command fgrep.","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from February 2013', u'String matching algorithms']","[u'Alfred Aho', u'Alfred V. Aho', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computational complexity theory', u'Computer science', u'Computer virus', u'Damerau\u2013Levenshtein distance', u'Deterministic acyclic finite state automaton', u'Dictionary of Algorithms and Data Structures', u'Digital object identifier', u'Directed acyclic word graph', u'Edit distance', u'Finite state machine', u'Generalized suffix tree', u'Grep', u'Hamming distance', u""Hirschberg's algorithm"", u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of Unix programs', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Parsing', u'Pattern matching', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Regular expression', u'Regular tree grammar', u'Rope (data structure)', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Wagner\u2013Fischer algorithm']"
Algorithm X,"""Algorithm X"" is the name Donald Knuth used in his paper ""Dancing Links"" to refer to ""the most obvious trial-and-error approach"" for finding all solutions to the exact cover problem. Technically, Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm. While Algorithm X is generally useful as a succinct explanation of how the exact cover problem may be solved, Knuth's intent in presenting it was merely to demonstrate the utility of the dancing links technique via an efficient implementation he called DLX.
The exact cover problem is represented in Algorithm X using a matrix A consisting of 0s and 1s. The goal is to select a subset of the rows so that the digit 1 appears in each column exactly once.
Algorithm X functions as follows:

The nondeterministic choice of r means that the algorithm essentially clones itself into independent subalgorithms; each subalgorithm inherits the current matrix A, but reduces it with respect to a different row r. If column c is entirely zero, there are no subalgorithms and the process terminates unsuccessfully.
The subalgorithms form a search tree in a natural way, with the original problem at the root and with level k containing each subalgorithm that corresponds to k chosen rows. Backtracking is the process of traversing the tree in preorder, depth first.
Any systematic rule for choosing column c in this procedure will find all solutions, but some rules work much better than others. To reduce the number of iterations, Knuth suggests that the column choosing algorithm select a column with the lowest number of 1s in it.","[u'Donald Knuth', u'Search algorithms']","[u'-yllion', u'AMS Euler', u'Algorithm', u'Algorithm X', u'ArXiv', u'Backtracking', u'CWEB', u'Computer Modern', u'Computers and Typesetting', u'Concrete Mathematics', u'Concrete Roman', u'Dancing Links', u'Dancing links', u'Depth-first', u'Deterministic algorithm', u""Dijkstra's algorithm"", u'Donald Knuth', u'Doubly linked list', u'Exact cover', u'Fisher\u2013Yates shuffle', u'Font', u'GNU MIX Development Kit', u'International Standard Book Number', u""Knuth's Simpath algorithm"", u'Knuth Prize', u'Knuth reward check', u'Knuth\u2013Bendix completion algorithm', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Literate programming', u'METAFONT', u'MIX', u'MMIX', u'Man or boy test', u'Nondeterministic algorithm', u'Potrzebie', u'Quater-imaginary base', u'Recursion (computer science)', u'Robinson\u2013Schensted\u2013Knuth correspondence', u'Search tree', u'Selected papers series of Knuth', u'Software', u'Surreal Numbers (book)', u'TeX', u'The Art of Computer Programming', u'The Complexity of Songs', u'Things a Computer Scientist Rarely Talks About', u'Trabb Pardo\u2013Knuth algorithm', u'WEB']"
Algorithms for Recovery and Isolation Exploiting Semantics,"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems.
Three main principles lie behind ARIES
Write ahead logging: Any change to an object is first recorded in the log, and the log must be written to stable storage before changes to the object are written to disk.
Repeating history during Redo: On restart after a crash, ARIES retraces the actions of a database before the crash and brings the system back to the exact state that it was in before the crash. Then it undoes the transactions still active at crash time.
Logging changes during Undo: Changes made to the database while undoing transactions are logged to ensure such an action isn't repeated in the event of repeated restarts.","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from March 2013', u'Database algorithms']","[u'Algorithm', u'Computer science', u'Database log', u'Database system', u'IBM DB2', u'Logging changes during Undo', u'Microsoft SQL Server', u'No-force', u'Repeating history during Redo', u'Write ahead logging']"
Algorithms for calculating variance,"Algorithms for calculating variance play a major role in computational statistics. A key problem in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values.","[u'Articles with example Python code', u'Articles with example pseudocode', u'Statistical algorithms', u'Statistical deviation and dispersion']","[u'Algebraic formula for the variance', u'Algorithm', u'Algorithms for calculating variance', u'Arithmetic overflow', u'Assumed mean', u""Bessel's correction"", u'Catastrophic cancellation', u'Central moment', u'Communications of the ACM', u'Compensated summation', u'Computational statistics', u'Covariance', u'Digital object identifier', u'Donald E. Knuth', u'Eric W. Weisstein', u'Estimator bias', u'Floating-point', u'Gene H. Golub', u'IEEE 754', u'International Standard Book Number', u'Invariant (mathematics)', u'Kahan summation algorithm', u'Kurtosis', u'Location parameter', u'Loss of significance', u'MathWorld', u'Mean', u'Numerical instability', u'One-pass algorithm', u'Online algorithm', u'Precision (arithmetic)', u'Python (programming language)', u'Recurrence relation', u'Skewness', u'Statistical population', u'Statistical sample', u'Technometrics', u'The Art of Computer Programming', u'Tony F. Chan', u'Variance']"
Alpha-beta pruning,"Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.","[u'All articles with dead external links', u'Articles with dead external links from September 2010', u'Articles with example pseudocode', u'Articles with inconsistent citation formats', u'CS1 errors: dates', u'Game artificial intelligence', u'Graph algorithms', u'Optimization algorithms and methods', u'Search algorithms']","[u'A* search algorithm', u'Alan Kotok', u'Albert W. Tucker', u'Alexander Brudno', u'All-pay auction', u'Allen Newell', u'Alphabeta (disambiguation)', u'Amos Tversky', u'Ariel Rubinstein', u""Arrow's impossibility theorem"", u'Arthur Samuel', u'B*', u'Backtracking', u'Backward induction', u'Bargaining problem', u'Battle of the sexes (game theory)', u'Bayesian game', u'Beam search', u'Bellman\u2013Ford algorithm', u'Bertrand paradox (economics)', u'Best-first search', u'Best first search', u'Bidirectional search', u'Big O notation', u'Blotto games', u""Bor\u016fvka's algorithm"", u'Bounded rationality', u'Branch and bound', u'Branching factor', u'Breadth-first search', u'British Museum algorithm', u'Centipede game', u'Cheap talk', u'Chess', u'Chicken (game)', u'Collusion', u'Combinatorial game theory', u'Combinatorial optimization', u'Confrontation analysis', u'Cooperative game', u'Coopetition', u'Coordination game', u'Core (game theory)', u'Correlated equilibrium', u'Cournot competition', u'D*', u'Daniel Kahneman', u'Dartmouth Conference', u'David K. Levine', u'David M. Kreps', u'Deadlock (game theory)', u'Depth-first search', u'Depth-limited search', u'Dictator game', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dollar auction', u'Dominance (game theory)', u'Donald B. Gillies', u'Donald Knuth', u'Drew Fudenberg', u'Dynamic programming', u'Economic equilibrium', u""Edmonds' algorithm"", u'El Farol Bar problem', u'Epsilon-equilibrium', u'Eric Maskin', u'Escalation of commitment', u'Evolutionarily stable strategy', u'Extensive-form game', u'Fair cake-cutting', u'Fair division', u'Floyd\u2013Warshall algorithm', u'Folk theorem (game theory)', u'Forward induction', u'Fringe search', u'Game theory', u'Game tree', u'Global games', u'Go (board game)', u'Graph traversal', u'Graphical game theory', u'Grim trigger', u'Guess 2/3 of the average', u'Harold W. Kuhn', u'Herbert A. Simon', u'Herv\xe9 Moulin', u'Heuristic', u'Hierarchy of beliefs', u'Hill climbing', u'Infinity', u'Information set (game theory)', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Jean-Fran\xe7ois Mertens', u'Jean Tirole', u'John Forbes Nash, Jr.', u'John Harsanyi', u'John Maynard Smith', u'John McCarthy (computer scientist)', u'John von Neumann', u""Johnson's algorithm"", u'Judea Pearl', u'Jump point search', u'Kenneth Arrow', u'Kenneth Binmore', u'Killer heuristic', u""Kruskal's algorithm"", u'Kuhn poker', u'Large Poisson game', u'Leonid Hurwicz', u'Lexicographic breadth-first search', u'List of algorithms', u'List of game theorists', u'List of games in game theory', u'Lloyd Shapley', u'MTD(f)', u'MTD-f', u'Markov perfect equilibrium', u'Markov strategy', u'Matching pennies', u'Mechanism design', u'Melvin Dresher', u'Merrill M. Flood', u'Mertens-stable equilibrium', u'Minimax', u'Monty Hall problem', u'N-player game', u'Nash bargaining game', u'Nash equilibrium', u'Negamax', u'Negascout', u'No-win situation', u'Nontransitive game', u'Normal-form game', u'OCLC', u'Oreilly Media', u'Oskar Morgenstern', u'Pareto efficiency', u'Paul Milgrom', u'Perfect information', u'Peter Norvig', u'Peyton Young', u'Pirate game', u'Ply (game theory)', u'Preference (economics)', u""Prim's algorithm"", u'Princess and monster game', u'Principal variation search', u""Prisoner's dilemma"", u'Prisoners and hats puzzle', u'Proper equilibrium', u'Pruning (algorithm)', u'Public goods game', u'Purification theorem', u'Quantal response equilibrium', u'Quasi-perfect equilibrium', u'Refutation table', u'Reinhard Selten', u'Rendezvous problem', u'Repeated game', u'Revelation principle', u'Risk dominance', u'Robert Aumann', u'Robert B. Wilson', u'Rock-paper-scissors', u'Roger Myerson', u'SMA*', u'SSS*', u'Samuel Bowles (economist)', u'Screening game', u'Search algorithm', u'Search game', u'Self-confirming equilibrium', u'Sequential equilibrium', u'Sequential game', u'Shapley value', u'Signaling game', u'Simultaneous game', u'Solution concept', u'Square root', u'Stag hunt', u'Stochastic game', u'Strategy (game theory)', u'Strictly determined game', u'Strong Nash equilibrium', u'Stuart J. Russell', u'Subgame perfect equilibrium', u'Succinct game', u'Symmetric game', u'Thomas Schelling', u'Tic-tac-toe', u'Tit for tat', u'Tragedy of the commons', u'Transposition table', u""Traveler's dilemma"", u'Tree traversal', u'Trembling hand perfect equilibrium', u'Tyranny of small decisions', u'UMI Research Press', u'Ultimatum game', u'United States', u""Unscrupulous diner's dilemma"", u'Variation (game tree)', u""Volunteer's dilemma"", u'War of attrition (game)', u'William Vickrey', u'Zero-sum game']"
Alpha max plus beta min algorithm,"The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares. The square root of the sum of two squares, also known as Pythagorean addition, is a useful function, because it finds the hypotenuse of a right triangle given the two side lengths, the norm of a 2-D vector, or the magnitude of a complex number z=a+bi given the real and imaginary parts.

The algorithm avoids performing the square and square-root operations, instead using simple operations such as comparison, multiplication, and addition. Some choices of the α and β parameters of the algorithm allow the multiplication operation to be reduced to a simple shift of binary digits that is particularly well suited to implementation in high-speed digital circuitry.
The approximation is expressed as:

Where  is the maximum absolute value of a and b and  is the minimum absolute value of a and b.
For the closest approximation, the optimum values for  and  are  and , giving a maximum error of 3.96%.","[u'All articles with links needing disambiguation', u'Approximation algorithms', u'Articles with links needing disambiguation from November 2014', u'Root-finding algorithms']","[u'Alpha\u2013beta pruning', u'Complex number', u'Hypot', u'Hypotenuse', u'Imaginary number', u'Magnitude (mathematics)', u'Minimax', u'Norm (mathematics)', u'Pythagorean addition', u'Real number', u'Richard G. Lyons', u'Square root', u'Vector (geometric)']"
Approximate counting algorithm,"The approximate counting algorithm allows the counting of a large number of events using a small amount of memory. Invented in 1977 by Robert Morris (cryptographer) of Bell Labs, it uses probabilistic techniques to increment the counter. It was fully analyzed in the early 1980s by Philippe Flajolet of INRIA Rocquencourt, who coined the name Approximate Counting, and strongly contributed to its recognition among the research community. The algorithm is considered one of the precursors of streaming algorithms, and the more general problem of determining the frequency moments of a data stream has been central to the field.",[u'Randomized algorithms'],"[u'Artificial intelligence', u'Bell Labs', u'Counter (digital)', u'Data compression', u'Exponent', u'INRIA', u'Order of magnitude', u'Philippe Flajolet', u'Powers of two', u'Pseudo-random', u'Randomized algorithm', u'Robert Morris (cryptographer)', u'Unbiased estimator']"
Apriori algorithm,Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.,"[u'Articles with example pseudocode', u'Data mining algorithms']","[u'Association rule learning', u'Association rules', u'Breadth-first search', u'Database', u'Databases', u'Downward closure lemma', u'ELKI', u'Hash tree (persistent data structure)', u'MIT license', u'Market basket analysis', u'Max-Miner', u'Orange (software)', u'R (programming language)', u'Stock-keeping unit', u'Winepi']"
Astronomical algorithms,"Astronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation). Examples of large and complex astronomical algorithms are those used to calculate the position of the Moon. A simple example is the calculation of the Julian day.
Numerical model of solar system discusses a generalized approach to local astronomical modeling. The variations séculaires des orbites planétaires describes an often used model.

","[u'Algorithms and data structures stubs', u'All articles lacking sources', u'All stub articles', u'Articles lacking sources from April 2010', u'Astrodynamics', u'Astronomy stubs', u'Calendar algorithms', u'Computational physics', u'Computer science stubs']","[u'Algorithm', u'Astrodynamics', u'Astronomy', u'Calendar', u'Celestial mechanics', u'Celestial navigation', u'Charge-coupled device', u'Data structure', u'Doomsday rule', u'Ephemeris', u'Jean Meeus', u'Julian day', u'List of algorithms', u'List of astronomical objects', u'Moon', u'Numerical model of solar system', u'Satellite navigation', u'Transformation from spherical coordinates to rectangular coordinates', u'Variations s\xe9culaires des orbites plan\xe9taires']"
B*,"In computer science, B* (pronounced ""B star"") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals). First published by Hans Berliner in 1979, it is related to the A* search algorithm.","[u'Combinatorial optimization', u'Game artificial intelligence', u'Graph algorithms', u'Routing algorithms', u'Search algorithms']","[u'A* search algorithm', u'Alpha-beta pruning', u'Alpha\u2013beta pruning', u'Artificial Intelligence: A Modern Approach', u'Artificial Intelligence (journal)', u'B*-tree', u'B-Tree', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Computer science', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Goal node', u'Graph search algorithm', u'Graph traversal', u'Hans Berliner', u'Hill climbing', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Maven (Scrabble)', u'Node (graph theory)', u""Prim's algorithm"", u'SMA*', u'Search game', u'Tree (graph theory)', u'Tree traversal']"
BFGS method,"In numerical optimization, the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems.
The BFGS method approximates Newton's method, a class of hill-climbing optimization techniques that seeks a stationary point of a (preferably twice continuously differentiable) function. For such problems, a necessary condition for optimality is that the gradient be zero. Newton's method and the BFGS methods are not guaranteed to converge unless the function has a quadratic Taylor expansion near an optimum. These methods use both the first and second derivatives of the function. However, BFGS has proven to have good performance even for non-smooth optimizations.
In quasi-Newton methods, the Hessian matrix of second derivatives doesn't need to be evaluated directly. Instead, the Hessian matrix is approximated using rank-one updates specified by gradient evaluations (or approximate gradient evaluations). Quasi-Newton methods are generalizations of the secant method to find the root of the first derivative for multidimensional problems. In multi-dimensional problems, the secant equation does not specify a unique solution, and quasi-Newton methods differ in how they constrain the solution. The BFGS method is one of the most popular members of this class. Also in common use is L-BFGS, which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B variant handles simple box constraints.",[u'Optimization algorithms and methods'],"[u'Approximation algorithm', u'Approximation theory', u'Augmented Lagrangian method', u'BHHH algorithm', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u""Broyden's method"", u'Charles George Broyden', u'Claude Lemar\xe9chal', u'Combinatorial optimization', u'Comparison of optimization software', u'Confidence interval', u'Convex minimization', u'Convex optimization', u'Credible interval', u'Criss-cross algorithm', u'Cutting-plane method', u'David G. Luenberger', u'Davidon\u2013Fletcher\u2013Powell formula', u'Derivative', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Donald Goldfarb', u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Eigen (C++ library)', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'GNU Scientific Library', u'Gauss\u2013Newton algorithm', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Gretl', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'International Standard Book Number', u'Iterative method', u'John Wiley & Sons', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u'Kuhn\u2013Tucker conditions', u'L-BFGS', u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local optimum', u'Local search (optimization)', u'MIT License', u'Mathematical Reviews', u'Mathematical optimization', u'Matrix inverse', u'Matroid', u'Metaheuristic', u'Minimum spanning tree', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear optimization', u'Nonlinear programming', u'Numerical analysis', u'Optimization (mathematics)', u'Optimization Toolbox', u'Optimization algorithm', u'Pattern search (optimization)', u'Penalty method', u""Powell's method"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Quasi-Newton methods', u'Revised simplex algorithm', u'SciPy', u'Secant method', u'Sequential quadratic programming', u'Sherman\u2013Morrison formula', u'Simplex algorithm', u'Simulated annealing', u'Springer-Verlag', u'Stationary point', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Taylor expansion', u'Truncated Newton method', u'Trust region', u'Wolfe conditions', u'Yinyu Ye']"
Baby-step giant-step,"In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm computing the discrete logarithm. The discrete log problem is of fundamental importance to the area of public key cryptography. Many of the most commonly used cryptography systems are based on the assumption that the discrete log is extremely difficult to compute; the more difficult it is, the more security it provides a data transfer. One way to increase the difficulty of the discrete log problem is to base the cryptosystem on a larger group.","[u'Articles with example C code', u'Group theory', u'Number theoretic algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Baillie\u2013PSW primality test', u'Big O notation', u'Binary GCD algorithm', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cyclic group', u'Daniel Shanks', u'Deterministic algorithm', u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'GNU MP', u'General number field sieve', u'Generating primes', u'Generating set of a group', u'Greatest common divisor', u'Group theory', u'Hash table', u'Index calculus algorithm', u'Integer factorization', u'Integer relation algorithm', u'Integer square root', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Meet-in-the-middle attack', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u'Public key cryptography', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Space-time tradeoff', u'Special number field sieve', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Backtracking,"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c (""backtracks"") as soon as it determines that c cannot possibly be completed to a valid solution.
The classic textbook example of the use of backtracking is the eight queens puzzle, that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned.
Backtracking can be applied only for problems which admit the concept of a ""partial candidate solution"" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate a large number of candidates with a single test.
Backtracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient (if not the most efficient) technique for parsing, for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog.
Backtracking depends on user-given ""black box procedures"" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm – although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time.
The term ""backtrack"" was coined by American mathematician D. H. Lehmer in the 1950s. The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility.","[u'All articles with unsourced statements', u'Articles with unsourced statements from January 2011', u'Operations research', u'Pages with URL errors', u'Pattern matching', u'Search algorithms']","[u'A* search algorithm', u'Algorithm', u'Algorithmics of sudoku', u'Alpha\u2013beta pruning', u'Amsterdam', u""Ariadne's thread (logic)"", u'B*', u'Backjumping', u'Backtracking line search', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u'Boolean-valued function', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Brute force search', u'Central processing unit', u'Chess', u'Chessboard', u'Combinatorial optimization', u'Computational problem', u'Constraint propagation', u'Constraint satisfaction problem', u'Crosswords', u'D*', u'Depth-first search', u'Depth-limited search', u'Derrick Henry Lehmer', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Eight queens puzzle', u'Elsevier', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph traversal', u'Hill climbing', u'Icon programming language', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u'Knapsack problem', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'Line search', u'List of algorithms', u'Logic programming', u'Logical conjunction', u'Mathematical optimization', u'Metaheuristic', u'Parsing', u'Peg solitaire', u'Planner programming language', u""Prim's algorithm"", u'Procedural parameter', u'Prolog', u'Puzzle', u'Queen (chess)', u'Recursion (computer science)', u'SMA*', u'SNOBOL', u'Search game', u'The Art of Computer Programming', u'Timestamp', u'Tree structure', u'Tree traversal', u'Variable trail', u'Verbal arithmetic']"
Bailey–Borwein–Plouffe formula,"The Bailey–Borwein–Plouffe formula (BBP formula) is a spigot algorithm for computing the nth binary digit of pi (symbol: π) using base 16 math. The formula can directly calculate the value of any given digit of π without calculating the preceding digits. The BBP is a summation-style formula that was discovered in 1995 by Simon Plouffe and was named after the authors of the paper in which the formula was published, David H. Bailey, Peter Borwein, and Simon Plouffe. Before that paper, it had been published by Plouffe on his own site. The formula is
.
The discovery of this formula came as a surprise. For centuries it had been assumed that there was no way to compute the nth digit of π without calculating all of the preceding n − 1 digits.
Since this discovery, many formulas for other irrational constants have been discovered of the general form

where α is the constant and p and q are polynomials in integer coefficients and b ≥ 2 is an integer base.
Formulas in this form are known as BBP-type formulas. Certain combinations of specific p, q and b result in well-known constants, but there is no systematic algorithm for finding the appropriate combinations; known formulas are discovered through experimental mathematics.","[u'All articles needing additional references', u'Articles needing additional references from March 2014', u'Pi algorithms']","[u""Ap\xe9ry's constant"", u'ArXiv', u""Bellard's formula"", u'Binary digit', u'Carry (arithmetic)', u""Catalan's constant"", u'Computing \u03c0', u'David H. Bailey', u'Digital object identifier', u'Eric W. Weisstein', u'Experimental mathematics', u'Feynman point', u'Helaman Ferguson', u'Hexadecimal', u'Infinity', u'Integer relation algorithm', u'Inverse tangent', u'Long multiplication', u'MathWorld', u'Mathematical Intelligencer', u'Mathematical Reviews', u'Mathematics of Computation', u'Modular arithmetic', u'Modular exponentiation', u'Peter Borwein', u'Pi', u'Polylogarithm ladder', u'Polynomial', u'Radix', u'Richard J. Lipton', u'Riemann zeta function', u'Series (mathematics)', u'Simon Plouffe', u'Spigot algorithm', u'Summation', u'Time complexity']"
Banker's algorithm,"The Banker's algorithm, sometimes referred to as the avoidance algorithm, is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an ""s-state"" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue.
The algorithm was developed in the design process for the THE operating system and originally described (in Dutch) in EWD108. When a new process enters a system, it must declare the maximum number of instances of each resource type that it may ever claim; clearly, that number may not exceed the total number of resources in the system. Also, when a process gets all its requested resources it must return them in a finite amount of time.","[u'All articles with unsourced statements', u'Articles with example pseudocode', u'Articles with unsourced statements from October 2015', u'Concurrency control algorithms', u'Dutch inventions']","[u'Algorithm', u'Deadlock', u'Deadly embrace', u'Dutch language', u'Edsger Dijkstra', u'Edsger W. Dijkstra', u'Interface (computer science)', u'Memory (computers)', u'Operating system', u'Resource (computer science)', u'Resource allocation', u'Semaphore (programming)', u'THE (operating system)', u'University of Texas at Austin']"
Basic Local Alignment Search Tool,"In bioinformatics, BLAST for Basic Local Alignment Search Tool is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. A BLAST search enables a researcher to compare a query sequence with a library or database of sequences, and identify library sequences that resemble the query sequence above a certain threshold.
Different types of BLASTs are available according to the query sequences. For example, following the discovery of a previously unknown gene in the mouse, a scientist will typically perform a BLAST search of the human genome to see if humans carry a similar gene; BLAST will identify sequences in the human genome that resemble the mouse gene based on similarity of sequence. The BLAST algorithm and program were designed by Stephen Altschul, Warren Gish, Webb Miller, Eugene Myers, and David J. Lipman at the National Institutes of Health and was published in the Journal of Molecular Biology in 1990 and cited over 50,000 times.","[u'All articles with unsourced statements', u'Articles with unsourced statements from August 2012', u'Bioinformatics algorithms', u'Bioinformatics software', u'Computational phylogenetics', u'Laboratory software', u'Public domain software']","[u'AIX operating system', u'Algorithm', u'Amino acid', u'Apple Macintosh', u'BLASTZ', u'BLAT (bioinformatics)', u'BLOSUM62', u'BWT', u'Bacteria', u'Barcode of Life Data Systems', u'Basic Local Alignment Search Tool', u'Bioinformatics', u'Blast (disambiguation)', u'Burrows-Wheeler Aligner', u'CLC bio', u'CS-BLAST', u'CUDA', u'Computational phylogenetics', u'Computer program', u'DNA Data Bank of Japan', u'DNA sequence', u'Database', u'David J. Lipman', u'David Lipman', u'Digital object identifier', u'ETBLAST', u'Ensembl', u'Eugene Koonin', u'Eugene Myers', u'European Bioinformatics Institute', u'European Nucleotide Archive', u'ExPASy', u'FASTA', u'FASTA format', u'FPGA', u'Field-programmable gate array', u'FlyBase', u'Formatdb', u'GNU/Linux', u'GenBank', u'Genbank', u'Gene Myers', u'Gene Ontology', u'HMMER', u'HTML', u'Heuristic', u'Heuristic algorithm', u'Hidden Markov Models', u'Human genome', u'InterPro', u'International Standard Book Number', u'Journal of Molecular Biology', u'K-mer', u'Linux', u'List of biological databases', u'List of software categories', u'MS-Windows', u'Mac OS X', u'Makeblastdb', u'Message Passing Interface', u'Microsoft Windows', u'Mitrionics', u'Molecular phylogenetics', u'Mus musculus', u'National Center for Biotechnology Information', u'National Institute of Genetics', u'National Institutes of Health', u'Needleman-Wunsch algorithm', u'Nucleotide', u'Operating system', u'PHI-base', u'PSI Protein Classifier', u'PatternHunter', u'Pennsylvania State University', u'Pfam', u'Phylogenetic tree', u'Plain text', u'Primary structure', u'Protein', u'Protein Data Bank', u'Protein Information Resource', u'Pthreads', u'PubMed Central', u'PubMed Identifier', u'Public domain', u'Reading frame', u'SIMD', u'Saccharomyces Genome Database', u'SciEngines GmbH', u'Sequence alignment', u'Sequence alignment software', u'Sequence database', u'Sequencing', u'Sequerome', u'Short Oligonucleotide Analysis Package', u'Smith-Waterman algorithm', u'Smith\u2013Waterman algorithm', u'Software developer', u'Software license', u'Software release life cycle', u'Solaris (operating system)', u'Species', u'Stephen Altschul', u'Structural motif', u'Swiss-Prot', u'Swiss Institute of Bioinformatics', u'TBLASTx', u'The Arabidopsis Information Resource', u'TrEMBL', u'UNIX', u'UniProt', u'University of Arizona', u'VectorBase', u'Warren Gish', u'Webb Miller', u'WormBase', u'XML', u'Zebrafish Information Network']"
Baum–Welch algorithm,"In electrical engineering, computer science, statistical computing and bioinformatics, the Baum–Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm and is named for Leonard E. Baum and Lloyd R. Welch.","[u'Bioinformatics algorithms', u'Markov models', u'Pages using citations with accessdate and no URL', u'Statistical algorithms']","[u'Base-pairs', u'Bioinformatics', u'C Sharp (programming language)', u'Coding theory', u'Computer science', u'Copy-number variation', u'Cryptanalysis', u'DNA microarray', u'Digital object identifier', u'EM algorithm', u'Electrical engineering', u'Eukaryotic', u'Exon', u'Forward-backward algorithm', u'GENSCAN', u'GLIMMER', u'Genomics', u'Hidden Markov Model', u'Hidden Markov model', u'Introns', u'Isochore (genetics)', u'James K. Baker', u'Java (programming language)', u'Leonard E. Baum', u'Lloyd R. Welch', u'Locus (genetics)', u'MATLAB', u'Maximum Likelihood', u'Maximum likelihood', u'Mendelian inheritance', u'Phonemes', u'Prokaryotic', u'PubMed Central', u'PubMed Identifier', u'Python (programming language)', u'R (programming language)', u'Specificity (statistics)', u'Speech Recognition', u'Speech processing', u'Statistical computing', u'Structural variations', u'Viterbi algorithm']"
Bead sort,"Bead sort is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science. Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers. Also, it would seem that even in the best case, the algorithm requires O(n2) space.

",[u'Sorting algorithms'],"[u'Abacus', u'Adaptive sort', u'American flag sort', u'Analog computer', u'Batcher odd\u2013even mergesort', u'Big O Notation', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cristian S. Calude', u'Cycle sort', u'Digital data', u'Digital hardware', u'European Association for Theoretical Computer Science', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'Implementation', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Joshua J. Arulanandham', u'Kibibyte', u'Library sort', u'List (computing)', u'Logarithm', u'Merge sort', u'Michael J. Dinneen', u'Natural algorithm', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Positive integer', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Software', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Beam search,"In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic which attempts to predict how close a partial solution is to a complete solution (goal state). But in beam search, only a predetermined number of best partial solutions are kept as candidates.
Beam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost. However, it only stores a predetermined number of best states at each level (called the beam width). Only those states are expanded next. The greater the beam width, the fewer states are pruned. With an infinite beam width, no states are pruned and beam search is identical to breadth-first search. The beam width bounds the memory required to perform the search. Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists). Beam search is not optimal (that is, there is no guarantee that it will find the best solution). It returns the first solution found.
The beam width can either be fixed or variable. One approach that uses a variable beam width starts with the width at a minimum. If no solution is found, the beam is widened and the procedure is repeated.",[u'Search algorithms'],"[u'A* search algorithm', u'Alpha\u2013beta pruning', u'Anytime algorithm', u'B*', u'Backtracking', u'Beam stack search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Carnegie Mellon University', u'CiteSeer', u'Completeness (logic)', u'Computer science', u'D*', u'Depth-first search', u'Depth-limited search', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph traversal', u'Heuristic (computer science)', u'Hill climbing', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Machine translation', u""Prim's algorithm"", u'Raj Reddy', u'SMA*', u'Search algorithm', u'Search game', u'Tree traversal']"
Beam stack search,"Beam Stack Search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to Depth-First Beam Search. Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution.

","[u'Algorithms and data structures stubs', u'All stub articles', u'Computer science stubs', u'Search algorithms']","[u'Algorithm', u'Anytime algorithm', u'Backtracking', u'Beam search', u'CiteSeer', u'Data structure', u'Depth-first search', u'Divide and conquer algorithm', u'Search algorithm']"
Beam tracing,"Beam tracing is an algorithm to simulate wave propagation. It was developed in the context of computer graphics to render 3D scenes, but it has been also used in other similar areas such as acoustics and electromagnetism simulations.
Beam tracing is a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with beams. Beams are shaped like unbounded pyramids, with (possibly complex) polygonal cross sections. Beam tracing was first proposed by Paul Heckbert and Pat Hanrahan.
In beam tracing, a pyramidal beam is initially cast through the entire viewing frustum. This initial viewing beam is intersected with each polygon in the environment, typically from nearest to farthest. Each polygon that intersects with the beam must be visible, and is removed from the shape of the beam and added to a render queue. When a beam intersects with a reflective or refractive polygon, a new beam is created in a similar fashion to ray-tracing.
A variant of beam tracing casts a pyramidal beam through each pixel of the image plane. This is then split up into sub-beams based on its intersection with scene geometry. Reflection and transmission (refraction) rays are also replaced by beams.This sort of implementation is rarely used, as the geometric processes involved are much more complex and therefore expensive than simply casting more rays through the pixel. Cone tracing is a similar technique using a cone instead of a complex pyramid.
Beam tracing solves certain problems related to sampling and aliasing, which can plague conventional ray tracing approaches. Since beam tracing effectively calculates the path of every possible ray within each beam (which can be viewed as a dense bundle of adjacent rays), it is not as prone to under-sampling (missing rays) or over-sampling (wasted computational resources). The computational complexity associated with beams has made them unpopular for many visualization applications. In recent years, Monte Carlo algorithms like distributed ray tracing (and Metropolis light transport?) have become more popular for rendering calculations.
A 'backwards' variant of beam tracing casts beams from the light source into the environment. Similar to backwards raytracing and photon mapping, backwards beam tracing may be used to efficiently model lighting effects such as caustics. Recently the backwards beam tracing technique has also been extended to handle glossy to diffuse material interactions (glossy backward beam tracing) such as from polished metal surfaces.
Beam tracing has been successfully applied to the fields of acoustic modelling and electromagnetic propagation modelling. In both of these applications, beams are used as an efficient way to track deep reflections from a source to a receiver (or vice versa). Beams can provide a convenient and compact way to represent visibility. Once a beam tree has been calculated, one can use it to readily account for moving transmitters or receivers.
Beam tracing is related in concept to cone tracing.",[u'Global illumination algorithms'],"[u'3D computer graphics', u'Acoustics', u'Algorithm', u'Aliasing', u'Caustic (optics)', u'Complex polygon', u'Computer graphics', u'Cone tracing', u'Distributed ray tracing', u'Electromagnetism', u'Image plane', u'Metropolis light transport', u'Monte Carlo method', u'Pat Hanrahan', u'Paul Heckbert', u'Photon mapping', u'Pixel', u'Polygon', u'Ray tracing (graphics)', u'Reflection (physics)', u'Refraction', u'Rendering (computer graphics)', u'Sample (signal)', u'Viewing frustum', u'Wave propagation']"
Bees algorithm,"In computer science and operations research, the Bees Algorithm is a population-based search algorithm which was developed in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the Bees Algorithm is that some measure of topological distance between the solutions is defined. The effectiveness and specific abilities of the Bees Algorithm have been proven in a number of studies.","[u'Artificial intelligence', u'Bees', u'Collective intelligence', u'Combinatorial algorithms', u'Optimization algorithms and methods']","[u'Active matter', u'Agent-based model', u'Agent-based model in biology', u'Allee effect', u'Altitudinal migration', u'Animal migration', u'Animal migration tracking', u'Animal navigation', u'Ant colony optimization algorithms', u'Ant robotics', u'Approximation algorithm', u'Artificial Ants', u'Artificial bee colony algorithm', u'Augmented Lagrangian method', u'Bait ball', u'Barrier function', u'Bat algorithm', u'Bellman\u2013Ford algorithm', u'Bird migration', u'Boids', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Cell migration', u'Clustering of self-propelled particles', u'Coded wire tag', u'Collective animal behavior', u'Collective intelligence', u'Collective motion', u'Combinatorial optimization', u'Comparison of optimization software', u'Computer science', u'Continuous optimization', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Crowd simulation', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Decentralised system', u'Diel vertical migration', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Eusociality', u'Evolutionary algorithm', u'Evolutionary computation', u'Exchange algorithm', u'Feeding frenzy', u'Firefly algorithm', u'Fish migration', u'Flock (birds)', u'Flocking (behavior)', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Glowworm swarm optimization', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Group size measures', u'Herd', u'Herd behavior', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Homing (biology)', u'Honey bees', u'Insect migration', u'Integer programming', u'Intelligent Small World Autonomous Robots for Micro-manipulation', u'Intelligent Water Drops', u'Iterative method', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Lepidoptera migration', u'Lessepsian migration', u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'L\xe9vy flight foraging hypothesis', u'Manufacturing Engineering Centre', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Microbial intelligence', u'Microbotics', u'Minimum spanning tree', u'Mixed-species foraging flock', u'Mobbing (animal behavior)', u'Monarch butterfly migration', u'Mutualism (biology)', u'Natal homing', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Operations research', u'Optimization algorithm', u'Pack (canine)', u'Pack hunter', u'Particle swarm optimization', u'Patterns of self-organization in ants', u'Penalty method', u'Philopatry', u""Powell's method"", u'Predator satiation', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Quorum sensing', u'Reverse migration (birds)', u'Revised simplex algorithm', u'Salmon run', u'Sardine run', u'Sea turtle migration', u'Search algorithm', u'Self-propelled particles', u'Sequential quadratic programming', u'Shoaling and schooling', u'Simplex algorithm', u'Simulated annealing', u'Sort sol (bird flock)', u'Spatial organization', u'Stigmergy', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Swarm (simulation)', u'Swarm behaviour', u'Swarm intelligence', u'Swarm robotics', u'Swarming (honey bee)', u'Swarming (military)', u'Swarming motility', u'Symbrion', u'Symmetric rank-one', u'Symmetry breaking of escaping ants', u'Tabu search', u'Task allocation and partitioning of social insects', u'Truncated Newton method', u'Trust region', u'Vicsek model', u'Waggle dance', u'Wolfe conditions']"
Bellman–Ford algorithm,"The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph. It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers. The algorithm is named after two of its developers, Richard Bellman and Lester Ford, Jr., who published it in 1958 and 1956, respectively; however, Edward F. Moore also published the same algorithm in 1957, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.
Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm. If a graph contains a ""negative cycle"" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect negative cycles and report their existence.","[u'Articles with example C code', u'Articles with example pseudocode', u'Dynamic programming', u'Graph algorithms', u'Polynomial-time problems']","[u'A* search algorithm', u'Algorithm', u'Alpha\u2013beta pruning', u'ArXiv', u'Autonomous system (Internet)', u'B*', u'Backtracking', u'Beam search', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u'Big O notation', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Charles E. Leiserson', u'Count to infinity', u'Cycle-cancelling', u'Cycle (graph theory)', u'D*', u'David Eppstein', u'Dense graph', u'Depth-first search', u'Depth-limited search', u""Dijkstra's Algorithm"", u""Dijkstra's algorithm"", u'Distance-vector routing protocol', u'Dynamic programming', u""Edmonds' algorithm"", u'Edward F. Moore', u'Expected value', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph (data structure)', u'Graph traversal', u'Greedy algorithm', u'Hill climbing', u'International Standard Book Number', u'Introduction to Algorithms', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jon Kleinberg', u'Jump point search', u""Kruskal's algorithm"", u'L. R. Ford, Jr.', u'Lexicographic breadth-first search', u'List of algorithms', u'Mathematical Reviews', u'Mathematical induction', u'Network topology', u""O'Reilly Media"", u""Prim's algorithm"", u'Random permutation', u'Relaxation (iterative method)', u'Richard Bellman', u'Robert Sedgewick (computer scientist)', u'Ron Rivest', u'Routing Information Protocol', u'SMA*', u'Search game', u'Shortest path', u'Single-source shortest path problem', u'Thomas H. Cormen', u'Tree traversal', u'Vertex (graph theory)', u'Walk (graph theory)', u'Weighted digraph', u'\xc9va Tardos']"
Benson's algorithm,"Benson's algorithm, named after Harold Benson, is a method for solving linear multi-objective optimization problems. This works by finding the ""efficient extreme points in the outcome set"". The primary concept in Benson's algorithm is to evaluate the upper image of the vector optimization problem by cutting planes.","[u'All stub articles', u'Applied mathematics stubs', u'Linear programming', u'Optimization algorithms and methods']","[u'Applied mathematics', u""Benson's algorithm (Go)"", u'Cutting-plane method', u'Digital object identifier', u'Extreme point', u'Go (game)', u'Harold Benson', u'International Standard Book Number', u'Linear programming', u'Multi-objective optimization', u'Multiobjective optimization', u'Vector optimization']"
Berkeley algorithm,"The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source. It was developed by Gusella and Zatti at the University of California, Berkeley in 1989  and like Cristian's algorithm is intended for use within intranets.",[u'Distributed algorithms'],"[u'Chang and Roberts algorithm', u'Clock synchronisation', u""Cristian's algorithm"", u'Digital object identifier', u'Distributed computing', u'Intranets', u'Leader election', u'Make (software)', u'Round-trip time']"
Berlekamp–Massey algorithm,"The Berlekamp–Massey algorithm is an algorithm that will find the shortest linear feedback shift register (LFSR) for a given binary output sequence. The algorithm will also find the minimal polynomial of a linearly recurrent sequence in an arbitrary field. The field requirement means that the Berlekamp–Massey algorithm requires all non-zero elements to have a multiplicative inverse. Reeds and Sloane offer an extension to handle a ring.
Elwyn Berlekamp invented an algorithm for decoding Bose–Chaudhuri–Hocquenghem (BCH) codes. James Massey recognized its application to linear feedback shift registers and simplified the algorithm. Massey termed the algorithm the LFSR Synthesis Algorithm (Berlekamp Iterative Algorithm), but it is now known as the Berlekamp–Massey algorithm.","[u'All articles with dead external links', u'Articles with German-language external links', u'Articles with dead external links from June 2015', u'Cryptanalytic algorithms', u'Error detection and correction']","[u'Algorithm', u'Assignment (computer science)', u'BCH code', u""Berlekamp's algorithm"", u'Berlekamp\u2013Welch algorithm', u'Bit', u'CiteSeer', u'Digital object identifier', u'Elwyn Berlekamp', u'Encyclopedia of Mathematics', u'Eric W. Weisstein', u'Exclusive or', u'Field (mathematics)', u'Finite field', u'International Standard Book Number', u'James Massey', u'Linear feedback shift register', u'MathWorld', u'Minimal polynomial (field theory)', u'N. J. A. Sloane', u'NLFSR', u'PlanetMath', u'Recurrence relation', u'Reeds\u2013Sloane algorithm', u'Reed\u2013Solomon error correction', u'Ring (mathematics)', u'Springer Science+Business Media']"
Best-first search,"Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.
Judea Pearl described best-first search as estimating the promise of node n by a ""heuristic evaluation function  which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain.""
Some authors have used ""best-first search"" to refer specifically to a search with a heuristic that attempts to predict how close the end of a path is to a solution, so that paths which are judged to be closer to a solution are extended first. This specific type of search is called greedy best-first search or pure heuristic search.
Efficient selection of the current best candidate for extension is typically implemented using a priority queue.
The A* search algorithm is an example of best-first search, as is B*. Best-first algorithms are often used for path finding in combinatorial search. (Note that neither A* nor B* is a greedy best-first search as they incorporate the distance from start in addition to estimated distances to the goal.)","[u'All articles with dead external links', u'Articles with dead external links from August 2014', u'Search algorithms']","[u'A* search algorithm', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Combinatorial search', u'D*', u'Depth-first search', u'Depth-limited search', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph (data structure)', u'Graph traversal', u'Greedy algorithm', u'Heuristic function', u'Hill climbing', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Judea Pearl', u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Peter Norvig', u""Prim's algorithm"", u'Priority queue', u'SMA*', u'Search algorithm', u'Search game', u'Stuart J. Russell', u'Tree traversal']"
Best Bin First,"Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces. The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible. Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.

","[u'Algorithms and data structures stubs', u'All stub articles', u'Computer science stubs', u'Search algorithms']","[u'Algorithm', u'CiteSeer', u'Data structure', u'Kd-tree', u'Nearest neighbor search', u'Search algorithm']"
Bidirectional search,"Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle. The reason for this approach is that in many cases it is faster: for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor b, and the distance from start to goal is d, each of the two searches has complexity O(bd/2) (in Big O notation), and the sum of these two search times is much less than the O(bd) complexity that would result from a single search from the beginning to the goal.
As in A* search, bi-directional search can be guided by a heuristic estimate of the remaining distance to the goal (in the forward tree) or from the start (in the backward tree).
Ira Pohl (1971) was the first one to design and implement a bi-directional heuristic search algorithm. Andrew Goldberg and others explained the correct termination conditions for the bidirectional version of Dijkstra’s Algorithm.","[u'Graph algorithms', u'Search algorithms']","[u'A*', u'A* search algorithm', u'Alpha\u2013beta pruning', u'Artificial Intelligence: A Modern Approach', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Big O notation', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branching factor', u'Breadth-first search', u'British Museum algorithm', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dijkstra\u2019s Algorithm', u'Directed graph', u'Donald Michie', u'Dynamic programming', u""Edmonds' algorithm"", u'Fifteen puzzle', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph search algorithm', u'Graph traversal', u'Heuristic (computer science)', u'Hill climbing', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Journal of the ACM', u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Peter Norvig', u""Prim's algorithm"", u'SMA*', u'Search game', u'Shortest path', u'State space search', u'Stuart J. Russell', u'Tree (graph theory)', u'Tree traversal', u'Vertex (graph theory)']"
Binary GCD algorithm,"The binary GCD algorithm, also known as Stein's algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers. Stein's algorithm uses simpler arithmetic operations than the conventional Euclidean algorithm; it replaces division with arithmetic shifts, comparisons, and subtraction. Although the algorithm was first published by the Israeli physicist and programmer Josef Stein in 1967, it may have been known in 1st-century China.

","[u'All articles with unsourced statements', u'Articles with example C code', u'Articles with unsourced statements from March 2014', u'Number theoretic algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alexander Stepanov', u'Algorithm', u'Ancient Egyptian multiplication', u'Arbitrary-precision arithmetic', u'Arithmetic shift', u'Asymptotic notation', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big-O notation', u'Bitwise operator', u'Brigitte Vall\xe9e', u'C (programming language)', u'Chakravala method', u'Charles E. Leiserson', u""Cipolla's algorithm"", u'CiteSeerX', u'Clifford Stein', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cut-the-knot', u'Digital object identifier', u'Discrete logarithm', u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Exclusive or', u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'International Standard Serial Number', u'Introduction to Algorithms', u'Josef Stein', u'Karatsuba algorithm', u'Least common multiple', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MIX', u'Mathematical Reviews', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic function', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u'Recursion (computer science)', u'Richard Brent (scientist)', u'Ronald L. Rivest', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Tail recursive', u'The Art of Computer Programming', u'The Nine Chapters on the Mathematical Art', u'Thomas H. Cormen', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Binary search algorithm,"In computer science, a binary search or half-interval search algorithm finds the position of a target value within a sorted array. The binary search algorithm can be classified as a dichotomic divide-and-conquer search algorithm and executes in logarithmic time.","[u'All articles needing cleanup', u'All articles with unsourced statements', u'Articles needing cleanup from April 2011', u'Articles with unsourced statements from August 2009', u'Articles with unsourced statements from October 2011', u'Cleanup tagged articles without a reason field from April 2011', u'Search algorithms', u'Wikipedia articles needing clarification from January 2015', u'Wikipedia pages needing cleanup from April 2011']","[u'.NET Framework', u'Addison-Wesley', u'Algorithm', u'Algorithm function', u'Array data structure', u'Best, worst and average case', u'Big O notation', u'Binary logarithm', u'Binary search algorithm', u'Bisection method', u'Branch table', u'C++', u'COBOL', u'CPAN', u'C (programming language)', u'C standard library', u'Cache (computing)', u'Charles E. Leiserson', u'Cocoa (API)', u'Computer science', u'Core Foundation', u'Decision problem', u'Dichotomic search', u'Digital object identifier', u'Divide and conquer algorithm', u'Donald Knuth', u'Eric W. Weisstein', u'Exponential search', u'Floor function', u'Fractional cascading', u'Function overloading', u'Generic programming', u'Go (programming language)', u'Index (database)', u'International Standard Book Number', u'Interpolation search', u'Introduction to Algorithms', u'Iteration', u'Java (programming language)', u'Jon Bentley', u'Linear search', u'Locality of reference', u'Logarithmic algorithm', u'Logarithmic time', u'MathWorld', u'Microsoft', u'Niklaus Wirth', u'Objective-C', u'Oracle Corporation', u'Perl', u'Permanent', u'Prentice Hall', u'Python (programming language)', u'Recursion', u'Reduction (complexity)', u'Ron Rivest', u'Ruby (programming language)', u'Run-time analysis', u'SIGCSE', u'Search algorithm', u'Self-balancing binary search tree', u'Signedness', u'Sorted array', u'Standard Template Library', u'Tail recursive', u'Telephone book', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Worst case analysis']"
Binary splitting,"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. In particular, it can be used to evaluate hypergeometric series at rational points. Given a series

where pn and qn are integers, the goal of binary splitting is to compute integers P(a, b) and Q(a, b) such that

The splitting consists of setting m = [(a + b)/2] and recursively computing P(a, b) and Q(a, b) from P(a, m), P(m, b), Q(a, m), and Q(m, b). When a and b are sufficiently close, P(a, b) and Q(a, b) can be computed directly from pa...pb and qa...qb.
Binary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. To take full advantage of the scheme, fast multiplication algorithms such as Toom–Cook and Schönhage–Strassen must be used; with ordinary O(n2) multiplication, binary splitting may render no speedup at all or be slower.
Since all subdivisions of the series can be computed independently of each other, binary splitting lends well to parallelization and checkpointing.
In a less specific sense, binary splitting may also refer to any divide and conquer algorithm that always divides the problem in two halves.",[u'Computer arithmetic algorithms'],"[u'Checkpointing', u'Class Library for Numbers', u'Divide and conquer algorithm', u'Hypergeometric series', u'Mathematics', u'Parallelization', u'Sch\xf6nhage\u2013Strassen algorithm', u'Series (mathematics)', u'Toom\u2013Cook multiplication']"
Bitap algorithm,"The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates–Gonnet algorithm) is an approximate string matching algorithm. The algorithm tells whether a given text contains a substring which is ""approximately equal"" to a given pattern, where approximate equality is defined in terms of Levenshtein distance — if the substring and pattern are within a given distance k of each other, then the algorithm considers them equal. The algorithm begins by precomputing a set of bitmasks containing one bit for each element of the pattern. Then it is able to do most of the work with bitwise operations, which are extremely fast.
The bitap algorithm is perhaps best known as one of the underlying algorithms of the Unix utility agrep, written by Udi Manber, Sun Wu, and Burra Gopal. Manber and Wu's original paper gives extensions of the algorithm to deal with fuzzy matching of general regular expressions.
Due to the data structures required by the algorithm, it performs best on patterns less than a constant length (typically the word length of the machine in question), and also prefers inputs over a small alphabet. Once it has been implemented for a given alphabet and word length m, however, its running time is completely predictable — it runs in O(mn) operations, no matter the structure of the text or the pattern.
The bitap algorithm for exact string searching was invented by Bálint Dömölki in 1964[1][2] and extended by R. K. Shyamasundar in 1977[3], before being reinvented in the context of fuzzy string searching by Manber and Wu in 1991[4][5] based on work done by Ricardo Baeza-Yates and Gaston Gonnet[6]. The algorithm was improved by Baeza-Yates and Navarro in 1996[7] and later by Gene Myers for long patterns in 1998[8].","[u'Articles with example C code', u'String matching algorithms']","[u'Agrep', u'Aho\u2013Corasick algorithm', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Big O notation', u'Bitmask', u'Bitwise operation', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Burra Gopal', u'Commentz-Walter algorithm', u'Communications of the ACM', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Damerau\u2013Levenshtein distance', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Edit distance', u'Fuzzy matching', u'Gaston Gonnet', u'Gene Myers', u'Generalized suffix tree', u'Gonzalo Navarro', u'Hamming distance', u""Hirschberg's algorithm"", u'Inner loop', u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Parsing', u'Pattern matching', u'Programming tool', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Regular expression', u'Regular tree grammar', u'Ricardo Baeza-Yates', u'Rope (data structure)', u'Running time', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Sun Wu (computer scientist)', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Udi Manber', u'University of Arizona', u'Unix', u'Wagner\u2013Fischer algorithm', u'Word length']"
Bitonic sorter,"Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of  comparators and have a delay of , where  is the number of items to be sorted.
A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A bitonic sequence is a sequence with  for some , or a circular shift of such a sequence.",[u'Sorting algorithms'],"[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Butterfly network', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Java (programming language)', u'Ken Batcher', u'Library sort', u'List (computing)', u'Merge sort', u'NIST', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel algorithm', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Python (programming language)', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Block Truncation Coding,"Block Truncation Coding, or BTC, is a type of lossy image compression technique for greyscale images. It divides the original images into blocks and then uses a quantiser to reduce the number of grey levels in each block whilst maintaining the same mean and standard deviation. It is an early predecessor of the popular hardware DXTC technique, although BTC compression method was first adapted to colour long before DXTC using a very similar approach called Color Cell Compression. BTC has also been adapted to video compression 
BTC was first proposed by E.J Delp and O.R. Mitchell  at Purdue University. Another variation of BTC is Absolute Moment Block Truncation Coding or AMBTC, in which instead of using the standard deviation the first absolute moment is preserved along with the mean. AMBTC is computationally simpler than BTC and also typically results in a lower Mean Squared Error (MSE). AMBTC was proposed by Maximo Lema and Robert Mitchell.
Using sub-blocks of 4x4 pixels gives a compression ratio of 4:1 assuming 8-bit integer values are used during transmission or storage. Larger blocks allow greater compression (""a"" and ""b"" values spread over more pixels) however quality also reduces with the increase in block size due to the nature of the algorithm.
The BTC algorithm was used for compressing Mars Pathfinder's rover images.","[u'Image compression', u'Lossy compression algorithms']","[u'Arithmetic mean', u'Color Cell Compression', u'Digital object identifier', u'Grey levels', u'Greyscale', u'International Standard Book Number', u'Lossy compression', u'Mars Pathfinder', u'Mean', u'Pixel', u'S3 Texture Compression', u'Standard deviation']"
Block nested loop,"A block-nested loop (BNL) is an algorithm used to join two relations in a relational database.
This algorithm is a variation on the simple nested loop join used to join two relations  and  (the ""outer"" and ""inner"" join operands, respectively). Suppose . In a traditional nested loop join,  will be scanned once for every tuple of . If there are many qualifying  tuples, and particularly if there is no applicable index for the join key on , this operation will be very expensive.
The block nested loop join algorithm improves on the simple nested loop join by only scanning  once for every group of  tuples. For example, one variant of the block nested loop join reads an entire page of  tuples into memory and loads them into a hash table. It then scans , and probes the hash table to find  tuples that match any of the tuples in the current page of . This reduces the number of scans of  that are necessary.
A more aggressive variant of this algorithm loads as many pages of  as can be fit in the available memory, loading all such tuples into a hash table, and then repeatedly scans . This further reduces the number of scans of  that are necessary. In fact, this algorithm is essentially a special-case of the classic hash join algorithm.
The block nested loop runs in  I/Os where  is the number of available pages of internal memory and  and  is size of  and  respectively in pages. Note that block nested loop runs in  I/Os if  fits in the available internal memory.","[u'All articles with unsourced statements', u'Articles with unsourced statements from August 2015', u'Join algorithms']","[u'Algorithm', u'Hash join', u'Hash table', u'Join (SQL)', u'Nested loop join', u'Page (computing)', u'Relational database']"
Bloom filter,"A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not, thus a Bloom filter has a 100% recall rate. In other words, a query returns either ""possibly in set"" or ""definitely not in set"". Elements can be added to the set, but not removed (though this can be addressed with a ""counting"" filter). The more elements that are added to the set, the larger the probability of false positives.
Bloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if ""conventional"" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses, an 85–15 form of the Pareto principle (Bloom (1970)).
More generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set (Bonomi et al. (2006)).","[u'All articles lacking in-text citations', u'All articles with dead external links', u'Articles lacking in-text citations from November 2009', u'Articles with dead external links from June 2010', u'Commons category with local link same as on Wikidata', u'Hashing', u'Lossy compression algorithms', u'Probabilistic data structures']","[u'Aggregate function', u'Algorithm', u'Andrei Broder', u'Apache Cassandra', u'Apache HBase', u'ArXiv', u'Arithmetic overflow', u'Array data structure', u'Associative array', u'Azuma\u2013Hoeffding inequality', u'Bernard Chazelle', u'BigTable', u'Bit array', u'Bitcoin', u'Bitwise operation', u'Bloom (shader effect)', u'Cache misses', u'Calvin Mooers', u'Cascading (software)', u'Chemical similarity', u'CiteSeer', u'Communications of the ACM', u'Count\u2013min sketch', u'Cuckoo hashing', u'Data structure', u'Data structures', u'Data synchronization', u'David Eppstein', u'Digital object identifier', u'Double hashing', u'Edge-notched card', u'Element (mathematics)', u'Enhanced double hashing', u'Exim', u'False positive', u'Feature hashing', u'Golomb coding', u'Google Chrome', u'Hash compaction', u'Hash function', u'Hash table', u'Hyphenation algorithm', u'IEEE/ACM Transactions on Networking', u'Information content', u'International Standard Book Number', u'Intersection (set theory)', u'Jaccard index', u'Lattice (order)', u'Lecture Notes in Computer Science', u'Linked list', u'Map (mathematics)', u'Medium', u'Michael T. Goodrich', u'Mihai P\u0103tra\u015fcu', u'MinHash', u'PRNG', u'Pareto principle', u'Perl', u'Peter Sanders (computer scientist)', u'Precision and recall', u'Probabilistic', u'PubMed Identifier', u'Quotient filter', u'Random binary tree', u'Random tree (disambiguation)', u'Randomized algorithm', u'Rapidly exploring random tree', u'SPIN model checker', u'Self-balancing binary search tree', u'Set (computer science)', u'Skip list', u'Squid (software)', u'Subgraph isomorphism problem', u'Superimposed code', u'Treap', u'Trie', u'Triple hashing', u'Type I and type II errors', u'Union (set theory)', u'University of Wisconsin\u2013Madison', u'Venti', u'Web cache', u'Workshop on Algorithms and Data Structures', u'World Wide Web', u'YouTube', u'Zatocoding']"
Bogosort,"In computer science, bogosort (also stupid sort, slowsort, random sort, shotgun sort or monkey sort) is a particularly ineffective sorting algorithm based on the generate and test paradigm. It is not useful for sorting, but may be used for educational purposes, to contrast it with other more realistic algorithms; it has also been used as an example in logic programming. If bogosort were used to sort a deck of cards, it would consist of checking if the deck were in order, and if it were not, throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name comes from the word bogus.","[u'Articles to be expanded from date=November 2015', u'Articles with invalid date parameter in template', u'Comparison sorts', u'Computer humor', u'Sorting algorithms', u'Use dmy dates from June 2011']","[u'Adaptive sort', u'Almost surely', u'American flag sort', u'Array data structure', u'Asymptotic analysis', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'CiteSeer', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Deck of cards', u'Digital object identifier', u'Expected value', u'Flashsort', u'Gnome sort', u'Google Code Jam', u'Heapsort', u'Heat death of the universe', u'Hybrid algorithm', u'In-place algorithm', u'Infinite monkey theorem', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Las Vegas algorithm', u'Lecture Notes in Computer Science', u'Library sort', u'List (computing)', u'Logic programming', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sort (Unix)', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Trial and error', u'Unix-like', u'WikiWikiWeb']"
Boosting (meta-algorithm),"Boosting is a machine learning ensemble meta-algorithm for reducing bias primarily and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
Robert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.
When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. ""Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner]."" Algorithms that achieve hypothesis boosting quickly became simply known as ""boosting"". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.","[u'All articles to be expanded', u'All articles to be merged', u'Articles to be expanded from December 2009', u'Articles to be merged from December 2012', u'Classification algorithms', u'Ensemble learning']","[u'AdaBoost', u'Alternating decision tree', u'AnyBoost', u'Boost by majority', u'Boosting methods for object categorization', u'Bootstrap aggregating', u'BrownBoost', u'Cascading classifiers', u'CiteSeer', u'CoBoosting', u'Convex function', u'Cross-validation (statistics)', u'Digital object identifier', u'Ensemble learning', u'Function space', u'GentleBoost', u'Gradient boosting', u'Gradient descent', u'G\xf6del Prize', u'International Standard Book Number', u'LPBoost', u'Leo Breiman', u'Leslie Valiant', u'Logistic regression', u'LogitBoost', u'Machine learning', u'MadaBoost', u'Margin classifier', u'Meta-algorithm', u'Michael Kearns', u'Michael Kearns (computer scientist)', u'Neural network', u'Orange (software)', u'Principle of maximum entropy', u'Random forest', u'RankBoost', u'Robert Schapire', u'Scikit-learn', u'Statistics', u'Supervised learning', u'Support vector machine', u'TotalBoost', u'Weka (machine learning)', u'Yoav Freund', u'Zhou Zhihua']"
Bootstrap aggregating,"Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.","[u'Computational statistics', u'Ensemble learning', u'Machine learning algorithms']","[u'Annals of Statistics', u'Anomaly detection', u'Artificial neural network', u'Artificial neural networks', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Bayes classifier', u'Bayesian network', u'Bias-variance dilemma', u'Boosting (machine learning)', u'Boosting (meta-algorithm)', u'Bootstrap (statistics)', u'Bootstrapping (statistics)', u'Canonical correlation analysis', u'CiteSeer', u'Classic data sets', u'Classification and regression tree', u'Cluster analysis', u'Computational learning theory', u'Conditional random field', u'Convolutional neural network', u'Cross-validation (statistics)', u'DBSCAN', u'Data mining', u'Decision tree learning', u'Deep learning', u'Digital object identifier', u'Dimensionality reduction', u'E (mathematical constant)', u'Empirical risk minimization', u'Ensemble learning', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Grammar induction', u'Graphical model', u'Hidden Markov model', u'Hierarchical clustering', u'Independent component analysis', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Learning to rank', u'Leo Breiman', u'Linear discriminant analysis', u'Linear regression', u'Local outlier factor', u'Local regression', u'Logistic regression', u'Machine Learning (journal)', u'Machine learning', u'Mean-shift', u'Meta-algorithm', u'Multilayer perceptron', u'Naive Bayes classifier', u'Non-negative matrix factorization', u'OPTICS algorithm', u'Online machine learning', u'Overfitting', u'Ozone', u'Perceptron', u'Peter Rousseeuw', u'Prime (symbol)', u'Principal component analysis', u'Probability distribution', u'Probably approximately correct learning', u'R (programming language)', u'Random forest', u'Random subspace method', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Sampling (statistics)', u'Self-organizing map', u'Semi-supervised learning', u'Statistical classification', u'Statistical learning theory', u'Structured prediction', u'Supervised learning', u'Support vector machine', u'T-distributed stochastic neighbor embedding', u'Training set', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Variance', u'Weighted nearest neighbour classifier']"
Borwein's algorithm,"In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/π. They devised several other algorithms. They published a book: Jonathon M. Borwein, Peter B. Borwein, Pi and the AGM - A Study in Analytic Number Theory and Computational Complexity, Wiley, New York, 1987. Many of their results are available in: Jorg Arndt, Christoph Haenel, Pi Unleashed, Springer, Berlin, 2001, ISBN 3-540-66572-2.","[u'All articles with unsourced statements', u'Articles with unsourced statements from June 2011', u'Pi algorithms']","[u'Algorithm', u'Bailey\u2013Borwein\u2013Plouffe formula', u'Gauss\u2013Legendre algorithm', u'International Standard Book Number', u'Jonathan Borwein', u'Mathematics', u'Peter Borwein', u'Pi', u'Ramanujan\u2013Sato series']"
Borůvka's algorithm,"Borůvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct.
It was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia. The algorithm was rediscovered by Choquet in 1938; again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Sollin  in 1965. Because Sollin was the only computer scientist in this list living in an English speaking country, this algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.
The algorithm begins by first examining each vertex and adding the cheapest edge from that vertex to another in the graph, without regard to already added edges, and continues joining these groupings in a like manner until a tree spanning all vertices is completed.","[u'CS1 Czech-language sources (cs)', u'CS1 French-language sources (fr)', u'CS1 maint: Unrecognized language', u'Graph algorithms', u'Spanning tree']","[u'A* search algorithm', u'Ackermann function', u'Algorithm', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Bernard Chazelle', u'Best-first search', u'Bidirectional search', u'Big O notation', u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Connected component (graph theory)', u'D*', u'David Eppstein', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Discrete Mathematics (journal)', u'Disjoint-set data structure', u'Dynamic programming', u""Edmonds' algorithm"", u'Electricity network', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph minor', u'Graph traversal', u'Gustave Choquet', u'Hill climbing', u'Hugo Steinhaus', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Jan \u0141ukasiewicz', u'Jaroslav Ne\u0161et\u0159il', u""Johnson's algorithm"", u'Jorge Urrutia Galicia', u'Julian Perkal', u'Jump point search', u'J\xf6rg-R\xfcdiger Sack', u'Kazimierz Florek', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'Lexicographic order', u'List of algorithms', u'M. Sollin', u'Mathematical Reviews', u'Minimum spanning tree', u'Moravia', u'Otakar Bor\u016fvka', u'Parallel computing', u'Planar graph', u""Prim's algorithm"", u'SMA*', u'Search game', u'Sollin', u'Stefan Zubrzycki', u'Tree traversal']"
Bowyer–Watson algorithm,"In computational geometry, the Bowyer–Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions. The algorithm can be used to obtain a Voronoi diagram of the points, which is the dual graph of the Delaunay triangulation.
The Bowyer–Watson algorithm is an incremental algorithm. It works by adding points, one at a time, to a valid Delaunay triangulation of a subset of the desired points. After every insertion, any triangles whose circumcircles contain the new point are deleted, leaving a star-shaped polygonal hole which is then re-triangulated using the new point. By using the connectivity of the triangulation to efficiently locate triangles to remove, the algorithm can take O(N log N) operations to triangulate N points, although special degenerate cases exist where this goes up to O(N2).
The algorithm is sometimes known just as the Bowyer Algorithm or the Watson Algorithm. Adrian Bowyer and David Watson devised it independently of each other at the same time, and each published a paper on it in the same issue of The Computer Journal (see below).",[u'Geometric algorithms'],"[u'Adrian Bowyer', u'Computational geometry', u'Delaunay triangulation', u'Digital object identifier', u'Dimension', u'Dual graph', u""Fortune's algorithm"", u'Hilbert curve', u'Pseudocode', u'Star-shaped polygon', u'The Computer Journal', u'Voronoi diagram']"
Boyer–Moore string search algorithm,"In computer science, the Boyer–Moore string search algorithm is an efficient string searching algorithm that is the standard benchmark for practical string search literature. It was developed by Robert S. Boyer and J Strother Moore in 1977. The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.","[u'Algorithms on strings', u'Articles with example C code', u'Articles with example Java code', u'Articles with example Python code', u'String matching algorithms']","[u'Aho\u2013Corasick algorithm', u'Algorithm', u'Andrew Odlyzko', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Bitap algorithm', u'Boost (C++ libraries)', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Brute-force search', u'C++', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computer science', u'D (programming language)', u'Damerau\u2013Levenshtein distance', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Donald Knuth', u'Edit distance', u'Generalized suffix tree', u'Go (programming language)', u'Hamming distance', u""Hirschberg's algorithm"", u'International Standard Book Number', u'International Standard Serial Number', u'J Strother Moore', u'James H. Morris', u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Leonidas J. Guibas', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Nqthm', u'Parsing', u'Pattern matching', u'Preprocessor', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Regular expression', u'Regular tree grammar', u'Robert S. Boyer', u'Rope (data structure)', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Substring', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Vaughan Pratt', u'Wagner\u2013Fischer algorithm', u'Zvi Galil']"
Boyer–Moore–Horspool algorithm,"In computer science, the Boyer–Moore–Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings. It was published by Nigel Horspool in 1980.
It is a simplification of the Boyer–Moore string search algorithm which is related to the Knuth–Morris–Pratt algorithm. The algorithm trades space for time in order to obtain an average-case complexity of O(N) on random text, although it has O(MN) in the worst case, where the length of the pattern is M and the length of the search string is N.","[u'All accuracy disputes', u'All articles needing additional references', u'All articles to be merged', u'Articles needing additional references from October 2015', u'Articles to be merged from March 2015', u'Articles with disputed statements from June 2015', u'Articles with example C code', u'Pages using citations with accessdate and no URL', u'String matching algorithms']","[u'Aho\u2013Corasick algorithm', u'Algorithm', u'Alphabet (formal languages)', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Average-case complexity', u'Big O notation', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'CiteSeer', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computer science', u'Damerau\u2013Levenshtein distance', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Edit distance', u'Generalized suffix tree', u'Hamming distance', u""Hirschberg's algorithm"", u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Needleman\u2013Wunsch algorithm', u'Nigel Horspool', u'Nondeterministic finite automaton', u'Parsing', u'Pattern matching', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Raita Algorithm', u'Regular expression', u'Regular tree grammar', u'Rope (data structure)', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Substring', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Wagner\u2013Fischer algorithm', u'Worst case']"
Branch and bound,"Branch and bound (BB or B&B) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as general real valued problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.
The algorithm depends on the efficient estimation of the lower and upper bounds of a region/branch of the search space and approaches exhaustive enumeration as the size (n-dimensional volume) of the region tends to zero.
The method was first proposed by A. H. Land and A. G. Doig in 1960 for discrete programming, and has become the most commonly used tool for solving NP-hard optimization problems. The name ""branch and bound"" first occurred in the work of Little et al. on the traveling salesman problem.","[u'All articles with unsourced statements', u'Articles with unsourced statements from July 2015', u'Articles with unsourced statements from September 2015', u'Combinatorial optimization', u'Optimization algorithms and methods', u'Wikipedia articles needing clarification from July 2015']","[u'0/1 knapsack problem', u'A* search algorithm', u'Algorithm', u'Alpha-beta pruning', u'Alpha\u2013beta pruning', u'Approximation algorithm', u'Artificial intelligence', u'Augmented Lagrangian method', u'B*', u'Backtracking', u'Barrier function', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and cut', u'Breadth-first search', u'British Museum algorithm', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Brute-force search', u'Candidate solution', u'Carnegie Mellon University', u'Combinatorial optimization', u'Comparison of optimization software', u'Computational phylogenetics', u'Computer vision', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cutting-plane method', u'Cutting plane', u'Cutting stock problem', u'D*', u'Data structure', u'Davidon\u2013Fletcher\u2013Powell formula', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Discrete optimization', u'Disjoint sets', u'Dynamic programming', u""Edmonds' algorithm"", u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'False noise analysis', u'Feasible region', u'Feature selection', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Fringe search', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Graph traversal', u'Greedy algorithm', u'Hessian matrix', u'Heuristic', u'Heuristic algorithm', u'Hill climbing', u'Integer linear programs', u'Integer programming', u'International Standard Book Number', u'Interval arithmetic', u'Interval contractor', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Iterative method', u""Johnson's algorithm"", u'Jump point search', u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u'Kurt Mehlhorn', u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Lexicographic breadth-first search', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'List of algorithms', u'Local convergence', u'Local search (optimization)', u'Machine learning', u'Mathematical optimization', u'Matroid', u'Maximum satisfiability problem', u'Metaheuristic', u'Minimum spanning tree', u'NP-hard', u'Nearest neighbor search', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Noise', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization algorithm', u'Penalty method', u'Peter Sanders (computer scientist)', u""Powell's method"", u""Prim's algorithm"", u'Priority queue', u'Probability', u'Push\u2013relabel maximum flow algorithm', u'Quadratic assignment problem', u'Quadratic programming', u'Quasi-Newton method', u'Revised simplex algorithm', u'SMA*', u'Search game', u'Search tree', u'Sequential quadratic programming', u'Set estimation', u'Set inversion', u'Simplex algorithm', u'Simulated annealing', u'Stack (data structure)', u'State space search', u'Statistics', u'Structured prediction', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Traveling salesman problem', u'Travelling salesman problem', u'Tree (graph theory)', u'Tree traversal', u'Truncated Newton method', u'Trust region', u'University of Copenhagen', u'Without loss of generality', u'Wolfe conditions']"
Branch and cut,"Branch and cut is a method of combinatorial optimization for solving integer linear programs (ILPs), that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values. Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations. Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called cut and branch.","[u'Combinatorial optimization', u'Optimization algorithms and methods']","[u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cutting-plane method', u'Cutting plane', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer', u'Integer linear program', u'Integer programming', u'Iterative method', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Linear programming relaxation', u'Local convergence', u'Local search (optimization)', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Minimum spanning tree', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization algorithm', u'Penalty method', u""Powell's method"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Revised simplex algorithm', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Truncated Newton method', u'Trust region', u'Wolfe conditions']"
Breadth-first search,"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key') and explores the neighbor nodes first, before moving to the next level neighbors.
BFS was invented in the late 1950s by E. F. Moore, who used it to find the shortest path out of a maze, and discovered independently by C. Y. Lee as a wire routing algorithm (published 1961).","[u'All articles needing additional references', u'Articles needing additional references from April 2012', u'Commons category with local link same as on Wikidata', u'Graph algorithms', u'Search algorithms']","[u'A* search algorithm', u'Adjacency list', u'Adjacency matrix', u'Aho-Corasick', u'Algorithm', u'Alpha\u2013beta pruning', u'Artificial Intelligence: A Modern Approach', u'Artificial intelligence', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u'Bipartite graph', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'British Museum algorithm', u'Cardinality', u'Charles E. Leiserson', u""Cheney's algorithm"", u'Cuthill\u2013McKee algorithm', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Edward F. Moore', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frankfurt', u'Fringe search', u'Garbage collection', u'Germany', u'Graph (data structure)', u'Graph traversal', u'Hill climbing', u'Implicit graph', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Level structure', u'Lexicographic breadth-first search', u'List of algorithms', u'Maximum flow problem', u'Multiple discovery', u'Peter Norvig', u""Prim's algorithm"", u'Queue (abstract data type)', u'Routing (electronic design automation)', u'SMA*', u'Search algorithm', u'Search game', u'Shortest path', u'Stack (abstract data type)', u'Steven Skiena', u'Stuart J. Russell', u'Tree (data structure)', u'Tree data structure', u'Tree traversal']"
Bresenham's line algorithm,"Bresenham's line algorithm is an algorithm that determines the points of an n-dimensional raster that should be selected in order to form a close approximation to a straight line between two points. It is commonly used to draw lines on a computer screen, as it uses only integer addition, subtraction and bit shifting, all of which are very cheap operations in standard computer architectures. It is one of the earliest algorithms developed in the field of computer graphics. An extension to the original algorithm may be used for drawing circles.
While algorithms such as Wu's algorithm are also frequently used in modern computer graphics because they can support antialiasing, the speed and simplicity of Bresenham's line algorithm means that it is still important. The algorithm is used in hardware such as plotters and in the graphics chips of modern graphics cards. It can also be found in many software graphics libraries. Because the algorithm is very simple, it is often implemented in either the firmware or the graphics hardware of modern graphics cards.
The label ""Bresenham"" is used today for a family of algorithms extending or modifying Bresenham's original algorithm.","[u'All articles needing additional references', u'All articles to be expanded', u'All articles with unsourced statements', u'Articles needing additional references from August 2012', u'Articles to be expanded from September 2011', u'Articles with example pseudocode', u'Articles with unsourced statements from December 2010', u'Commons category with local link same as on Wikidata', u'Computer graphics algorithms', u'Digital geometry']","[u'Absolute value', u'Algorithm', u'Association for Computing Machinery', u'Bitwise operation', u'Calcomp plotter', u'Computer architecture', u'Computer graphics', u'Digital differential analyzer (graphics algorithm)', u'Digital object identifier', u'Firmware', u'Graphics card', u'Graphics hardware', u'Graphics library', u'Graphics processing unit', u'IBM 1401', u'International Business Machines', u'International Standard Book Number', u'Jack Elton Bresenham', u'Midpoint circle algorithm', u'National Institute of Standards and Technology', u'Octant (plane geometry)', u'Plotter', u'Pseudocode', u'Raster graphics', u'Slope', u'Spatial anti-aliasing', u'Uv mapping', u'Voxel', u""Xiaolin Wu's line algorithm""]"
Bron–Kerbosch algorithm,"In computer science, the Bron–Kerbosch algorithm is an algorithm for finding maximal cliques in an undirected graph. That is, it lists all subsets of vertices with the two properties that each pair of vertices in one of the listed subsets is connected by an edge, and no listed subset can have any additional vertices added to it while preserving its complete connectivity. The Bron–Kerbosch algorithm was designed by Dutch scientists Joep Kerbosch and Coenraad Bron, who published its description in 1973. Although other algorithms for solving the clique problem have running times that are, in theory, better on inputs that have few maximal independent sets, the Bron–Kerbosch algorithm and subsequent improvements to it are frequently reported as being more efficient in practice than the alternatives. It is well-known and widely used in application areas of graph algorithms such as computational chemistry.
A contemporaneous algorithm of Akkoyunlu (1973), although presented in different terms, can be viewed as being the same as the Bron–Kerbosch algorithm, as it generates the same recursive search tree.","[u'Articles with example pseudocode', u'Graph algorithms']","[u'Algorithm', u'ArXiv', u'Backtracking', u'Clique (graph theory)', u'Clique problem', u'Coenraad Bron', u'Complete graph', u'Computational chemistry', u'Computer science', u'David Eppstein', u'Degeneracy (graph theory)', u'Degree (graph theory)', u'Digital object identifier', u'Empty set', u'Glossary of graph theory', u'Graph (mathematics)', u'International Standard Book Number', u'Joep Kerbosch', u'Leo Moser', u'Linear time', u'Mathematical Reviews', u'Neighborhood (graph theory)', u'Netherlands', u'Output-sensitive algorithm', u'Polynomial time', u'Recursion', u'Social network', u'Sparse graph']"
BrownBoost,"BrownBoost is a boosting algorithm that may be robust to noisy datasets. BrownBoost is an adaptive version of the boost by majority algorithm. As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. BrownBoost was introduced by Yoav Freund in 2001.","[u'Classification algorithms', u'Ensemble learning']","[u'AdaBoost', u'Alternating decision tree', u'AnyBoost', u'Boost by majority', u'Boosting (machine learning)', u'Boosting (meta-algorithm)', u'Generalization error', u'JBoost', u'LogitBoost', u'Machine learning', u""Newton's method"", u'Yoav Freund']"
Bruun's FFT algorithm,"Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. Because its operations involve only real coefficients until the last computation stage, it was initially proposed as a way to efficiently compute the discrete Fourier transform (DFT) of real data. Bruun's algorithm has not seen widespread use, however, as approaches based on the ordinary Cooley–Tukey FFT algorithm have been successfully adapted to real data with at least as much efficiency. Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley–Tukey in the face of finite numerical precision (Storn, 1993).
Nevertheless, Bruun's algorithm illustrates an alternative algorithmic framework that can express both itself and the Cooley–Tukey algorithm, and thus provides an interesting perspective on FFTs that permits mixtures of the two algorithms and other generalizations.

",[u'FFT algorithms'],"[u'Chinese Remainder Theorem', u'Cooley\u2013Tukey FFT algorithm', u'Degree of a polynomial', u'Discrete Fourier transform', u'Discrete cosine transform', u'Fast Fourier transform', u'ICASSP', u'IEEE', u'Polynomial', u'Polynomial remainder theorem', u'Power of two', u'Relatively prime polynomials', u'Root of unity', u'Sic']"
Bubble sort,"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller elements ""bubble"" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort. It can be practical if the input is usually in sort order but may occasionally have some out-of-order elements nearly in position.","[u'All articles with unsourced statements', u'Articles with example pseudocode', u'Articles with unsourced statements from August 2015', u'Commons category with local link same as on Wikidata', u'Comparison sorts', u'Pages with syntax highlighting errors', u'Sorting algorithms', u'Stable sorts', u'Wikipedia articles needing clarification from October 2014']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Average-case complexity', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Big o notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Branch predictor', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail shaker sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Dictionary of Algorithms and Data Structures', u'Donald Knuth', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introduction to Algorithms', u'Introsort', u'Inversion (discrete mathematics)', u'Iteration', u'JSort', u'Jargon File', u'Library sort', u'List (computing)', u'Merge sort', u'National Institute of Standards and Technology', u'Odd-even sort', u'Odd\u2013even sort', u'On-Line Encyclopedia of Integer Sequences', u'Oscillating merge sort', u'Owen Astrachan', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Ronald L. Rivest', u'Sartaj Sahni', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Swap (computer science)', u'The Art of Computer Programming', u'The Tortoise and the Hare', u'Thomas H. Cormen', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Bucket sort,"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, and is a cousin of radix sort in the most to least significant digit flavour. Bucket sort is a generalization of pigeonhole sort. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.
Bucket sort works as follows:
Set up an array of initially empty ""buckets"".
Scatter: Go over the original array, putting each object in its bucket.
Sort each non-empty bucket.
Gather: Visit the buckets in order and put all elements back into the original array.","[u'All articles needing expert attention', u'Articles needing expert attention from November 2008', u'Articles needing expert attention with no reason or talk parameter', u'Articles with example pseudocode', u'Computer science articles needing expert attention', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Analysis of algorithms', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket (computing)', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Dictionary of Algorithms and Data Structures', u'Distribution sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introduction to Algorithms', u'Introsort', u'JSort', u'J sort', u'Library sort', u'List (computing)', u'Merge sort', u'Mergesort', u'National Institute of Standards and Technology', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Post office', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Ronald L. Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Thomas H. Cormen', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Buddy memory allocation,"The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. This system makes use of splitting memory into halves to try to give a best-fit. According to Donald Knuth, the buddy system was invented in 1963 by Harry Markowitz, who won the 1990 Nobel Memorial Prize in Economics, and was first described by Kenneth C. Knowlton (published 1965). Buddy memory allocation is relatively easy to implement. It supports limited but efficient splitting and coalescing of memory blocks.","[u'Memory management algorithms', u'Use dmy dates from August 2012']","[u'Binary tree', u'Coalescing (computer science)', u'Communications of the ACM', u'Data compaction', u'Donald Knuth', u'Dynamic allocation', u'Exclusive OR', u'Fragmentation (computer)', u'Harry Markowitz', u'International Standard Book Number', u'Ken Knowlton', u'Linux kernel', u'Memory allocation', u'Memory pool', u'Nobel Memorial Prize in Economics', u'Programmer', u'Slab allocation', u'Stack-based memory allocation', u'The Art of Computer Programming', u'Wrox Press']"
Bully algorithm,"The bully algorithm is a programming mechanism that applies a hierarchy to nodes on a system, making a process coordinator or slave. This is used as a method in distributed computing for dynamically electing a coordinator by process ID number. The process with the highest process ID number is selected as the coordinator.","[u'All Wikipedia articles needing clarification', u'All self-contradictory articles', u'Distributed algorithms', u'Self-contradictory articles from January 2015', u'Wikipedia articles needing clarification from January 2015']","[u'Chang and Roberts algorithm', u'Distributed Computing', u'Distributed computing']"
Burrows–Wheeler transform,"The Burrows–Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters. This is useful for compression, since it tends to be easy to compress a string that has runs of repeated characters by techniques such as move-to-front transform and run-length encoding. More importantly, the transformation is reversible, without needing to store any additional data. The BWT is thus a ""free"" method of improving the efficiency of text compression algorithms, costing only some extra computation.","[u'Articles with example Python code', u'Articles with example pseudocode', u'Lossless compression algorithms', u'Transforms']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'ArXiv', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Base pair', u'Best, worst and average case', u'Bijection', u'Bijective', u'Bit rate', u'Bowtie (sequence analysis)', u'Byte pair encoding', u'Bzip2', u'Cambridge University Press', u'Canonical Huffman code', u'ChIP-Seq', u'Chain code', u'Character string (computer science)', u'Chen\u2013Fox\u2013Lyndon theorem', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEC Systems Research Center', u'DEFLATE', u'DNA', u'DNA sequencing', u'Data compression', u'David Wheeler (British computer scientist)', u'David Wheeler (computer scientist)', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Eland (software)', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'End-of-file', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Genome', u'Golomb coding', u'Hash function', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'International Standard Book Number', u'International Standard Serial Number', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Lexicographic order', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Lyndon word', u'M. Lothaire', u'Macroblock', u'Maq', u'Michael Burrows', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Next-generation sequencing', u'Null character', u'Nyquist\u2013Shannon sampling theorem', u'Optimization (computer science)', u'PAQ', u'Palo Alto', u'Peak signal-to-noise ratio', u'Permutation', u'Pixel', u'Prediction by partial matching', u'Pseudocode', u'Psychoacoustics', u'PubMed Central', u'PubMed Identifier', u'Pyramid (image processing)', u'Python (programming language)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sorting', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Suffix array', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'Zentralblatt MATH', u'\u039c-law algorithm']"
Burstsort,"Burstsort and its variants are cache-efficient algorithms for sorting strings and are faster than radix sort for large data sets of common strings, first published in 2003.
Burstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are ""burst"", giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.
Burstsort was introduced as a sort that is similar to MSD Radix Sort, but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And even though asymptotically it is the same as radix sort, with time complexity of O(wn) (w - word length and n - number of strings to be sorted), but due to more optimal memory distribution it tends to be twice as fast on big data sets of strings.","[u'Algorithms and data structures stubs', u'All stub articles', u'Computer science stubs', u'Sorting algorithms']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Data set', u'Data structure', u'Digital object identifier', u'Dynamic array', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'String (computer science)', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Trie']"
Buzen's algorithm,"In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem. This method was first proposed by Jeffrey P. Buzen in 1973. Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.
Performing a naïve computation of the normalising constant requires enumeration of all states. For a system with N jobs and M states there are  states. Buzen's algorithm ""computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions."" This is a significant improvement and allows for computations to be performed with much larger networks.

","[u'Queueing theory', u'Statistical algorithms', u'Stochastic processes']","[u'Adversarial queueing network', u'Arrival theorem', u'BCMP network', u'Balance equation', u'Bene\u0161 method', u'Bulk queue', u""Burke's theorem"", u'Continuous-time Markov chain', u'D/M/1 queue', u'Data buffer', u'Decomposition method (queueing theory)', u'Digital object identifier', u'Erlang (unit)', u'Erlang distribution', u'Expected value', u'Exponentially distributed', u'FIFO (computing and electronics)', u'Flow-equivalent server method', u'Flow control (data)', u'Fluid limit', u'Fluid queue', u'Fork\u2013join queue', u'G-network', u'G/G/1 queue', u'G/M/1 queue', u'Gordon F. Newell', u'Gordon\u2013Newell theorem', u'Heavy traffic approximation', u'Information system', u'JSTOR', u'Jackson network', u'Jeffrey P. Buzen', u'Kelly network', u""Kendall's notation"", u""Kingman's formula"", u'LIFO (computing)', u'Layered queueing network', u'Lindley equation', u""Little's law"", u'Loss network', u'M/D/1 queue', u'M/D/c queue', u'M/G/1 queue', u'M/G/k queue', u'M/M/1 queue', u'M/M/c queue', u'M/M/\u221e queue', u'Marginal distribution', u'Markovian arrival process', u'Matrix analytic method', u'Mean field theory', u'Mean value analysis', u'Message queue', u'Network congestion', u'Network scheduler', u'Normalization constant', u'Operations Research (journal)', u'Pipeline (software)', u'Poisson process', u'Pollaczek\u2013Khinchine formula', u'Polling system', u'Probability distribution', u'Probability theory', u'Processor sharing', u'Product-form solution', u'Quality of service', u'Quasireversibility', u'Queueing theory', u'Rational arrival process', u'Reflected Brownian motion', u'Retrial queue', u'Scheduling (computing)', u'Shortest job first', u'Shortest remaining time', u'Teletraffic engineering', u'Traffic equations']"
Byte pair encoding,"Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. A table of the replacements is required to rebuild the original data. The algorithm was first described publicly by Philip Gage in a February 1994 article ""A New Algorithm for Data Compression"" in the C Users Journal.",[u'Lossless compression algorithms'],"[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Recursion', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
C4.5 algorithm,"C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier.
It became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.

","[u'All NPOV disputes', u'All articles lacking in-text citations', u'Articles lacking in-text citations from July 2008', u'Classification algorithms', u'Decision trees', u'NPOV disputes from August 2011']","[u'Boosting (meta-algorithm)', u'Data mining', u'Decision tree learning', u'Entropy (information theory)', u'ID3 algorithm', u'Information gain', u'Java (programming language)', u'Lecture Notes in Computer Science', u'Open source', u'Pseudocode', u'Ross Quinlan', u'Springer Science+Business Media', u'Statistical classification', u'Weka (machine learning)', u'Winnow (algorithm)']"
CYK algorithm,"In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami. It employs bottom-up parsing and dynamic programming.
The standard version of CYK operates only on context-free grammars given in Chomsky normal form (CNF). However any context-free grammar may be transformed to a CNF grammar expressing the same language (Sipser 1997).
The importance of the CYK algorithm stems from its high efficiency in certain situations. Using Landau symbols, the worst case running time of CYK is , where n is the length of the parsed string and |G| is the size of the CNF grammar G. This makes it one of the most efficient parsing algorithms in terms of worst-case asymptotic complexity, although other algorithms exist with better average running time in many practical scenarios.",[u'Parsing algorithms'],"[u'Air Force Cambridge Research Laboratories', u'Algorithm', u'Analysis of algorithms', u'Asymptotic complexity', u'Big O Notation', u'Boolean matrix', u'Bottom-up parsing', u'Chomsky normal form', u'CiteSeer', u'Computational Intelligence (journal)', u'Computer science', u'Context-free grammar', u'Coppersmith\u2013Winograd algorithm', u'Courant Institute of Mathematical Sciences', u'Digital object identifier', u'Donald Knuth', u'Dynamic programming', u'Earley parser', u'Formal grammar', u'GLR parser', u'Information and Computation', u'International Standard Book Number', u'John Cocke', u'Journal of Computer and System Sciences', u'Journal of the ACM', u'Landau symbol', u'Leslie Valiant', u'Matrix multiplication algorithm', u'Michael Sipser', u'New York University', u'Packrat parser', u'Parse tree', u'Parser', u'Parsing', u'Pseudocode', u'Recognizer', u'Stochastic context-free grammar', u'Tadao Kasami', u'The Art of Computer Programming', u'Weighted context-free grammar']"
Cache algorithms,"In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions—​or algorithms—​that a computer program or a hardware-maintained structure can follow in order to manage a cache of information stored on the computer. When the cache is full, the algorithm must choose which items to discard to make room for the new ones.","[u'Cache (computing)', u'Memory management algorithms', u'Use dmy dates from August 2012']","[u'ARM architecture', u'Adaptive Replacement Cache', u'Algorithm', u'Benchmark (computing)', u'CPU cache', u'CPU caches', u'Cache-oblivious algorithm', u'Cache (computing)', u'Cache coherency', u'Cache pollution', u'Clock with Adaptive Replacement', u'Computer (magazine)', u'Computer program', u'Computing', u'Digital object identifier', u'Distributed cache', u'International Standard Book Number', u'LIRS caching algorithm', u'Least-Frequently Used', u'Locality of reference', u'L\xe1szl\xf3 B\xe9l\xe1dy', u'Multi Queue (MQ) caching algorithm', u'Nimrod Megiddo', u'Optimization (computer science)', u'Page replacement algorithm', u'Pseudo-LRU', u'VLDB', u'Yuanyuan Zhou']"
Cannon's algorithm,"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.
It is especially suitable for computers laid out in an N × N mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.
The main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.
The Scalable Universal Matrix Multiplication Algorithm (SUMMA) is a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries.","[u'All stub articles', u'Applied mathematics stubs', u'Distributed algorithms', u'Matrix multiplication algorithms']","[u'Algorithm for matrix multiplication', u'Applied mathematics', u'Basic Linear Algebra Subprograms', u'CPU cache', u'Cache-oblivious algorithm', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Computer science', u'Distributed algorithm', u'Floating point', u'Lynn Elliot Cannon', u'Matrix decomposition', u'Matrix multiplication', u'Matrix multiplication algorithm', u'Mesh networking', u'Multiprocessing', u'Numerical linear algebra', u'Numerical stability', u'PLAPACK', u'SIMD', u'ScaLAPACK', u'Sparse matrix', u'System of linear equations', u'Systolic array', u'Translation lookaside buffer']"
Canopy clustering algorithm,"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.
The algorithm proceeds as follows, using two thresholds  (the loose distance) and  (the tight distance), where  .
Begin with the set of data points to be clustered.
Remove a point from the set, beginning a new 'canopy'.
For each point left in the set, assign it to the new canopy if the distance less than the loose distance .
If the distance of the point is additionally less than the tight distance , remove it from the original set.
Repeat from step 2 until there are no more data points in the set to cluster.
These relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm.
An important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4.
Since the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the curse of dimensionality. Only when a cheap and approximative – low-dimensional – distance function is available, the produced canopies will preserve the clusters produced by K-means.

","[u'Algorithms and data structures stubs', u'All articles with dead external links', u'All stub articles', u'Articles with dead external links from September 2015', u'Computer science stubs', u'Data clustering algorithms', u'Statistical algorithms']","[u'Algorithm', u'Andrew McCallum', u'Computer cluster', u'Curse of dimensionality', u'Data clustering', u'Data set', u'Data structure', u'Digital object identifier', u'Hierarchical clustering', u'K-means algorithm']"
Chaitin's algorithm,"Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric. It is named after its designer, Gregory Chaitin. Chaitin's algorithm was the first register allocation algorithm that made use of coloring of the interference graph for both register allocations and spilling.
Chaitin's algorithm was presented on the 1982 SIGPLAN Symposium on Compiler Construction, and published in the symposium proceedings. It was extension of an earlier 1981 paper on the use of graph coloring for register allocation. Chaitin's algorithm formed the basis of a large section of research into register allocators.

",[u'Graph algorithms'],"[u'Algorithm', u'Graph coloring', u'Gregory Chaitin', u'Interference graph', u'Register allocation', u'SIGPLAN', u'Spill metric']"
Chakravala method,"The chakravala method (Sanskrit: चक्रवाल विधि) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation. It is commonly attributed to Bhāskara II, (c. 1114 – 1185 CE) although some attribute it to Jayadeva (c. 950 ~ 1000 CE). Jayadeva pointed out that Brahmagupta's approach to solving equations of this type could be generalized, and he then described this general method, which was later refined by Bhāskara II in his Bijaganita treatise. He called it the Chakravala method: chakra meaning ""wheel"" in Sanskrit, a reference to the cyclic nature of the algorithm. E. O. Selenius held that no European performances at the time of Bhāskara, nor much later, exceeded its marvellous height of mathematical complexity.
This method is also known as the cyclic method and contains traces of mathematical induction.","[u'Articles containing Sanskrit-language text', u'Brahmagupta', u'Diophantine equations', u'Indian mathematics', u'Number theoretic algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algebra', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u""Bhaskara's lemma"", u'Bh\u0101skara II', u'Bijaganita', u'Binary GCD algorithm', u'Brahmagupta', u""Brahmagupta's identity"", u""Cipolla's algorithm"", u'Continued fraction', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Discrete logarithm', u""Dixon's factorization method"", u'Edmund F. Robertson', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Europe', u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Florian Cajori', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Hermann Hankel', u'Indeterminate equation', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Jayadeva (mathematician)', u""John J. O'Connor (mathematician)"", u'John Stillwell', u'Karatsuba algorithm', u'Lagrange', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MacTutor History of Mathematics archive', u'Mathematical induction', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pell's equation"", u'Pierre de Fermat', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic equation', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u'Sanskrit', u'Sanskrit language', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Springer Science+Business Media', u'Square root', u'The American Mathematical Monthly', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'University of St Andrews', u'Wheel factorization', u'William Brouncker, 2nd Viscount Brouncker', u""Williams' p + 1 algorithm""]"
Chan's algorithm,"In computational geometry, Chan's algorithm, named after Timothy M. Chan, is an optimal output-sensitive algorithm to compute the convex hull of a set P of n points, in 2- or 3-dimensional space. The algorithm takes O(n log h) time, where h is the number of vertices of the output (the convex hull). In the planar case, the algorithm combines an O(n log n) algorithm (Graham scan, for example) with Jarvis march, in order to obtain an optimal O(n log h) time. Chan's algorithm is notable because it is much simpler than the Kirkpatrick–Seidel algorithm, and it naturally extends to 3-dimensional space. This paradigm has been independently developed by Frank Nielsen in his Ph. D. thesis.",[u'Convex hull algorithms'],"[u'Binary search', u'Computational Geometry', u'Computational geometry', u'Convex hull', u'Discrete and Computational Geometry', u'Gift wrapping algorithm', u'Graham scan', u'Jarvis march', u'Kirkpatrick\u2013Seidel algorithm', u'Output-sensitive algorithm', u'Timothy M. Chan', u'Trapezoid']"
Christofides algorithm,"The goal of the Christofides approximation algorithm (named after Nicos Christofides) is to find a solution to the instances of the traveling salesman problem where the edge weights satisfy the triangle inequality. Let  be an instance of TSP, i.e.  is a complete graph on the set  of vertices with weight function  assigning a nonnegative real weight to every edge of .

","[u'Approximation algorithms', u'Graph algorithms', u'Spanning tree', u'Travelling salesman problem']","[u'Approximation algorithm', u'Complete graph', u'Degree (graph theory)', u'Eulerian circuit', u'Eulerian path', u'Hamiltonian circuit', u'Minimum spanning tree', u'Multigraph', u'Perfect matching', u'Pseudo-code', u'Traveling salesman problem', u'Triangle inequality']"
Clock with Adaptive Replacement,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated. Paging happens when a page fault occurs and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.
When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.
The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known.","[u'All Wikipedia articles needing clarification', u'All articles with unsourced statements', u'Articles with unsourced statements from June 2008', u'Memory management algorithms', u'Online algorithms', u'Use dmy dates from August 2012', u'Virtual memory', u'Wikipedia articles needing clarification from August 2011', u'Wikipedia articles needing clarification from January 2014']","[u'ARM architecture', u'Adaptive Replacement Cache', u'Adaptive replacement cache', u'Amortized analysis', u""B\xe9l\xe1dy's anomaly"", u'Cache algorithms', u'Clairvoyance', u'Computer', u'Demand paging', u'Digital object identifier', u'Dirty bit', u'FreeBSD', u'Full duplex', u'Garbage collection (computer science)', u'Hash table', u'Intel i860', u'Journaling file system', u'Kernel (computer science)', u'Linux', u'Linux kernel', u'Locality of reference', u'L\xe1szl\xf3 B\xe9l\xe1dy', u'Memory management', u'Memory management (operating systems)', u'OS/390', u'Object-oriented programming', u'Online algorithm', u'Online problem', u'OpenVMS', u'Operating system', u'Page fault', u'Page replacement algorithm', u'Paging', u'Prefetch input queue', u'Scheduling (computing)', u'Solaris (operating system)', u'The LIRS caching algorithm', u'Tree data structure', u'VAX', u'VAX/VMS', u'Virtual memory', u'Working set']"
Closest pair problem,"The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. The closest pair problem for points in the Euclidean plane was among the first geometric problems which were treated at the origins of the systematic study of the computational complexity of geometric algorithms.
A naive algorithm of finding distances between all pairs of points and selecting the minimum requires O(dn2) time. It turns out that the problem may be solved in O(n log n) time in a Euclidean space or Lp space of fixed dimension d. In the algebraic decision tree model of computation, the O(n log n) algorithm is optimal. The optimality follows from the observation that the element uniqueness problem (with the lower bound of Ω(n log n) for time complexity) is reducible to the closest pair problem: checking whether the minimal distance is 0 after the solving of the closest pair problem answers the question whether there are two coinciding points.
In the computational model which assumes that the floor function is computable in constant time the problem can be solved in O(n log log n) time. If we allow randomization to be used together with the floor function, the problem can be solved in O(n) time.","[u'All articles with unsourced statements', u'Articles with example pseudocode', u'Articles with unsourced statements from October 2015', u'Geometric algorithms']","[u'Algebraic decision tree', u'Analysis of algorithms', u'Big O notation', u'Bounding box', u'Brute-force search', u'Charles E. Leiserson', u'Clifford Stein', u'Computational geometry', u'Dan Hoey', u'Data structure', u'Delaunay triangulation', u'Divide and conquer (algorithm)', u'Dynamic problem (algorithms)', u'Element uniqueness problem', u'Euclidean space', u'Floor function', u'GIS', u'IEEE', u'Introduction to Algorithms', u'Lp space', u'Master theorem', u'Metric space', u'Michael Ian Shamos', u'Model of computation', u'Nearest neighbor search', u'Recursion', u'Ronald L. Rivest', u'SIAM J. Comput.', u'Set (abstract data type)', u'Set cover problem', u'Symposium on Foundations of Computer Science', u'Thomas H. Cormen', u'Voronoi diagram']"
Cocktail sort,"Cocktail sort, also known as bidirectional bubble sort, cocktail shaker sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list. This sorting algorithm is only marginally more difficult to implement than a bubble sort, and solves the problem of turtles in bubble sorts. It provides only marginal performance improvements, and does not improve asymptotic performance; like the bubble sort, it is not of practical interest (insertion sort is preferred for simple sorts), though it finds some use in education.","[u'Articles with example pseudocode', u'Comparison sorts', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'The Art of Computer Programming', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Comb sort,Comb sort is a relatively simple sorting algorithm originally designed by Włodzimierz Dobosiewicz in 1980. Later it was rediscovered by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort.,"[u'All articles needing additional references', u'Articles needing additional references from March 2011', u'Articles with example pseudocode', u'Comparison sorts', u'Sorting algorithms']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bubblesort', u'Bucket sort', u'Burstsort', u'Byte Magazine', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Digital object identifier', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Information Processing Letters', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shell sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Swap (computer science)', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Cone algorithm,"In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles. Its applications include computational surface science and computational nano science. The cone algorithm was first described in a publication about nanogold in 2005.
The cone algorithm works well with clusters in condensed phases, including solid and liquid phases. It can handle the situations when one configuration includes multiple clusters or when holes exist inside clusters. It can also be applied to a cluster iteratively to identify multiple sub-surface layers.

","[u'Geometric algorithms', u'Molecular modelling software']","[u'Algorithm', u'Computational geometry', u'Digital object identifier', u'Nanogold', u'Nanotechnology', u'Surface science']"
Cone tracing,"Cone tracing and beam tracing are a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with thick rays.

","[u'Computer graphics', u'Global illumination algorithms']","[u'Airy disc', u'Algorithm', u'Aliasing', u'Beam tracing', u'Computer graphics', u'Depth of field', u'Distributed ray tracing', u'Focal plane', u'Gaussian filter', u'Global illumination', u'Image noise', u'Lanczos resampling', u'Light transport theory', u'Mipmap', u'Monte Carlo method', u'Nyquist\u2013Shannon sampling theorem', u'Pinhole camera', u'Point spread function', u'Ray tracing (graphics)', u'Rectangular function', u'Sample (signal)', u'Sinc function', u'Solid angle', u'Sparse voxel octree']"
Congruence of squares,"In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms.","[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Integer factorization algorithms', u'Modular arithmetic']","[u'Congruence relation', u'Continued fraction factorization', u""Dixon's factorization method"", u'Equation', u'Euclidean algorithm', u'Factor base', u""Fermat's factorization method"", u'General number field sieve', u'Greatest common divisor', u'Integer', u'Integer factorization', u'Number theory', u'Quadratic sieve']"
Context tree weighting,"The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004). The CTW algorithm is an “ensemble method,” mixing the predictions of many underlying variable order Markov models, where each such model is constructed using zero-order conditional probability estimators.","[u'All stub articles', u'Computer science stubs', u'Lossless compression algorithms']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Computer science', u'Constant bitrate', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Journal of Artificial Intelligence Research', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Markov model', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
Convex hull algorithms,"Algorithms that construct convex hulls of various objects have a broad range of applications in mathematics and computer science.
In computational geometry, numerous algorithms are proposed for computing the convex hull of a finite set of points, with various computational complexities.
Computing the convex hull means that a non-ambiguous and efficient representation of the required convex shape is constructed. The complexity of the corresponding algorithms is usually estimated in terms of n, the number of input points, and h, the number of points on the convex hull.",[u'Convex hull algorithms'],"[u'Algebraic decision tree', u'Analysis of algorithms', u'Big O notation', u'CGAL', u""Chan's algorithm"", u'Charles E. Leiserson', u'Clifford Stein', u'Computational geometry', u'Computer science', u'Convex function', u'Convex hull', u'Convex polygon', u'Convex polyhedron', u'Convex polytope', u'Data structure', u'David Avis', u'David G. Kirkpatrick', u'David Mount', u'Decision tree model', u'Digital object identifier', u'Dynamic convex hull', u'Eric W. Weisstein', u'Face (geometry)', u'Franco P. Preparata', u'G. T. Toussaint', u'Gift wrapping algorithm', u'Godfried Toussaint', u'Graham scan', u'Half-space (geometry)', u'Integer sorting', u'International Standard Book Number', u'Introduction to Algorithms', u'Kirkpatrick\u2013Seidel algorithm', u'Linear time', u'Luc Devroye', u'Marc van Kreveld', u'Mark Overmars', u'Mark de Berg', u'MathWorld', u'Mathematics', u'Orthogonal convex hull', u'Otfried Schwarzkopf', u'Output-sensitive algorithm', u'Parabola', u'Quadrilateral', u'Quickhull', u'Quicksort', u'Raimund Seidel', u'Reduction (complexity)', u'Ronald Graham', u'Ronald L. Rivest', u'S.J. Hong', u'Selim Akl', u'Simple polygon', u'Sorting', u'Springer-Verlag', u'Thomas H. Cormen', u'Timothy M. Chan', u'Ultimate convex hull algorithm']"
Cooley–Tukey FFT algorithm,"The Cooley–Tukey algorithm, named after J.W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm. It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size N = N1N2 in terms of smaller DFTs of sizes N1 and N2, recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers). Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.
Because the Cooley-Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT. For example, Rader's or Bluestein's algorithm can be used to handle large prime factors that cannot be decomposed by Cooley–Tukey, or the prime-factor algorithm can be exploited for greater efficiency in separating out relatively prime factors.
The algorithm, along with its recursive application, was invented by Carl Friedrich Gauss. Cooley and Tukey independently rediscovered and popularized it 160 years later.","[u'Articles with example pseudocode', u'FFT algorithms']","[u'2 Pallas', u'3 Juno', u'Adding machine', u'Amortize', u'Array data structure', u'Asteroid', u'Binary numeral system', u'Bit-reversal permutation', u""Bluestein's FFT algorithm"", u'Breadth-first', u'Breadth-first search', u'Butterfly (FFT algorithm)', u'Butterfly diagram', u'C. Sidney Burrus', u'CPU cache', u'CPU pipeline', u'Cache-oblivious algorithm', u'Cache (computing)', u'Carl David Tolm\xe9 Runge', u'Carl Friedrich Gauss', u'Chinese Remainder Theorem', u'Column-major order', u'Composite number', u'Computer', u'Cornelius Lanczos', u'Dataflow diagram', u'Depth-first', u'Depth-first search', u'Digital object identifier', u'Discrete Fourier transform', u'Divide and conquer algorithm', u'E (mathematical constant)', u'Fast Fourier transform', u'Floating point', u'G. C. Danielson', u'GNU General Public License', u'Helium-3', u'IBM 7094', u'In-place algorithm', u'International Business Machines', u'International Standard Book Number', u'JSTOR', u'James Cooley', u'John Tukey', u'Lemma (mathematics)', u'Linearithmic', u'Mathematics of Computation', u'Memory locality', u'New Latin', u'Nuclear testing', u'Out-of-core', u'Permutation', u'Power of two', u'Prime-factor FFT algorithm', u'Princeton University', u'Processor register', u'Pseudocode', u""Rader's FFT algorithm"", u'Recursion', u'Relatively prime', u'Richard Garwin', u'Roots of unity', u'Row-major order', u'SIMD', u'Smooth number', u'Soviet Union', u'Split-radix FFT algorithm', u'Stride of an array', u'Transpose', u'Twiddle factor']"
Coppersmith–Winograd algorithm,"In linear algebra, the Coppersmith–Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known algorithm for square matrix multiplication until 2010. It can multiply two  matrices in  time  (see Big O notation). This is an improvement over the naïve  time algorithm and the  time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical. It is possible to improve the exponent further; however, the exponent must be at least 2 (because an  matrix has  values, and all of them have to be read at least once to calculate the exact result).
In 2010, Andrew Stothers gave an improvement to the algorithm,  In 2011, Virginia Williams combined a mathematical short-cut from Stothers' paper with her own insights and automated optimization on computers, improving the bound to  In 2014, François Le Gall simplified the methods of Williams and obtained an improved bound of 
The Coppersmith–Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.
Henry Cohn, Robert Kleinberg, Balázs Szegedy and Chris Umans have re-derived the Coppersmith–Winograd algorithm using a group-theoretic construction. They also showed that either of two different conjectures would imply that the optimal exponent of matrix multiplication is 2, as has long been suspected. However, they were not able to formulate a specific solution leading to a better running-time than Coppersmith-Winograd at the time.

","[u'Matrix multiplication algorithms', u'Matrix theory', u'Numerical linear algebra', u'Use dmy dates from July 2013']","[u'Algorithm', u'ArXiv', u'Bal\xe1zs Szegedy', u'Basic Linear Algebra Subprograms', u'Big O notation', u'CPU cache', u'Cache-oblivious algorithm', u'Chris Umans', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Computational complexity of mathematical operations', u'Digital object identifier', u'Don Coppersmith', u'Floating point', u'Gauss\u2013Jordan elimination', u'Group theory', u'Henry Cohn', u'ISSAC', u'International Standard Book Number', u'Linear algebra', u'Matrix decomposition', u'Matrix multiplication', u'Matrix multiplication algorithm', u'Multiprocessing', u'Numerical linear algebra', u'Numerical stability', u'Robert Kleinberg', u'SIMD', u'Shmuel Winograd', u'Sparse matrix', u'Strassen algorithm', u'System of linear equations', u'Translation lookaside buffer']"
Counting sort,"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, radix sort, that can handle larger keys more efficiently.
Because counting sort uses key values as indexes into an array, it is not a comparison sort, and the Ω(n log n) lower bound for comparison sorting does not apply to it. Bucket sort may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires linked lists, dynamic arrays or a large amount of preallocated memory to hold the sets of items within each bucket, whereas counting sort instead stores a single number (the count of items) per bucket.","[u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Big O notation', u'Bit vector', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Collection (computing)', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Cycle sort', u'Digital object identifier', u'Donald Knuth', u'Dynamic array', u'Flashsort', u'Gnome sort', u'Guy Blelloch', u'Harold H. Seward', u'Heapsort', u'Histogram', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer', u'Integer sorting', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'JSort', u'John Reif', u'Library sort', u'Linked list', u'List (computing)', u'Lower bound', u'MIT Press', u'Massachusetts Institute of Technology', u'McGraw-Hill', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel algorithm', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Prefix sum', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Robert Sedgewick (computer scientist)', u'Ron Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'Symposium on Foundations of Computer Science', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Cristian's algorithm,"Cristian's Algorithm (introduced by Flaviu Cristian in 1989) is a method for clock synchronization which can be used in many fields of distributive computer science but is primarily used in low-latency intranets. Cristian observed that this simple algorithm is probabilistic, in that it only achieves synchronization if the round-trip time (RTT) of the request is short compared to required accuracy. It also suffers in implementations using a single server, making it unsuitable for many distributive applications where redundancy may be crucial.

","[u'Distributed algorithms', u'Synchronization']","[u'Allan variance', u'Berkeley algorithm', u'Clock synchronization', u'DAYTIME', u'Digital object identifier', u'ICMP Timestamp', u'ICMP Timestamp Reply', u'International Atomic Time', u'Intranets', u'NTP pool', u'NTP server misuse and abuse', u'Ntpd', u'Ntpdate', u'OpenNTPD', u'Precision Time Protocol', u'Round-trip time', u'Synchronization', u'TIME protocol', u'Time server', u'UTC']"
Cuthill–McKee algorithm,"In the mathematical subfield of matrix theory, the Cuthill–McKee algorithm (CM), named for Elizabeth Cuthill and J. McKee , is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth. The reverse Cuthill–McKee algorithm (RCM) due to Alan George is the same algorithm but with the resulting index numbers reversed. In practice this generally results in less fill-in than the CM ordering when Gaussian elimination is applied.
The Cuthill McKee algorithm is a variant of the standard breadth-first search algorithm used in graph algorithms. It starts with a peripheral node and then generates levels  for  until all nodes are exhausted. The set  is created from set  by listing all vertices adjacent to all nodes in . These nodes are listed in increasing degree. This last detail is the only difference with the breadth-first search algorithm.

","[u'Graph algorithms', u'Matrix theory', u'Sparse matrices']","[u'Adjacency matrix', u'Algorithm', u'Association for Computing Machinery', u'Band matrix', u'Bandwidth (matrix theory)', u'Boost C++ Libraries', u'Breadth-first search', u'Degree (graph theory)', u'Graph (mathematics)', u'Graph bandwidth', u'Level structure', u'Mathematics', u'Matrix (mathematics)', u'N-tuple', u'Peripheral vertex', u'Sparse matrix', u'Symmetric matrix', u'Vertex (graph theory)']"
Cycle detection,"In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values.
For any function ƒ that maps a finite set S to itself, and any initial value x0 in S, the sequence of iterated function values

must eventually use the same value twice: there must be some i ≠ j such that xi = xj. Once this happens, the sequence must continue periodically, by repeating the same sequence of values from xi to xj−1. Cycle detection is the problem of finding i and j, given ƒ and x0.

","[u'Articles with example Python code', u'Combinatorial algorithms', u'Fixed points (mathematics)']","[u'Abelian group', u'Alan Sherman', u'Algorithm', u'Andrew Yao', u'Average case analysis', u'Burt Kaliski', u'Celestial mechanics', u'Cellular automaton', u'Claus P. Schnorr', u'Common Lisp', u'Computational group theory', u'Computer program', u'Computer science', u'Computer simulation', u'Cryptographic hash function', u'Cryptography', u'Cycle detection (graph theory)', u'Cycle graph', u'Data Encryption Standard', u'Data structure', u'Digital object identifier', u'Directed graph', u'Discrete logarithm', u'Donald Knuth', u'Eric Allender', u'Faith Ellen', u'Finite set', u'Function (mathematics)', u'Functional graph', u'Glossary of graph theory', u'Graph theory', u'Greatest common divisor', u'Hash collision', u'Hash table', u'Hendrik Lenstra', u'Infinite loop', u'Integer factorization', u'Iterated function', u'JSTOR', u'Linear congruential generator', u'Linked list', u'Link\xf6ping University', u'Maria Klawe', u'Mathematical folklore', u'Number theory', u'Oscillator (cellular automaton)', u'Periodic sequence', u'Phase space', u'Pointer algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's rho algorithm"", u'Power of two', u'Pseudorandom number generator', u'Python (programming language)', u'Reachability', u'Rho (letter)', u'Richard Brent (scientist)', u'Robert Sedgewick (computer scientist)', u'Robert W. Floyd', u'Ron Rivest', u'S-expression', u'Sequence', u'Shape analysis (software)', u'Space complexity', u'Stack (data structure)', u'The Tortoise and the Hare', u'William Kahan']"
Cycle sort,"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.
Unlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.
Minimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.","[u'Articles with example pseudocode', u'Comparison sorts', u'Online sorts', u'Sorting algorithms']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cyclic permutation', u'EEPROM', u'Flash memory', u'Flashsort', u'Gnome sort', u'Heapsort', u'Histogram', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'Linear search', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Perfect hash function', u'Permutation', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Time complexity', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
D*,"D* (pronounced ""D star"") is any one of the following three related incremental search algorithms:
The original D*, by Anthony Stentz, is an informed incremental search algorithm.
Focused D* is an informed incremental heuristic search algorithm by Anthony Stentz that combines ideas of A* and the original D*. Focused D* resulted from a further development of the original D*.
D* Lite is an incremental heuristic search algorithm by Sven Koenig and Maxim Likhachev that builds on LPA*, an incremental heuristic search algorithm that combines ideas of A* and Dynamic SWSF-FP.
All three search algorithms solve the same assumption-based path planning problems, including planning with the freespace assumption, where a robot has to navigate to given goal coordinates in unknown terrain. It makes assumptions about the unknown part of the terrain (for example: that it contains no obstacles) and finds a shortest path from its current coordinates to the goal coordinates under these assumptions. The robot then follows the path. When it observes new map information (such as previously unknown obstacles), it adds the information to its map and, if necessary, replans a new shortest path from its current coordinates to the given goal coordinates. It repeats the process until it reaches the goal coordinates or determines that the goal coordinates cannot be reached. When traversing unknown terrain, new obstacles may be discovered frequently, so this replanning needs to be fast. Incremental (heuristic) search algorithms speed up searches for sequences of similar search problems by using experience with the previous problems to speed up the search for the current one. Assuming the goal coordinates do not change, all three search algorithms are more efficient than repeated A* searches.
D* and its variants have been widely used for mobile robot and autonomous vehicle navigation. Current systems are typically based on D* Lite rather than the original D* or Focused D*. In fact, even Stentz's lab uses D* Lite rather than D* in some implementations. Such navigation systems include a prototype system tested on the Mars rovers Opportunity and Spirit and the navigation system of the winning entry in the DARPA Urban Challenge, both developed at Carnegie Mellon University.
The original D* was introduced by Anthony Stentz in 1994. The name D* comes from the term ""Dynamic A*"", because the algorithm behaves like A* except that the arc costs can change as the algorithm runs.","[u'Graph algorithms', u'Robot control', u'Search algorithms']","[u'A*', u'A* search algorithm', u'Alpha\u2013beta pruning', u'Autonomous vehicle', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Carnegie Mellon University', u'CiteSeer', u'D-STAR', u'DARPA Urban Challenge', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph traversal', u'Hill climbing', u'Incremental heuristic search', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Mobile robot', u'Navigation research', u'Opportunity rover', u""Prim's algorithm"", u'SMA*', u'Search game', u'Specific detectivity', u'Spirit rover', u'Sven Koenig (computer scientist)', u'Tree traversal']"
DBSCAN,"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996. It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.
In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD.",[u'Data clustering algorithms'],"[u'AAAI Press', u'Anomaly detection', u'Apache Commons', u'Artificial neural network', u'Association for Computing Machinery', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Ball tree', u'Bayesian network', u'Bias-variance dilemma', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Canonical correlation analysis', u'CiteSeer', u'Cluster analysis', u'Clustering high-dimensional data', u'Computational learning theory', u'Conditional random field', u'Convolutional neural network', u'Curse of dimensionality', u'Data clustering', u'Data mining', u'Decision tree learning', u'Deep learning', u'Digital object identifier', u'Dimensionality reduction', u'ELKI', u'Empirical risk minimization', u'Ensemble learning', u'Euclidean distance', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Fixed-radius near neighbors', u'Grammar induction', u'Graphical model', u'Hans-Peter Kriegel', u'Hidden Markov model', u'Hierarchical clustering', u'Independent component analysis', u'Inlining', u'International Standard Book Number', u'K-distance graph', u'K-means algorithm', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Kd-tree', u'Learning to rank', u'Linear discriminant analysis', u'Linear regression', u'Local outlier factor', u'Logistic regression', u'Machine learning', u'Mean-shift', u'Metric (mathematics)', u'Minkowski distance', u'Multilayer perceptron', u'Naive Bayes classifier', u'Non-negative matrix factorization', u'OPTICS algorithm', u'Online machine learning', u'Perceptron', u'PreDeCon', u'Principal component analysis', u'Probably approximately correct learning', u'Pseudocode', u'R* tree', u'R (programming language)', u'Random forest', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'SUBCLU', u'Scikit-learn', u'Self-organizing map', u'Semi-supervised learning', u'Spatial index', u'Springer-Verlag', u'Statistical classification', u'Statistical learning theory', u'Structured prediction', u'Supervised learning', u'Support vector machine', u'T-distributed stochastic neighbor embedding', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Weka (machine learning)']"
DEFLATE (algorithm),"In computing, deflate is a data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding. It was originally defined by Phil Katz for version 2 of his PKZIP archiving tool and was later specified in RFC 1951.
The original algorithm as designed by Katz was patented as U.S. Patent 5,051,745 and assigned to PKWARE, Inc. As stated in the RFC document, Deflate is widely thought to be implementable in a manner not covered by patents. This has led to its widespread use, for example in gzip compressed files, PNG image files and the .ZIP file format for which Katz originally designed it.","[u'All articles needing additional references', u'Articles needing additional references from January 2009', u'Lossless compression algorithms']","[u'.NET Compact Framework', u'.NET Framework', u'7-Zip', u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'AdvanceCOMP', u'Algebraic code-excited linear prediction', u'Algorithm', u'Android (operating system)', u'Apache HTTP Server', u'Apache license', u'Application-specific integrated circuit', u'Arithmetic coding', u'Assembly language', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'BSD license', u'Bit', u'Bit rate', u'Bitstream', u'Borland', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Bzip2', u'C++', u'C (programming language)', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Commodore 128', u'Commodore 64', u'Common Lisp', u'Companding', u'Comparison of file archivers', u'Compression artifact', u'Computing', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'Crypto++', u'Data compression', u'Debian Free Software Guidelines', u'Deblocking filter', u'DeflOpt', u'Deflate', u'Deflation (disambiguation)', u'Delta encoding', u'Device driver', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'FPGA', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'GIMP', u'GNU General Public License', u'GNU Lesser General Public License', u'Golomb coding', u'Gzip', u'Huffman coding', u'IOS', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'International Standard Book Number', u'Internet Engineering Task Force', u'Java (programming language)', u'Javascript (programming language)', u'Karhunen\u2013Lo\xe8ve theorem', u'Ken Silverman', u'Kolmogorov complexity', u'L. Peter Deutsch', u'LGPL', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Linux (kernel)', u'List of archive formats', u'List of file archivers', u'Load balancer', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Lua (programming language)', u'M68000', u'MIT License', u'MOS Technology 6502', u'MSX', u'Mac OS', u'Macroblock', u'Microsoft .NET Framework', u'Microsoft Windows', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Mono (software)', u'Motion compensation', u'Move-to-front transform', u'Multiple-image Network Graphics', u'Nyquist\u2013Shannon sampling theorem', u'OpenSolaris', u'PAQ', u'PCI-X', u'PCI Local Bus', u'PCIe', u'PDP-11 architecture', u'PKWARE, Inc.', u'PKZIP', u'PKZip', u'PNGOUT', u'Pascal (programming language)', u'Peak signal-to-noise ratio', u'Phil Katz', u'Pixel', u'Plan 9 from Bell Labs', u'Portable Network Graphics', u'Prediction by partial matching', u'Psychoacoustics', u'PuTTY', u'Public Domain', u'Pyramid (image processing)', u'Python (programming language)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Red Gate Software', u'Redundancy (information theory)', u'Reference (computer science)', u'Run-length encoding', u'SAM Coup\xe9', u'Sampling (signal processing)', u'Seed7', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Silverlight', u'Sliding scale', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Storage area network', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Virtex (FPGA)', u'Vulnerability (computing)', u'Warped linear predictive coding', u'Wavelet compression', u'Windows Phone', u'Xamarin', u'Xbox 360', u'Xilinx', u'Z80', u'ZIP (file format)', u'ZIP file format', u'Zlib', u'Zlib License', u'Zopfli', u'\u039c-law algorithm']"
Daitch–Mokotoff Soundex,"Daitch–Mokotoff Soundex (D–M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch. It is a refinement of the Russell and American Soundex algorithms designed to allow greater accuracy in matching of Slavic and Yiddish surnames with similar pronunciation but differences in spelling.
Daitch–Mokotoff Soundex is sometimes referred to as ""Jewish Soundex"" and ""Eastern European Soundex"", although the authors discourage use of these nicknames for the algorithm because the algorithm itself is independent of the fact the motivation for creating the new system was the poor results of predecessor systems when dealing with Slavic and Yiddish surnames.","[u'Genealogy', u'Phonetic algorithms']","[u'Alexander Beider', u'Avotaynu', u'Gary Mokotoff', u'Phonetic algorithm', u'Pronunciation', u'Randy Daitch', u'Slavic (language)', u'Soundex', u'Stephen P. Morse', u'Surname', u'Where Once We Walked', u'Yiddish']"
Damm algorithm,"In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors. It was presented by H. Michael Damm in 2004.

","[u'Algebraic structures', u'CS1 German-language sources (de)', u'CS1 errors: external links', u'Checksum algorithms', u'Group theory', u'Latin squares']","[u'Algorithm', u'Cayley table', u'Check digit', u'Digital object identifier', u'Error detection', u'Exponentiation', u'ISBN', u'International Standard Book Number', u'International Standard Serial Number', u'Inverse element', u'Latin square', u'Order (group theory)', u'Permutation', u'Quasigroup', u'Transcription error', u'Verhoeff algorithm', u'X']"
Dancing Links,"In computer science, dancing links, also known as DLX, is the technique suggested by Donald Knuth to efficiently implement his Algorithm X. Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem. Some of the better-known exact cover problems include tiling, the n queens problem, and Sudoku.
The name dancing links stems from the way the algorithm works, as iterations of the algorithm cause the links to ""dance"" with partner links so as to resemble an ""exquisitely choreographed dance."" Knuth credits Hiroshi Hitotsumatsu and Kōhei Noshita with having invented the idea in 1979, but it is his paper which has popularized it.","[u'Articles containing video clips', u'Donald Knuth', u'Linked lists', u'Search algorithms', u'Sudoku']","[u'-yllion', u'AMS Euler', u'Algorithm', u'Algorithm X', u'ArXiv', u'Backtracking', u'Big O notation', u'CWEB', u'Computer Modern', u'Computer science', u'Computers and Typesetting', u'Concrete Mathematics', u'Concrete Roman', u'Depth-first', u'Digital object identifier', u""Dijkstra's algorithm"", u'Donald Knuth', u'Doubly linked list', u'Eight queens puzzle', u'Exact cover', u'Fisher\u2013Yates shuffle', u'Font', u'GNU MIX Development Kit', u'Hadoop', u""Knuth's Algorithm X"", u""Knuth's Simpath algorithm"", u'Knuth Prize', u'Knuth reward check', u'Knuth\u2013Bendix completion algorithm', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Literate programming', u'METAFONT', u'MIX', u'MMIX', u'Man or boy test', u'MapReduce', u'Nondeterministic algorithm', u'Polycube', u'Potrzebie', u'Quater-imaginary base', u'Recursion (computer science)', u'Robinson\u2013Schensted\u2013Knuth correspondence', u'Selected papers series of Knuth', u'Software', u'Sparse matrix', u'Sudoku', u'Surreal Numbers (book)', u'TeX', u'Tessellation', u'The Art of Computer Programming', u'The Complexity of Songs', u'Things a Computer Scientist Rarely Talks About', u'Trabb Pardo\u2013Knuth algorithm', u'WEB']"
Dekker's algorithm,"Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming. The solution is attributed to Dutch mathematician Th. J. Dekker by Edsger W. Dijkstra in an unpublished paper on sequential process descriptions and his manuscript on cooperating sequential processes. It allows two threads to share a single-use resource without conflict, using only shared memory for communication.
It avoids the strict alternation of a naïve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented.","[u'All articles needing additional references', u'Articles needing additional references from May 2015', u'Concurrency control algorithms', u'Dutch inventions']","[u'Busy wait', u'Busy waiting', u'C++11', u'CPU', u'Concurrent programming', u'Critical section', u'Deadlock', u'Dutch people', u'Edsger W. Dijkstra', u'Eisenberg & McGuire algorithm', u'Infinite loop', u""Lamport's bakery algorithm"", u'Loop-invariant code motion', u'Mathematician', u'Memory barrier', u'Memory ordering', u'Mutual exclusion', u""Peterson's algorithm"", u'Pseudocode', u'Resource starvation', u'Semaphore (programming)', u'Shared memory (interprocess communication)', u'Symmetric multiprocessing', u""Szymanski's Algorithm"", u'Test-and-set', u'Theodorus Dekker', u'University of Texas at Austin', u'Volatile variable']"
Delayed column generation,"Column generation or delayed column generation is an efficient algorithm for solving larger linear programs.
The overarching idea is that many linear programs are too large to consider all the variables explicitly. Since most of the variables will be non-basic and assume a value of zero in the optimal solution, only a subset of variables need to be considered in theory when solving the problem. Column generation leverages this idea to generate only the variables which have the potential to improve the objective function—that is, to find variables with negative reduced cost (assuming without loss of generality that the problem is a minimization problem).
The problem being solved is split into two problems: the master problem and the subproblem. The master problem is the original problem with only a subset of variables being considered. The subproblem is a new problem created to identify a new variable. The objective function of the subproblem is the reduced cost of the new variable with respect to the current dual variables, and the constraints require that the variable obey the naturally occurring constraints.
The process works as follows. The master problem is solved—from this solution, we are able to obtain dual prices for each of the constraints in the master problem. This information is then utilized in the objective function of the subproblem. The subproblem is solved. If the objective value of the subproblem is negative, a variable with negative reduced cost has been identified. This variable is then added to the master problem, and the master problem is re-solved. Re-solving the master problem will generate a new set of dual values, and the process is repeated until no negative reduced cost variables are identified. The subproblem returns a solution with non-negative reduced cost, we can conclude that the solution to the master problem is optimal.
In many cases, this allows large linear programs that had been previously considered intractable to be solved. The classical example of a problem where this is successfully used is the cutting stock problem. One particular technique in linear programming which uses this kind of approach is the Dantzig–Wolfe decomposition algorithm. Additionally, column generation has been applied to many problems such as crew scheduling, vehicle routing, and the capacitated p-median problem.","[u'All stub articles', u'Applied mathematics stubs', u'Optimization algorithms and methods']","[u'Applied mathematics', u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Capacitated p-median problem', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Crew scheduling', u'Criss-cross algorithm', u'Cutting-plane method', u'Cutting stock problem', u'Dantzig\u2013Wolfe decomposition', u'Davidon\u2013Fletcher\u2013Powell formula', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'Iterative method', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Minimum spanning tree', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization (mathematics)', u'Optimization algorithm', u'Penalty method', u""Powell's method"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Reduced cost', u'Revised simplex algorithm', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Truncated Newton method', u'Trust region', u'Vehicle routing', u'Without loss of generality', u'Wolfe conditions']"
Depth-first search,"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root (selecting some arbitrary node as the root in the case of a graph) and explores as far as possible along each branch before backtracking.
A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.","[u'All articles needing additional references', u'Articles containing video clips', u'Articles needing additional references from July 2010', u'Articles with example pseudocode', u'Commons category with local link same as on Wikidata', u'Graph algorithms', u'Search algorithms']","[u'A* search algorithm', u'Algorithm', u'Alpha\u2013beta pruning', u'Analysis of algorithms', u'Artificial intelligence', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best, worst and average case', u'Best-first search', u'Bias', u'Biconnected graph', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branching factor', u'Breadth-first search', u'Bridge (graph theory)', u'British Museum algorithm', u'Charles E. Leiserson', u'Charles Pierre Tr\xe9maux', u'Clifford Stein', u'Connected component (graph theory)', u'Control flow graph', u'D*', u'Decision problem', u'Degree (graph theory)', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Directed acyclic graph', u'Donald Knuth', u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph (data structure)', u'Graph theory', u'Graph traversal', u'Group (mathematics)', u'Halting problem', u'Heuristics', u'Hill climbing', u'International Standard Book Number', u'Introduction to Algorithms', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Jean Pelletier-Thibert', u'John Hopcroft', u""Johnson's algorithm"", u'Jon Kleinberg', u'Journal of the ACM', u'Jump point search', u""Kruskal's algorithm"", u'Kurt Mehlhorn', u'Lexicographic breadth-first search', u'List of algorithms', u'Maze', u'Maze generation', u'Maze solving algorithm', u'Memory management', u'Michael T. Goodrich', u'OCLC', u'P-complete', u'Parallel algorithm', u'Parse tree', u'Patrice Ossona de Mendez', u'Peter Sanders (computer scientist)', u'Pierre Rosenstiehl', u'Planarity testing', u'Polish notation', u""Prim's algorithm"", u'Reverse Polish notation', u'Robert Tarjan', u'Roberto Tamassia', u'Ronald L. Rivest', u'SMA*', u'Sample (statistics)', u'Search algorithm', u'Search game', u'Search games', u'Shimon Even', u'Spanning tree (mathematics)', u'Strongly connected components', u'Thomas H. Cormen', u'Time complexity', u'Topological sorting', u'Tree (data structure)', u'Tree data structure', u'Tree traversal', u'Tr\xe9maux tree', u'\xc9va Tardos']"
Deutsch-Jozsa algorithm,"The Deutsch–Jozsa algorithm is a quantum algorithm, proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998. Although of little practical use, it is one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm. It is also a deterministic algorithm, meaning that it always produces an answer, and that answer is always correct.

",[u'Quantum algorithms'],"[u'Adiabatic quantum computation', u'Algorithmic cooling', u'ArXiv', u'Artur Ekert', u'BPP (complexity)', u'BQP', u'Bibcode', u'Cavity quantum electrodynamics', u'Charge qubit', u'Circuit quantum electrodynamics', u'Classical capacity', u'Cluster state', u'Constant function', u'Constructive interference', u'Controlled NOT gate', u'David Deutsch', u'Destructive interference', u'Deterministic', u'Deterministic algorithm', u'Digital object identifier', u'EQP (complexity)', u'Entanglement-assisted classical capacity', u'Entanglement-assisted stabilizer formalism', u'Entanglement distillation', u'Flux qubit', u'Function domain', u""Grover's algorithm"", u'Hadamard transform', u'Hadamard transformation', u'Kane quantum computer', u'LOCC', u'Linear optical quantum computing', u'Loss\u2013DiVincenzo quantum computer', u'Michele Mosca', u'Nitrogen-vacancy center', u'Nuclear magnetic resonance quantum computer', u'One-way quantum computer', u'Optical lattice', u'Oracle machine', u'Phase qubit', u'PostBQP', u'Promise problem', u'QMA', u'Quantum Fourier transform', u'Quantum Turing machine', u'Quantum algorithm', u'Quantum annealing', u'Quantum capacity', u'Quantum channel', u'Quantum circuit', u'Quantum complexity theory', u'Quantum computer', u'Quantum convolutional code', u'Quantum cryptography', u'Quantum decoherence', u'Quantum energy teleportation', u'Quantum error correction', u'Quantum gate', u'Quantum information', u'Quantum information science', u'Quantum key distribution', u'Quantum network', u'Quantum optics', u'Quantum phase estimation algorithm', u'Quantum programming', u'Quantum teleportation', u'Qubit', u'Randomized algorithm', u'Richard Cleve', u'Richard Jozsa', u""Shor's algorithm"", u""Simon's problem"", u'Spin (physics)', u'Stabilizer code', u'Superconducting quantum computing', u'Superdense coding', u'Symposium on Foundations of Computer Science', u'Symposium on Theory of Computing', u'Timeline of quantum computing', u'Topological quantum computer', u'Trapped ion quantum computer', u'Ultracold atom', u'Universal quantum simulator', u'XOR gate']"
Dictionary coder,"A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder. When the encoder finds such a match, it substitutes a reference to the string's position in the data structure.","[u'All articles needing additional references', u'Articles needing additional references from September 2014', u'Lossless compression algorithms']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Circular buffer', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Concordance (publishing)', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Data structure', u'Deblocking filter', u'Delta encoding', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 (algorithm)', u'LZ77 and LZ78', u'LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Mobile app', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Personal digital assistant', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'String (computer science)', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
Difference-map algorithm,"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.
Although originally conceived as a general method for solving the phase problem, the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, Ramsey numbers, diophantine equations, and Sudoku, as well as sphere- and disk-packing problems. Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.
The difference-map algorithm is a generalization of two iterative methods: Fienup's Hybrid input output (HIO) algorithm for phase retrieval  and the Douglas-Rachford algorithm for convex optimization. Iterative methods, in general, have a long history in phase retrieval and convex optimization. The use of this style of algorithm for hard, non-convex problems is a more recent development.","[u'Constraint programming', u'Search algorithms']","[u'2-SAT', u'3-SAT', u'Absolute value', u'Applied Optics', u'Boolean satisfiability problem', u'Chaos theory', u'Coherent light', u'Constraint (mathematics)', u'Constraint satisfaction', u'Convex optimization', u'Diophantine equations', u'Discrete Fourier transform', u'Douglas-Rachford algorithm', u'Dynamical system', u'Euclidean space', u'Fixed point (mathematics)', u'Fraunhofer diffraction', u'Hybrid input output (HIO) algorithm for phase retrieval', u'Incomplete algorithm', u'Iterative methods', u'Journal of the Optical Society of America A', u'Literal (mathematical logic)', u'Local search (optimization)', u'Map (mathematics)', u'Meta-algorithm', u'NP-complete', u'Phase problem', u'Physical Review', u'Proceedings of the National Academy of Sciences', u'Projection (linear algebra)', u'Protein structure prediction', u'Ramsey numbers', u'Search algorithm', u'Set intersection', u'Strange attractor', u'Sudoku', u'Support (mathematics)', u'Unitary transformation']"
Difference map algorithm,"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.
Although originally conceived as a general method for solving the phase problem, the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, Ramsey numbers, diophantine equations, and Sudoku, as well as sphere- and disk-packing problems. Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.
The difference-map algorithm is a generalization of two iterative methods: Fienup's Hybrid input output (HIO) algorithm for phase retrieval  and the Douglas-Rachford algorithm for convex optimization. Iterative methods, in general, have a long history in phase retrieval and convex optimization. The use of this style of algorithm for hard, non-convex problems is a more recent development.","[u'Constraint programming', u'Search algorithms']","[u'2-SAT', u'3-SAT', u'Absolute value', u'Applied Optics', u'Boolean satisfiability problem', u'Chaos theory', u'Coherent light', u'Constraint (mathematics)', u'Constraint satisfaction', u'Convex optimization', u'Diophantine equations', u'Discrete Fourier transform', u'Douglas-Rachford algorithm', u'Dynamical system', u'Euclidean space', u'Fixed point (mathematics)', u'Fraunhofer diffraction', u'Hybrid input output (HIO) algorithm for phase retrieval', u'Incomplete algorithm', u'Iterative methods', u'Journal of the Optical Society of America A', u'Literal (mathematical logic)', u'Local search (optimization)', u'Map (mathematics)', u'Meta-algorithm', u'NP-complete', u'Phase problem', u'Physical Review', u'Proceedings of the National Academy of Sciences', u'Projection (linear algebra)', u'Protein structure prediction', u'Ramsey numbers', u'Search algorithm', u'Set intersection', u'Strange attractor', u'Sudoku', u'Support (mathematics)', u'Unitary transformation']"
Digital Differential Analyzer (graphics algorithm),"In computer graphics, a digital differential analyzer (DDA) is hardware or software used for linear interpolation of variables over an interval between start and end point. DDAs are used for rasterization of lines, triangles and polygons. In its simplest implementation, the DDA algorithm interpolates values in interval by computing for each xi the equations xi = xi−1+1/m, yi = yi−1 + m, where Δx = xend − xstart and Δy = yend − ystart and m = Δy/Δx

","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from June 2011', u'Articles with example C code', u'Computer graphics algorithms', u'Digital geometry']","[u""Bresenham's line algorithm"", u'Computer graphics', u'Digital differential analyzer', u'Fixed-point arithmetic', u'Floating-point', u'Floating-point unit', u'Integer', u'Interval (mathematics)', u'Linear interpolation', u'Rasterization', u'Variable (computer science)', u""Xiaolin Wu's line algorithm""]"
Dijkstra's algorithm,"Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.
The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Dijkstra's original algorithm does not use a min-priority queue and runs in time  (where  is the number of nodes). The idea of this algorithm is also given in (Leyzorek et al. 1957). The implementation based on a min-priority queue implemented by a Fibonacci heap and running in  (where  is the number of edges) is due to (Fredman & Tarjan 1984). This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights.
In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search.","[u'1959 in computer science', u'Articles with example pseudocode', u'Combinatorial optimization', u'Commons category with local link same as on Wikidata', u'Dutch inventions', u'Graph algorithms', u'Routing algorithms', u'Search algorithms', u'Use dmy dates from February 2011']","[u'A* algorithm', u'A* search algorithm', u'A-star algorithm', u'Adjacency list', u'Admissible heuristic', u'Algorithm', u'Alpha\u2013beta pruning', u'Artificial Intelligence: A Modern Approach', u'Artificial intelligence', u'Asymptotic computational complexity', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u'Big O notation', u'Binary heap', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Brodal queue', u'Charles Babbage Institute', u'Charles E. Leiserson', u'Clifford Stein', u'Computer scientist', u'Consistent heuristic', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u'Directed acyclic graph', u'Directed graph', u'Donald Knuth', u'Dover Publications', u'Dual linear program', u""Dykstra's projection algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Edsger W. Dijkstra', u'Euclidean shortest path', u'Fast marching method', u'Fibonacci heap', u'Flood fill', u'Floyd\u2013Warshall algorithm', u'Francis & Taylor', u'Fringe search', u'Graph (abstract data type)', u'Graph (data structure)', u'Graph labeling', u'Graph traversal', u'Greedy algorithm', u'Hill climbing', u'IEEE', u'IS-IS', u'Information Processing Letters', u'International Standard Book Number', u'Intersection (road)', u'Introduction to Algorithms', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Kurt Mehlhorn', u'Lexicographic breadth-first search', u'Linear programming', u'Link-state routing protocol', u'List of algorithms', u'Longest path problem', u'MIT Press', u'McGraw\u2013Hill', u'Michael Fredman', u'Min-priority queue', u'Minimum spanning tree', u'Motion planning', u'Negative cycle', u'Neighbourhood (graph theory)', u'OSPF', u'PDF', u'Pairing heap', u'Peter Norvig', u'Peter Sanders (computer scientist)', u""Prim's algorithm"", u'Principle of Optimality', u'Priority queue', u'Probability distribution', u'Radix heap', u'Reduced cost', u'Richard Bellman', u'Robert Endre Tarjan', u'Robert Tarjan', u'Robotics', u'Ronald L. Rivest', u'Routing protocol', u'SMA*', u'Search algorithm', u'Search game', u'Self-balancing binary search tree', u'Shortest path problem', u'Shortest path tree', u'Sparse graph', u'Stuart J. Russell', u'Subroutine', u'Thomas H. Cormen', u'Time complexity', u'Transportation Science', u'Tree traversal', u'Vertex (graph theory)', u'Wavefront']"
Dijkstra-Scholten algorithm,"The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.
First, let us consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.","[u'Graph algorithms', u'Termination algorithms']","[u'Algorithm', u'Carel S. Scholten', u'Digital object identifier', u'Distributed system', u'Divide and conquer algorithm', u'Edsger W. Dijkstra', u""Huang's algorithm"", u'International Standard Book Number', u'Mathematical Reviews', u'Node (networking)', u'Process graph', u'Spanning tree (mathematics)', u'Termination analysis', u'Tree (data structure)']"
Dinic's algorithm,"Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network, conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz. The algorithm runs in  time and is similar to the Edmonds–Karp algorithm, which runs in  time, in that it uses shortest augmenting paths. The introduction of the concepts of the level graph and blocking flow enable Dinic's algorithm to achieve its performance.","[u'Graph algorithms', u'Network flow']","[u'Bipartite matching', u'Breadth-first search', u'Dynamic trees', u'Edmonds\u2013Karp algorithm', u'Flow network', u'Ford\u2013Fulkerson algorithm', u'Hopcroft\u2013Karp algorithm', u'International Standard Book Number', u'Maximum flow', u'Maximum flow problem', u'Oded Goldreich', u'Shimon Even', u'Strongly polynomial', u'Yefim Dinitz']"
Disk scheduling,"Input/output (I/O) scheduling is the method that computer operating systems use to decide in which order the block I/O operations will be submitted to storage volumes. I/O scheduling is sometimes called disk scheduling.
I/O scheduling usually has to work with hard disk drives that have long access times for requests placed far away from the current position of the disk head (this operation is called a seek). To minimize the effect this has on system performance, most I/O schedulers implement a variant of the elevator algorithm that reorders the incoming randomly ordered requests so the associated data would be accessed with minimal arm/head movement.
I/O schedulers can have many purposes depending on the goals; common purposes include the following:
To minimize time wasted by hard disk seeks
To prioritize a certain processes' I/O requests
To give a share of the disk bandwidth to each running process
To guarantee that certain requests will be issued before a particular deadline
Common scheduling disciplines include the following:
Random scheduling (RSS)
First In, First Out (FIFO), also known as First Come First Served (FCFS)
Last In, First Out (LIFO)
Shortest seek first, also known as Shortest Seek / Service Time First (SSTF)
Elevator algorithm, also known as SCAN (including its variants, C-SCAN, LOOK, and C-LOOK)
N-Step-SCAN SCAN of N records at a time
FSCAN, N-Step-SCAN where N equals queue size at start of the SCAN cycle
Completely Fair Queuing (CFQ) on Linux
Anticipatory scheduling
Noop scheduler
Deadline scheduler
mClock scheduler","[u'All articles needing additional references', u'Articles needing additional references from February 2013', u'Disk scheduling algorithms']","[u'Access Time', u'Anticipatory scheduling', u'CFQ', u'Computer', u'Deadline scheduler', u'Elevator algorithm', u'FIFO (computing and electronics)', u'FSCAN', u'Hard disk', u'Hard disk drive', u'Input/output', u'LIFO (computing)', u'Linux kernel', u'N-Step-SCAN', u'Native Command Queuing', u'Noop scheduler', u'Operating system', u'Process (computing)', u'Process management (computing)', u'Random scheduling', u'Robert Love', u'Scheduling (computing)', u'Shortest seek first', u'Tagged Command Queuing', u'Volume (computing)']"
Distributed algorithm,"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.
Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.

",[u'Distributed algorithms'],"[u'Algorithm', u'Atomic commit', u'Central processing unit', u'Computer hardware', u'Concurrency (computer science)', u'Consensus (computer science)', u'Distributed computing', u'Information processing', u'International Standard Book Number', u'Leader election', u'Morgan Kaufmann Publishers', u'Mutual exclusion', u'Non-blocking data structures', u'Parallel algorithm', u'Paxos algorithm', u'Process control', u'Replication (computer science)', u'Resource allocation', u'Scientific computing', u'Search algorithm', u'Spanning tree', u'Spanning tree (mathematics)', u'Telecommunications', u'Three-phase commit protocol', u'Two-phase commit protocol', u'Vertex coloring']"
Divide and conquer algorithm,"In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.
This divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).
Understanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.
The correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.","[u'Algorithms', u'Operations research', u'Optimization algorithms and methods', u'Pages with citations lacking titles']","[u'Akra\u2013Bazzi method', u'Algorithm', u'Algorithm design', u'Analysis of algorithms', u'Anatolii Alexeevitch Karatsuba', u'Andrey Kolmogorov', u""Arm's-length recursion"", u'Asymptotic complexity', u'Babylonia', u'Big O notation', u'Binary search', u'Bisection algorithm', u'Bottom-up design', u'Branch and bound', u'Breadth first recursion', u'Cache-oblivious algorithm', u'Call stack', u'Carl Friedrich Gauss', u'Chart parsing', u'Computer science', u'Conditional (programming)', u'Cooley-Tukey FFT algorithm', u'Digital object identifier', u'Discrete Fourier transform', u'Divide and rule', u'Doklady Akademii Nauk SSSR', u'Donald Knuth', u'Dynamic programming', u'Euclidean algorithm', u'Fast Fourier transform', u'Fibonacci number', u'Floating point', u'Fork\u2013join model', u'Geometric series', u'Greatest common divisor', u'Heuristic (computer science)', u'Hybrid algorithm', u'IBM 80 series Card Sorters', u'Insertion sort', u'International Standard Book Number', u'John Mauchly', u'John von Neumann', u'Karatsuba algorithm', u'Loop (computing)', u'Loop nest optimization', u'Loop unwinding', u'MapReduce', u'Master theorem', u'Mathematical induction', u'Memoization', u'Memory cache', u'Merge sort', u'Multiplication algorithm', u'Non-Uniform Memory Access', u'Numerical algorithm', u'Pairwise summation', u'Paradigm', u'Partial evaluation', u'Post office', u'Priority queue', u'Prune and search', u'Queue (data structure)', u'Quicksort', u'Radix sort', u'Recurrence relation', u'Recursion', u'Recursion (computer science)', u'Root-finding algorithm', u'Sorting algorithm', u'Source code generation', u'Stack (data structure)', u'Stack overflow', u'Strassen algorithm', u'Subroutine', u'Syntactic analysis', u'Tail recursion', u'The Art of Computer Programming', u'Theorem', u'Top-down parser', u'Tower of Hanoi', u'Virtual memory', u'Yuri Petrovich Ofman']"
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output.","[u'All articles to be expanded', u'All articles with unsourced statements', u'All pages needing factual verification', u'Articles to be expanded from September 2012', u'Articles with example pseudocode', u'Articles with unsourced statements from February 2012', u'Articles with unsourced statements from February 2014', u'Binary arithmetic', u'Computer arithmetic', u'Computer arithmetic algorithms', u'Division (mathematics)', u'Wikipedia articles needing clarification from July 2015', u'Wikipedia articles needing factual verification from June 2015']","[u'AMD', u'Algorithm', u'Analysis of algorithms', u'Approximation', u'Barrett reduction', u'Binomial theorem', u'Chunking (division)', u'Cryptography', u'Digital object identifier', u'Division (mathematics)', u'Double precision', u'Equioscillation theorem', u""Euclid's Elements"", u'Euclidean division', u'Extended precision', u'Fixed point arithmetic', u'Floating point', u'Fused multiply\u2013add', u'Greatest common divisor', u'Hexadecimal', u'Integer (computer science)', u'International Standard Book Number', u'Karatsuba algorithm', u'Long division', u'Lookup table', u'Microprocessor', u'Modular arithmetic', u'Multiplication algorithm', u'Multiplicative inverse', u""Newton's method"", u'OCLC', u'Original Intel Pentium (P5 microarchitecture)', u'Output-sensitive algorithm', u'Pentium FDIV bug', u'Power of two', u'Precision (computer science)', u'Quotient', u'Radix', u'Rate of convergence', u'Relative error', u'Remainder', u'Remez algorithm', u'Round-off error', u'Sch\xf6nhage\u2013Strassen algorithm', u'Short division', u'Single precision', u'Toom\u2013Cook multiplication']"
Dixon's algorithm,"In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) is a general-purpose integer factorization algorithm; it is the prototypical factor base method. Unlike for other factor base methods, its run-time bound comes with a rigorous proof that does not rely on conjectures about the smoothness properties of the values taken by polynomial.
The algorithm was designed by John D. Dixon, a mathematician at Carleton University, and was published in 1981.",[u'Integer factorization algorithms'],"[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big-O notation', u'Binary GCD algorithm', u'Block Lanczos algorithm for nullspace of a matrix over a finite field', u'Carleton University', u'Chakravala method', u""Cipolla's algorithm"", u'Congruence of squares', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Dickman\u2013de Bruijn function', u'Digital object identifier', u'Discrete logarithm', u'Elliptic curve primality', u'Elliptic curve primality proving', u""Euclid's algorithm"", u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u'Factor base', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'Gaussian elimination', u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'JSTOR', u'John D. Dixon', u'Karatsuba algorithm', u'L-notation', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Linear algebra', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Mathematics of Computation', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u'Pseudo-random', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Square number', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Doomsday algorithm,"The Doomsday rule or Doomsday algorithm is a way of calculating the day of the week of a given date. It provides a perpetual calendar because the Gregorian calendar moves in cycles of 400 years.
This algorithm for mental calculation was devised by John Conway after drawing inspiration from Lewis Carroll's work on a perpetual calendar algorithm. It takes advantage of each year having a certain day of the week (the doomsday) upon which certain easy-to-remember dates fall; for example, 4/4, 6/6, 8/8, 10/10, 12/12, and the last day of February all occur on the same day of the week in any given year. Applying the Doomsday algorithm involves three steps:
Determine the ""anchor day"" for the century.
Use the anchor day for the century to calculate the doomsday for the year.
Choose the closest date out of the ones that always fall on the doomsday (e.g. 4/4, 6/6, 8/8), and count the number of days (modulo 7) between that date and the date in question to arrive at the day of the week.
This technique applies to both the Gregorian calendar A.D. and the Julian calendar, although their doomsdays will usually be different days of the week.
Since this algorithm involves treating days of the week like numbers modulo 7, John Conway suggests thinking of the days of the week as ""Noneday"" or ""Sansday"" (for Sunday), ""Oneday"", ""Twosday"", ""Treblesday"", ""Foursday"", ""Fiveday"", and ""Six-a-day"".
The algorithm is simple enough for anyone with basic arithmetic ability to do the calculations mentally. Conway can usually give the correct answer in under two seconds. To improve his speed, he practices his calendrical calculations on his computer, which is programmed to quiz him with random dates every time he logs on.","[u'1973 introductions', u'All articles with unsourced statements', u'Articles with unsourced statements from January 2008', u'Calendar algorithms', u'Gregorian calendar', u'Julian calendar']","[u'7-Eleven', u'Algorithm', u'American Civil War', u'Anno Domini', u'Astronomical year numbering', u'Calculating the day of the week', u'Christmas Day', u'Common subexpression', u'Common year starting on Friday', u'Common year starting on Monday', u'Common year starting on Saturday', u'Common year starting on Sunday', u'Common year starting on Thursday', u'Common year starting on Tuesday', u'Common year starting on Wednesday', u'Computus', u'Dominical letter', u'Floor and ceiling functions', u'Floor function', u'Fort Sumter', u'Gregorian Calendar', u'Gregorian calendar', u'Halloween', u'ISO week date', u'John Horton Conway', u'Julian calendar', u'Julian day', u'Leap year starting on Friday', u'Leap year starting on Monday', u'Leap year starting on Saturday', u'Leap year starting on Sunday', u'Leap year starting on Thursday', u'Leap year starting on Tuesday', u'Leap year starting on Wednesday', u'Lewis Carroll', u'March 0', u'Mental calculation', u'Modular arithmetic', u'Ordinal date', u'Perpetual calendar', u'Proleptic Gregorian calendar', u'Proleptic Julian calendar', u'Quotient', u'Scientific American', u'September 11, 2001 attacks', u'Solstices', u'Wayback Machine', u'Working time', u'World Trade Center (1973-2001)', u'Y2K', u""Zeller's congruence""]"
Double Metaphone,"Lawrence Philips redirects here. For the football player, see Lawrence Phillips.
Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.
The original author later produced a new version of the algorithm, which he named Double Metaphone. Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. In 2009 Lawrence Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings.",[u'Phonetic algorithms'],"[u'ASCII', u'C/C++ Users Journal', u'Caverphone', u'Celtic languages', u'Ch (digraph)', u'Chinese language', u'Consonant', u'English language', u'French language', u'Germanic languages', u'Greek language', u'Italian language', u'Lawrence Phillips', u'Match Rating Approach', u'New York State Identification and Intelligence System', u'Phonetic algorithm', u'Sh (digraph)', u'Slavic languages', u'Soundex', u'Spanish language', u'Th (digraph)', u'Theta', u'Vowels']"
Double dabble,"In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation. It is also known as the shift and add 3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency. The algorithm operates as follows:
Suppose the original number to be converted is stored in a register that is n bits wide. Reserve a scratch space wide enough to hold both the original number and its BCD representation; 4×ceil(n/3) bits will be enough. It takes a maximum of 4 bits in binary to store each decimal digit.
Then partition the scratch space into BCD digits (on the left) and the original register (on the right). For example, if the original number to be converted is eight bits wide, the scratch space would be partitioned as follows:

100s Tens Ones   Original
0010 0100 0011   11110011

The diagram above shows the binary representation of 24310 in the original register, and the BCD representation of 243 on the left.
The scratch space is initialized to all zeros, and then the value to be converted is copied into the ""original register"" space on the right.

0000 0000 0000   11110011

The algorithm then iterates n times. On each iteration, the entire scratch space is left-shifted one bit. However, before the left-shift is done, any BCD digit which is greater than 4 is incremented by 3. The increment ensures that a value of 5, incremented and left-shifted, becomes 16, thus correctly ""carrying"" into the next BCD digit.
The double-dabble algorithm, performed on the value 24310, looks like this:

0000 0000 0000   11110011   Initialization
0000 0000 0001   11100110   Shift
0000 0000 0011   11001100   Shift
0000 0000 0111   10011000   Shift
0000 0000 1010   10011000   Add 3 to ONES, since it was 7
0000 0001 0101   00110000   Shift
0000 0001 1000   00110000   Add 3 to ONES, since it was 5
0000 0011 0000   01100000   Shift
0000 0110 0000   11000000   Shift
0000 1001 0000   11000000   Add 3 to TENS, since it was 6
0001 0010 0001   10000000   Shift
0010 0100 0011   00000000   Shift
   2    4    3
       BCD

Now eight shifts have been performed, so the algorithm terminates. The BCD digits to the left of the ""original register"" space display the BCD encoding of the original value 243.
Another example for the double dabble algorithm - value 6524410.

 104  103  102   101  100    Original binary
0000 0000 0000 0000 0000   1111111011011100   Initialization
0000 0000 0000 0000 0001   1111110110111000   Shift left (1st)
0000 0000 0000 0000 0011   1111101101110000   Shift left (2nd)
0000 0000 0000 0000 0111   1111011011100000   Shift left (3rd)
0000 0000 0000 0000 1010   1111011011100000   Add 3 to 100, since it was 7
0000 0000 0000 0001 0101   1110110111000000   Shift left (4th)
0000 0000 0000 0001 1000   1110110111000000   Add 3 to 100, since it was 5
0000 0000 0000 0011 0001   1101101110000000   Shift left (5th)
0000 0000 0000 0110 0011   1011011100000000   Shift left (6th)
0000 0000 0000 1001 0011   1011011100000000   Add 3 to 101, since it was 6
0000 0000 0001 0010 0111   0110111000000000   Shift left (7th)
0000 0000 0001 0010 1010   0110111000000000   Add 3 to 100, since it was 7
0000 0000 0010 0101 0100   1101110000000000   Shift left (8th)
0000 0000 0010 1000 0100   1101110000000000   Add 3 to 101, since it was 5
0000 0000 0101 0000 1001   1011100000000000   Shift left (9th)
0000 0000 1000 0000 1001   1011100000000000   Add 3 to 102, since it was 5
0000 0000 1000 0000 1100   1011100000000000   Add 3 to 100, since it was 9
0000 0001 0000 0001 1001   0111000000000000   Shift left (10th)
0000 0001 0000 0001 1100   0111000000000000   Add 3 to 100, since it was 9
0000 0010 0000 0011 1000   1110000000000000   Shift left (11th)
0000 0010 0000 0011 1011   1110000000000000   Add 3 to 100, since it was 8
0000 0100 0000 0111 0111   1100000000000000   Shift left (12th)
0000 0100 0000 1010 0111   1100000000000000   Add 3 to 101, since it was 7
0000 0100 0000 1010 1010   1100000000000000   Add 3 to 100, since it was 7
0000 1000 0001 0101 0101   1000000000000000   Shift left (13th)
0000 1011 0001 0101 0101   1000000000000000   Add 3 to 103, since it was 8
0000 1011 0001 1000 0101   1000000000000000   Add 3 to 101, since it was 5
0000 1011 0001 1000 1000   1000000000000000   Add 3 to 100, since it was 5
0001 0110 0011 0001 0001   0000000000000000   Shift left (14th)
0001 1001 0011 0001 0001   0000000000000000   Add 3 to 103, since it was 6
0011 0010 0110 0010 0010   0000000000000000   Shift left (15th)
0011 0010 1001 0010 0010   0000000000000000   Add 3 to 102, since it was 6
0110 0101 0010 0100 0100   0000000000000000   Shift left (16th)
   6    5    2    4    4
            BCD

Sixteen shifts have been performed, so the algorithm terminates. The BCD digits is: 6*104 + 5*103 + 2*102 + 4*101 + 4*100 = 65244.","[u'Articles with example C code', u'Binary arithmetic', u'Computer arithmetic algorithms']","[u'Algorithm', u'Binary-coded decimal', u'Binary numbers', u'C (programming language)', u'Computer science', u'Digital object identifier', u'International Standard Book Number', u'Latency (engineering)', u'Malloc', u'Pedagogical', u'Processor register']"
Dynamic Markov compression,"Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool. It uses predictive arithmetic coding similar to prediction by partial matching (PPM), except that the input is predicted one bit at a time (rather than one byte at a time). DMC has a good compression ratio and moderate speed, similar to PPM, but requires somewhat more memory and is not widely implemented. Some recent implementations include the experimental compression programs hook by Nania Francesco Antonio, ocamyd by Frank Schwellinger, and as a submodel in paq8l by Matt Mahoney. These are based on the 1993 implementation in C by Gordon Cormack.","[u'Lossless compression algorithms', u'Markov models']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Claude Shannon', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context mixing', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Gordon Cormack', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nigel Horspool', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
Dynamic Programming,"In mathematics, management science, economics, computer science,and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space. (Each of the subproblem solutions is indexed in some way, typically based on the values of its input parameters, so as to facilitate its lookup.) The act of storing solutions to subproblems is called ""memoization"". In contrast, a more naive method would not recognize that a particular subproblem has already been solved previously, and would repeatedly solve the same subproblem many times.
This approach is especially useful when the number of repeating subproblems grows exponentially as a function of the size of the input.
Dynamic programming is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure (described below). When applicable, the method takes far less time than other methods that don't take advantage of the subproblem overlap (like depth-first search).
Dynamic programming algorithms are used for optimization (for example, finding the shortest path between two points, or the fastest way to multiply many matrices). A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. The alternatives are many, such as using a greedy algorithm, which picks the locally optimal choice at each branch in the road. The locally optimal choice may be a poor choice for the overall solution. While a greedy algorithm does not guarantee an optimal solution, it is often faster to calculate. Fortunately, some greedy algorithms (such as minimum spanning trees) are proven to lead to the optimal solution.
For example, let's say that you have to get from point A to point B as fast as possible, in a given city, during rush hour. A dynamic programming algorithm will look at finding the shortest paths to points close to A, and use those solutions to eventually find the shortest path to B. On the other hand, a greedy algorithm will start you driving immediately and will pick the road that looks the fastest at every intersection. As you can imagine, this strategy might not lead to the fastest arrival time, since you might take some ""easy"" streets and then find yourself hopelessly stuck in a traffic jam.
Sometimes, applying memoization to a naive basic recursive solution already results in a dynamic programming solution with asymptotically optimal time complexity; however, the optimal solution to some problems requires more sophisticated dynamic programming algorithms. Some of these may be recursive as well but parametrized differently from the naive solution. Others can be more complicated and cannot be implemented as a recursive function with memoization. Examples of these are the two solutions to the Egg Dropping puzzle below.","[u'All articles needing additional references', u'All articles with unsourced statements', u'Articles needing additional references from April 2014', u'Articles needing additional references from May 2013', u'Articles with unsourced statements from May 2009', u'Dynamic programming', u'Equations', u'Mathematical optimization', u'Operations research', u'Optimal control', u'Optimization algorithms and methods', u'Systems engineering', u'Wikipedia articles with GND identifiers']","[u'Alexander Zasedatelev', u'Approximation algorithm', u'Artificial neural networks', u'Associative array', u'Augmented Lagrangian method', u'Backtracking', u'Backward induction', u'Barrier function', u'Beat (music)', u'Bellman equation', u'Bellman\u2013Ford algorithm', u'Big-O notation', u'Bioinformatics', u'Bitonic tour', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Brute-force search', u'Bulletin of the American Mathematical Society', u'CYK algorithm', u'Call-by-name', u'Call-by-need', u'Capital (economics)', u'Chain matrix multiplication', u'Charles DeLisi', u'Charles E. Leiserson', u'Charles Erwin Wilson', u'Chart parser', u'Checkerboard', u'Clifford Stein', u'Clique-width', u'Combinatorial optimization', u'Common Lisp', u'Comparison of optimization software', u'Computational complexity of mathematical operations', u'Computer chess', u'Computer science', u'Context-free grammar', u'Convex minimization', u'Convex optimization', u'Convexity in economics', u'Correspondence problem', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'De Boor algorithm', u'Depth-first search', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Discounting', u'Discrete-time', u'Divide and conquer algorithm', u'Dover Publications', u'Duckworth\u2013Lewis method', u'Dynamic algorithm', u'Dynamic problem', u'Dynamic programming', u'Dynamic programming language', u'Dynamic time warping', u'Earley algorithm', u'Economics', u'Edit distance', u'Edmonds\u2013Karp algorithm', u'Edward Prescott', u'Ellipsoid method', u'Engineering', u'Evolutionary algorithm', u'Exchange algorithm', u'Exponential growth', u'Exponential time', u'Fibonacci sequence', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Genetics', u'Georgii Gurskii', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hanoi', u'Hessian matrix', u'Heuristic algorithm', u'Hidden Markov model', u'Hill climbing', u'IBM System R', u'IEEE', u'Integer programming', u'Integrated Authority File', u'International Standard Book Number', u'Interval scheduling', u'Introduction to Algorithms', u'Iterative method', u'J (programming language)', u'Jacques Philippe Marie Binet', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u'Knapsack problem', u""Kruskal's algorithm"", u'Lattice models', u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Levenshtein distance', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Linear search problem', u'Local convergence', u'Local search (optimization)', u'Longest common subsequence problem', u'Longest common substring problem', u'Longest increasing subsequence problem', u'Management science', u'Markov decision process', u'Mathematica', u'Mathematical Reviews', u'Mathematical game', u'Mathematical optimization', u'Mathematics', u'Matrix chain multiplication', u'Matroid', u'Maximum subarray problem', u'Memoization', u'Memoize', u'Mergesort', u'Metaheuristic', u'Method of undetermined coefficients', u'Minimum spanning tree', u'Music information retrieval', u'Nancy Stokey', u'National Diet Library', u'Needleman\u2013Wunsch algorithm', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Non-convexity (economics)', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Nucleosome', u'On-Line Encyclopedia of Integer Sequences', u'Optimal substructure', u'Optimization algorithm', u'Overlapping subproblem', u'Partition problem', u'Patricia Selinger', u'Penalty method', u'Perl', u'Photoshop', u""Powell's method"", u'Principle of Optimality', u'Programming language', u'Prolog', u'Pseudo-polynomial time', u'Push\u2013relabel maximum flow algorithm', u'Puzzle', u'Quadratic programming', u'Quasi-Newton method', u'Quicksort', u'RNA structure', u'Recursion', u'Recursion (computer science)', u'Recursive least squares', u'Referential transparency (computer science)', u'Refutation table', u'Revised simplex algorithm', u'Richard Bellman', u'Robert E. Lucas', u'Ronald L. Rivest', u'Scheme (programming language)', u'Seam carving', u'Sequence alignment', u'Sequential quadratic programming', u'Shortest path problem', u'Simplex algorithm', u'Simulated annealing', u'Smith\u2013Waterman algorithm', u'State variable', u'Stochastic programming', u'String (computer science)', u'Structural alignment', u'Subgradient method', u'Subroutine', u'Subset sum problem', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Systems analysis', u'Tabu search', u'Taylor & Francis', u'The Mathematical Association of America', u'Thomas H. Cormen', u'Time-invariant system', u'Top-down', u'Top-down and bottom-up design', u'Tower of Hanoi', u'Transcription factor', u'Transposition table', u'Travelling salesman problem', u'Tree decomposition', u'Tree structure', u'Treewidth', u'Truncated Newton method', u'Trust region', u'U.S. English', u'Undirected graph', u'Utility', u'Viterbi algorithm', u'Wolfe conditions', u'Word wrap']"
Dynamic time warping,"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed. For instance, similarities in walking patterns could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data which can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. Also it is seen that it can be used in partial shape matching application.
In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restrictions. The sequences are ""warped"" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.","[u'Articles with example pseudocode', u'CS1 maint: Explicit use of et al.', u'Dynamic programming', u'Machine learning algorithms', u'Time series analysis']","[u'Acceleration', u'Algorithm', u'Digital object identifier', u'Elastic matching', u'Functional data analysis', u'International Standard Book Number', u'International Standard Serial Number', u'Levenshtein distance', u'Multiple alignment', u'Nearest neighbour classifiers', u'Non-linear', u'Optimal matching', u'PubMed Central', u'PubMed Identifier', u'Sequence alignment', u'Shape analysis (digital geometry)', u'Signature recognition', u'Speaker recognition', u'Speech recognition', u'Time series', u'Time series analysis', u'Triangle inequality']"
Earley parser,"In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in abbreviated, more legible form in a journal).
Earley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case , where n is the length of the parsed string, quadratic time for unambiguous grammars , and linear time for almost all LR(k) grammars. It performs particularly well when the rules are written left-recursively.","[u'Dynamic programming', u'Parsing algorithms']","[u'Algorithm', u'C++', u'CYK algorithm', u'C (programming language)', u'Chart parser', u'Communications of the ACM', u'Compiler', u'Computational linguistics', u'Computer science', u'Context-free grammar', u'Context-free language', u'Digital object identifier', u'Domain-specific language', u'Dynamic programming', u'Empty string', u'Formal grammar', u'Haskell (programming language)', u'International Standard Book Number', u'JavaScript', u'Jay Earley', u'LL parser', u'LR parser', u'Left recursion', u'Lexical analysis', u'List of algorithms', u'Mathematical Reviews', u'Nigel Horspool', u'Parsing', u'Perl', u'Python (programming language)', u'Racket (programming language)', u'Scheme (programming language)', u'String (computer science)', u'Terminal and nonterminal symbols', u'The Computer Journal', u'Theoretical Computer Science (journal)', u'Tuple']"
Earliest deadline first scheduling,"Earliest deadline first (EDF) or least time to go is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (task finishes, new task released, etc.) the queue will be searched for the process closest to its deadline. This process is the next to be scheduled for execution.
EDF is an optimal scheduling algorithm on preemptive uniprocessors, in the following sense: if a collection of independent jobs, each characterized by an arrival time, an execution requirement and a deadline, can be scheduled (by any algorithm) in a way that ensures all the jobs complete by their deadline, the EDF will schedule this collection of jobs so they all complete by their deadline.
With scheduling periodic processes that have deadlines equal to their periods, EDF has a utilization bound of 100%. Thus, the schedulability test for EDF is:

where the  are the worst-case computation-times of the  processes and the  are their respective inter-arrival periods (assumed to be equal to the relative deadlines).
That is, EDF can guarantee that all deadlines are met provided that the total CPU utilization is not more than 100%. Compared to fixed priority scheduling techniques like rate-monotonic scheduling, EDF can guarantee all the deadlines in the system at higher loading.
However, when the system is overloaded, the set of processes that will miss deadlines is largely unpredictable (it will be a function of the exact deadlines and time at which the overload occurs.) This is a considerable disadvantage to a real time systems designer. The algorithm is also difficult to implement in hardware and there is a tricky issue of representing deadlines in different ranges (deadlines can't be more precise than the granularity of the clock used for the scheduling). If a modular arithmetic is used to calculate future deadlines relative to now, the field storing a future relative deadline must accommodate at least the value of the ((""duration"" {of the longest expected time to completion} * 2) + ""now""). Therefore EDF is not commonly found in industrial real-time computer systems.
Instead, most real-time computer systems use fixed priority scheduling (usually rate-monotonic scheduling). With fixed priorities, it is easy to predict that overload conditions will cause the low-priority processes to miss deadlines, while the highest-priority process will still meet its deadline.
There is a significant body of research dealing with EDF scheduling in real-time computing; it is possible to calculate worst case response times of processes in EDF, to deal with other types of processes than periodic processes and to use servers to regulate overloads.","[u'All Wikipedia articles needing clarification', u'Processor scheduling algorithms', u'Real-time computing', u'Wikipedia articles needing clarification from June 2013']","[u'AQuoSA', u'Central processing unit', u'Computer hardware', u'Dynamic priority scheduling', u'Fixed priority pre-emptive scheduling', u'Hierarchical scheduler', u'Kernel-based Virtual Machine', u'Least common multiple', u'Linux kernel', u'OSEK', u'Priority inheritance', u'Priority inversion', u'Priority queue', u'Rate-monotonic scheduling', u'Real-time computing', u'Real-time operating system', u'SCHED DEADLINE', u'Scheduling algorithm', u'Temporal isolation among virtual machines', u'Time slice', u'Virtual machine']"
Edmonds's algorithm,"In graph theory, a branch of mathematics, Edmonds' algorithm or Chu–Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem. The algorithm was proposed independently first by Yoeng-jin Chu and Tseng-hong Liu (1965) and then by Jack Edmonds (1967).",[u'Graph algorithms'],"[u'A* search algorithm', u'Algorithm', u'Alpha\u2013beta pruning', u'Arborescence (graph theory)', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u'Blossom algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'C++', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Directed graph', u'Dynamic programming', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph theory', u'Graph traversal', u'Hill climbing', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Jack Edmonds', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'MIT License', u'Minimum spanning tree', u'Open source', u""Prim's algorithm"", u'Robert Tarjan', u'SMA*', u'Search game', u'Spanning subgraph', u'Sparse graph', u'Tree traversal']"
Edmonds–Karp algorithm,"In computer science, the Edmonds–Karp algorithm is an implementation of the Ford–Fulkerson method for computing the maximum flow in a flow network in O(V E2) time. The algorithm was first published by Yefim (Chaim) Dinic in 1970 and independently published by Jack Edmonds and Richard Karp in 1972. Dinic's algorithm includes additional techniques that reduce the running time to O(V2E).","[u'Graph algorithms', u'Network flow']","[u'Association for Computing Machinery', u'Augmenting path', u'Big O notation', u'Breadth-first search', u'Charles E. Leiserson', u'Clifford Stein', u'Computer science', u'Digital object identifier', u""Dinic's algorithm"", u'Flow network', u'Ford\u2013Fulkerson algorithm', u'International Standard Book Number', u'Introduction to Algorithms', u'Jack Edmonds', u'Max flow min cut theorem', u'Maximum flow problem', u'Richard Karp', u'Ronald L. Rivest', u'Thomas H. Cormen']"
Elevator algorithm,"The elevator algorithm (also SCAN) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.
This algorithm is named after the behavior of a building elevator, where the elevator continues to travel in its current direction (up or down) until empty, stopping only to let individuals off or to pick up new individuals heading in the same direction.
From an implementation perspective, the drive maintains a buffer of pending read/write requests, along with the associated cylinder number of the request. Lower cylinder numbers indicate that the cylinder is closest to the spindle, and higher numbers indicate the cylinder is farther away.

","[u'All articles needing additional references', u'Articles needing additional references from November 2007', u'Disk scheduling algorithms', u'Sorting algorithms']","[u'Cylinder (disk drive)', u'Data buffer', u'Disk drive', u'Elevator', u'FIFO (computing and electronics)', u'FSCAN', u'Hard disk', u'I/O scheduling', u'LOOK algorithm', u'N-Step-SCAN', u'Resource starvation', u'Shortest seek first']"
Embedded Zerotree Wavelet,"Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a subband transform (such as the wavelet transform) will be zero, or very close to zero. This occurs because ""real world"" images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.
By considering the transformed coefficients as a tree (or trees) with the lowest frequency coefficients at the root node and with the children of each tree node being the spatially related coefficients in the next higher frequency subband, there is a high probability that one or more subtrees will consist entirely of coefficients which are zero or nearly zero, such subtrees are called zerotrees. Due to this, we use the terms node and coefficient interchangeably, and when we refer to the children of a coefficient, we mean the child coefficients of the node in the tree where that coefficient is located. We use children to refer to directly connected nodes lower in the tree and descendants to refer to all nodes which are below a particular node in the tree, even if not directly connected.
In zerotree based image compression scheme such as EZW and SPIHT, the intent is to use the statistical properties of the trees in order to efficiently code the locations of the significant coefficients. Since most of the coefficients will be zero or close to zero, the spatial locations of the significant coefficients make up a large portion of the total size of a typical compressed image. A coefficient (likewise a tree) is considered significant if its magnitude (or magnitudes of a node and all its descendants in the case of a tree) is above a particular threshold. By starting with a threshold which is close to the maximum coefficient magnitudes and iteratively decreasing the threshold, it is possible to create a compressed representation of an image which progressively adds finer detail. Due to the structure of the trees, it is very likely that if a coefficient in a particular frequency band is insignificant, then all its descendants (the spatially related higher frequency band coefficients) will also be insignificant.
EZW uses four symbols to represent (a) a zerotree root, (b) an isolated zero (a coefficient which is insignificant, but which has significant descendants), (c) a significant positive coefficient and (d) a significant negative coefficient. The symbols may be thus represented by two binary bits. The compression algorithm consists of a number of iterations through a dominant pass and a subordinate pass, the threshold is updated (reduced by a factor of two) after each iteration. The dominant pass encodes the significance of the coefficients which have not yet been found significant in earlier iterations, by scanning the trees and emitting one of the four symbols. The children of a coefficient are only scanned if the coefficient was found to be significant, or if the coefficient was an isolated zero. The subordinate pass emits one bit (the most significant bit of each coefficient not so far emitted) for each coefficient which has been found significant in the previous significance passes. The subordinate pass is therefore similar to bit-plane coding.
There are several important features to note. Firstly, it is possible to stop the compression algorithm at any time and obtain an approximation of the original image, the greater the number of bits received, the better the image. Secondly, due to the way in which the compression algorithm is structured as a series of decisions, the same algorithm can be run at the decoder to reconstruct the coefficients, but with the decisions being taken according to the incoming bit stream. In practical implementations, it would be usual to use an entropy code such as arithmetic code to further improve the performance of the dominant pass. Bits from the subordinate pass are usually random enough that entropy coding provides no further coding gain.
The coding performance of EZW has since been exceeded by SPIHT and its many derivatives.","[u'Commons category with local link same as on Wikidata', u'Image compression', u'Lossless compression algorithms', u'Trees (data structures)', u'Wavelets']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'IEEE', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'SPIHT', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tree (graph theory)', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'Wavelet transform', u'\u039c-law algorithm']"
Euclidean algorithm,"In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in Euclid's Elements (c. 300 BC). It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules, and is one of the oldest numerical algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.
The Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 147 = 252 − 105. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until one of the two numbers reaches zero. When that occurs, the other number (the one that is not zero) is the GCD of the original two numbers. By reversing the steps, the GCD can be expressed as a sum of the two original numbers each multiplied by a positive or negative integer, e.g., 21 = 5 × 105 + (−2) × 252. The fact that the GCD can always be expressed in this way is known as Bézout's identity.
The version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two. With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844, and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.
The Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it is a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.","[u'Articles containing proofs', u'Articles with example pseudocode', u'CS1 German-language sources (de)', u'Commons category with local link same as on Wikidata', u'Euclid', u'Featured articles', u'Number theoretic algorithms']","[u'AKS primality test', u'Absolute value', u'Abstract algebra', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alfred Aho', u'Algebraic integer', u'Algorithm', u'Almost all', u'Ancient Egyptian multiplication', u'Andr\xe9 Weil', u'Arnold Sch\xf6nhage', u'Aryabhata', u'Associative property', u'Associativity', u'BCH code', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Bartel Leendert van der Waerden', u'Berlekamp\u2013Massey algorithm', u'Bibcode', u'Big-O notation', u'Big O notation', u'Binary GCD algorithm', u'Binary numeral system', u'Binary operation', u'Binary search tree', u""B\xe9zout's identity"", u'C. Stanley Ogilvy', u'Calkin\u2013Wilf tree', u'Cambridge University Press', u'Carl Friedrich Gauss', u'Carl Pomerance', u'Chakravala method', u'Chinese remainder theorem', u""Cipolla's algorithm"", u'Clark Kimberling', u'Claude Gaspard Bachet de M\xe9ziriac', u'Commensurability (mathematics)', u'Commutative property', u'Commutative ring', u'Commutativity', u'Compass (drawing tool)', u'Complex number', u'Computational complexity theory', u'Computer programming', u'Constant time', u'Continued fraction', u'Continued fraction factorization', u'Control theory', u'Coordinate vector', u'Coprime integers', u""Cornacchia's algorithm"", u'Cryptographic protocol', u'Cut-the-knot', u'Cyclotomic field', u'David Fowler (mathematician)', u'David J. Darling', u'Degree of a polynomial', u'Derivative', u'Derrick Henry Lehmer', u'Determinant', u'Digital object identifier', u'Diophantine approximation', u'Diophantine equation', u'Diophantus', u'Discrete logarithm', u'Disquisitiones Arithmeticae', u'Distributive property', u'Distributivity', u'Division (mathematics)', u""Dixon's factorization method"", u'Domain (ring theory)', u'Donald Knuth', u'Eisenstein integer', u'Electronic commerce', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Empty product', u'Eric Bach', u'Eric W. Weisstein', u'Ernst Kummer', u'Error-correcting code', u'Euclid', u""Euclid's Elements"", u""Euclid's lemma"", u'Euclidean (disambiguation)', u'Euclidean division', u'Euclidean domain', u'Euclidean geometry', u'Euclidean rhythm', u'Eudoxus of Cnidus', u""Euler's factorization method"", u""Euler's totient function"", u'Euler\u2013Mascheroni constant', u'Extended Euclidean algorithm', u""Fermat's Last Theorem"", u""Fermat's factorization method"", u""Fermat's theorem on sums of two squares"", u'Fermat primality test', u'Fibonacci number', u'Field norm', u'Finite field', u'Florian Cajori', u'Fraction (mathematics)', u'Function field sieve', u'Fundamental theorem of arithmetic', u""F\xfcrer's algorithm"", u'GCD domain', u'Gabriel Lam\xe9', u""Gal's accurate tables"", u'Gaussian integer', u'General number field sieve', u'Generalized Riemann hypothesis', u'Generating primes', u'Golden ratio', u'Greatest common divisor', u'Group (mathematics)', u'Hans Heilbronn', u'Harold Edwards (mathematician)', u'Harold Stark', u'Helaman Ferguson', u'Hendrik Lenstra', u'Henri Cohen (number theorist)', u'Hurwitz quaternion', u'I. N. Herstein', u'IEEE Computer Society Press', u'Ideal (ring theory)', u'Ideal number', u'Imaginary unit', u'Index calculus algorithm', u'Infinitesimal', u'Integer', u'Integer factorization', u'Integer relation algorithm', u'Integer square root', u'Integral domain', u'International Standard Book Number', u'Internet', u'Interval (mathematics)', u'Invertible matrix', u'Irrational number', u'Irreducible element', u'Irreducible fraction', u'Irreducible polynomial', u'Ivan Matveyevich Vinogradov', u'Ivor Grattan-Guinness', u'JSTOR', u'Jacques Charles Fran\xe7ois Sturm', u'Jeffrey Shallit', u'Jeffrey Ullman', u'John Hopcroft', u'John Stillwell', u'Joseph Liouville', u'Karatsuba algorithm', u""Lagrange's four-square theorem"", u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Leopold Kronecker', u'Library of Congress Control Number', u'Linear combination', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'L\xe1szl\xf3 Lov\xe1sz', u'MIT Press', u'Manfred R. Schroeder', u'MathWorld', u'Mathematical Association of America', u'Mathematical Reviews', u'Mathematical Treatise in Nine Sections', u'Mathematical induction', u'Mathematician', u'Mathematics', u'Matrix (mathematics)', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Modular multiplicative inverse', u'Modulo operation', u'Monoid', u'Multiplication algorithm', u'Natural number', u'Nicholas Saunderson', u'Nicomachus', u'Norm-Euclidean field', u'Norm (mathematics)', u'Number theory', u'OCLC', u'Oren Patashnik', u'Oxford University Press', u'Peter Gustav Lejeune Dirichlet', u'Peter Shor', u'PlanetMath', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial', u'Polynomial greatest common divisor', u'Polynomial long division', u'Primality test', u'Prime number', u'Principal ideal', u'Principal ideal domain', u""Proth's theorem"", u'Pseudocode', u'Pythagoras', u'Pythagorean triple', u""P\xe9pin's test"", u'Qin Jiushao', u'Quadratic Frobenius test', u'Quadratic integer', u'Quadratic residue', u'Quadratic sieve', u'Quotient', u'RSA algorithm', u'Rational number', u'Rational sieve', u'Real number', u'Recurrence relation', u'Reed\u2013Solomon code', u'Remainder', u'Richard Dedekind', u'Riemann zeta function', u'Ring (mathematics)', u'Ring theory', u'Roger Cotes', u'Ronald Graham', u'Root of unity', u'Ross Honsberger', u'Routh\u2013Hurwitz stability criterion', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Serge Lang', u'Sergei Tabachnikov', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Society for Industrial and Applied Mathematics', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Square root of 2', u'Stan Wagon', u'Stern\u2013Brocot tree', u""Sturm's theorem"", u'Sturm chain', u'Sun Tzu (mathematician)', u'System of linear equations', u'T. L. Heath', u'Telescoping series', u'Temporary variable', u'The Art of Computer Programming', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Underdetermined system', u'Uniform cost model', u'Unique factorization domain', u'Von Mangoldt function', u'Vorlesungen \xfcber Zahlentheorie', u'Well-order', u'Wheel factorization', u'William J. LeVeque', u""Williams' p + 1 algorithm"", u'Zentralblatt MATH', u'Zero of a function', u'\xc9variste Galois', u'\xd8ystein Ore']"
Euclidean shortest path problem,"The Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles.
In two dimensions, the problem can be solved in polynomial time in a model of computation allowing addition and comparisons of real numbers, despite theoretical difficulties involving the numerical precision needed to perform such calculations. These algorithms are based on two different principles, either performing a shortest path algorithm such as Dijkstra's algorithm on a visibility graph derived from the obstacles or (in an approach called the continuous Dijkstra method) propagating a wavefront from one of the points until it meets the other.
In three (and higher) dimensions the problem is NP-hard in the general case , but there exist efficient approximation algorithms that run in polynomial time based on the idea of finding a suitable sample of points on the obstacle edges and performing a visibility graph calculation using these sample points.
There are many results on computing shortest paths which stays on a polyhedral surface. Given two points s and t, say on the surface of a convex polyhedron, the problem is to compute a shortest path that never leaves the surface and connects s with t. This is a generalization of the problem from 2-dimension but it is much easier than the 3-dimensional problem.
Also, there are variations of this problem, where the obstacles are weighted, i.e., one can go through an obstacle, but it incurs an extra cost to go through an obstacle. The standard problem is the special case where the obstacles have infinite weight. This is termed as the weighted region problem in the literature.

","[u'All stub articles', u'Combinatorics stubs', u'Computational geometry', u'Geometric algorithms', u'Geometry stubs']","[u'Algorithmica', u'Combinatorics', u'Computational geometry', u'Convex polyhedron', u'Der-Tsai Lee', u'Digital object identifier', u""Dijkstra's algorithm"", u'Discrete and Computational Geometry', u'Euclidean space', u'Franco P. Preparata', u'Geometry', u'Godfried Toussaint', u'International Standard Book Number', u'Journal of the ACM', u'KernelCAD', u'NP-hard', u'Numerical precision', u'Polyhedron', u'Polynomial time', u'SIAM Journal on Computing', u'Shortest path problem', u'Springer-Verlag', u'Subhash Suri', u'Symposium on Computational Geometry', u'Visibility graph', u'Weighted region problem']"
Expectation-maximization algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.","[u'Data clustering algorithms', u'Estimation theory', u'Machine learning algorithms', u'Missing data', u'Optimization algorithms and methods', u'Statistical algorithms']","[u'A. W. F. Edwards', u'Affine connection', u'Allen Craig', u'Anders Martin-L\xf6f', u'Annals of Statistics', u'Anomaly detection', u'Arthur P. Dempster', u'Artificial neural network', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Baum-Welch algorithm', u'Bayes theorem', u'Bayesian inference', u'Bayesian network', u'Bias-variance dilemma', u'Bibcode', u'Bimodal distribution', u'Binomial distribution', u'Biometrika', u'Boosting (machine learning)', u'Bootstrap aggregating', u'C.F. Jeff Wu', u'Canonical correlation analysis', u'Christopher Bishop', u'CiteSeer', u'Closed-form expression', u'Cluster analysis', u'Communications in Statistics', u'Computational learning theory', u'Computer vision', u'Conditional probability distribution', u'Conditional random field', u'Conjugate gradient', u'Continuous random variable', u'Convolutional neural network', u'D. Basu', u'D. R. Cox', u'DBSCAN', u'Data clustering', u'Data mining', u'David J.C. MacKay', u'Decision tree learning', u'Deep learning', u'Density estimation', u'Derivative', u'Digital object identifier', u'Dimensionality reduction', u'Discrete random variable', u'Donald Rubin', u'ELKI', u'Empirical risk minimization', u'Ensemble learning', u'Entropy (information theory)', u'Expectation-maximization algorithm', u'Expected value', u'Exponential family', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Frank Dellaert', u'Gauss\u2013Newton method', u'Geoffrey Hinton', u'George A. Barnard', u'Geyser', u""Gibbs' inequality"", u'Gradient descent', u'Grammar induction', u'Graphical model', u'Graphical models', u'Hidden Markov model', u'Hierarchical clustering', u'Independent component analysis', u'Indicator function', u'Information geometry', u'Inside-outside algorithm', u'International Standard Book Number', u'Item response theory', u'Iterative method', u'JSTOR', u'Journal of the Royal Statistical Society, Series B', u'Journal of the Royal Statistical Society, Series C', u'K-means algorithm', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Kalman filter', u'Kullback\u2013Leibler divergence', u'Latent variable', u'Latent variables', u'Learning to rank', u'Likelihood function', u'Linear discriminant analysis', u'Linear regression', u'Local maximum', u'Local outlier factor', u'Logistic regression', u'MM algorithm', u'Machine learning', u'Marginal likelihood', u'Markov blanket', u'Mathematical Reviews', u'Mathematical singularity', u'Maximum a posteriori', u'Maximum likelihood', u'Maximum likelihood estimate', u'Maximum likelihood estimation', u'Maximum likelihood estimator', u'Mean-shift', u'Medical imaging', u'Message passing (disambiguation)', u'Metaheuristic', u'Michael I. Jordan', u'Misnomer', u'Missing values', u'Mixture model', u'Multilayer perceptron', u'Multivariate normal distribution', u'Naive Bayes classifier', u'Nan Laird', u'Natural language processing', u'Newton\u2013Raphson', u'Non-negative matrix factorization', u'Normal distribution', u'OPTICS algorithm', u'Old Faithful', u'Online machine learning', u'Ordered subset expectation maximization', u'Parameter', u'Parameters', u'Per Martin-L\xf6f', u'Perceptron', u'Positron emission tomography', u'Posterior probabilities', u'Principal component analysis', u'Probabilistic context-free grammar', u'Probability density function', u'Probability distribution', u'Probably approximately correct learning', u'Psychometrics', u'Random-restart hill climbing', u'Random forest', u'Rasch model', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Robert Tibshirani', u'SOCR', u'Saddle point', u'Self-organizing map', u'Semi-supervised learning', u'Simulated annealing', u'Single photon emission computed tomography', u'Statistical classification', u'Statistical learning theory', u'Statistical model', u'Statistics', u'Structured prediction', u'Sufficient statistic', u'Supervised learning', u'Support vector machine', u'T-distributed stochastic neighbor embedding', u'Total absorption spectroscopy', u'Trevor Hastie', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Variational Bayes', u'Viterbi algorithm', u'Weighted average', u'Yasuo Matsuyama']"
Exponential backoff,"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.","[u'Ethernet', u'Networking algorithms', u'Scheduling algorithms', u'Use dmy dates from July 2013', u'Wikipedia articles incorporating text from the Federal Standard 1037C']","[u'Algorithm', u'Carrier sense multiple access with collision avoidance', u'Carrier sense multiple access with collision detection', u'Computer networks', u'Control theory', u'Copyright status of work by the U.S. government', u'Data', u'Data frame', u'Ethernet', u'Expected value', u'Exponential growth', u'Feedback', u'General Services Administration', u'IEEE 802.3', u'Interval (mathematics)', u'Media access control', u'Network congestion avoidance', u'Retransmission (data networks)', u'Slot time', u'Time', u'Triangular number', u'Uniform distribution (discrete)']"
Exponentiating by squaring,"In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, or more generally of an element of a semigroup, like a polynomial or a square matrix. Some variants are commonly referred to as square-and-multiply algorithms or binary exponentiation. These can be of quite general use, for example in modular arithmetic or powering of matrices. For semigroups for which additive notation is commonly used, like elliptic curves used in cryptography, this method is also referred to as double-and-add.","[u'Computer arithmetic', u'Computer arithmetic algorithms', u'Exponentials']","[u'Abelian group', u'Addition-chain exponentiation', u'Addition chain', u'Big-O notation', u'Binary expansion', u'Brauer', u'Commutative', u'Compiler', u'Computer programming', u'Cryptography', u'Elliptic curve', u'Floor function', u'Group (mathematics)', u'Hamming weight', u'Heuristic', u'Identity matrix', u'Mathematics', u'Matrix (math)', u'Modular arithmetic', u'Modular exponentiation', u'Montgomery reduction', u'Non-adjacent form', u'Number', u'Peter Montgomery (mathematician)', u'Polynomial', u'Positive integer', u'Precomputation', u'Public-key cryptography', u'Recursion (computer science)', u'Remainder', u'Result', u'Ring (mathematics)', u'Ruby (programming language)', u'Semigroup', u'Side-channel attack', u'Signed-digit representation', u'Square matrix', u'Static typing', u'Tail call', u'Timing attack', u'Up to', u'Vectorial addition chain']"
Extended Euclidean algorithm,"In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, which computes, besides the greatest common divisor of integers a and b, the coefficients of Bézout's identity, that is integers x and y such that

It allows one to compute also, with almost no extra cost, the quotients of a and b by their greatest common divisor.
Extended Euclidean algorithm also refers to a very similar algorithm for computing the polynomial greatest common divisor and the coefficients of Bézout's identity of two univariate polynomials.
The extended Euclidean algorithm is particularly useful when a and b are coprime, since x is the modular multiplicative inverse of a modulo b, and y is the modular multiplicative inverse of b modulo a. Similarly, the polynomial extended Euclidean algorithm allows one to compute the multiplicative inverse in algebraic field extensions and, in particular in finite fields of non prime order. It follows that both extended Euclidean algorithms are widely used in cryptography. In particular, the computation of the modular multiplicative inverse is an essential step in RSA public-key encryption method.","[u'Articles with example pseudocode', u'Euclid', u'Number theoretic algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algebraic field extension', u'Algorithm', u'Ancient Egyptian multiplication', u'Arithmetic', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Bijective', u'Binary GCD algorithm', u""B\xe9zout's identity"", u'Chakravala method', u'Charles E. Leiserson', u""Cipolla's algorithm"", u'Clifford Stein', u'Coding theory', u'Computer algebra', u'Computer programming', u'Content (algebra)', u'Continued fraction factorization', u'Coprime', u""Cornacchia's algorithm"", u'Cryptography', u'Discrete logarithm', u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve primality', u'Elliptic curve primality proving', u""Euclid's lemma"", u'Euclidean algorithm', u'Euclidean division', u'Euclidean division of polynomials', u'Euclidean domain', u""Euler's factorization method"", u""Fermat's factorization method"", u'Fermat primality test', u'Field (mathematics)', u'Finite field', u'Finite field arithmetic', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'Introduction to Algorithms', u'Irreducible polynomial', u'Karatsuba algorithm', u'Leading coefficient', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Linear congruence theorem', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Modular multiplicative inverse', u'Monic polynomial', u'Multiplication algorithm', u'Multiplicative inverse', u'Number theory', u'Parallel assignment', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial greatest common divisor', u'Primality test', u'Prime field', u'Prime number', u'Primitive polynomial (ring theory)', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Quotient ring', u'RSA (algorithm)', u'Rational sieve', u'Resultant', u'Ronald L. Rivest', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Simple extension', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Subresultant', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Unit (ring theory)', u'Univariate polynomial', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'Z/nZ']"
FELICS,"FELICS, which stands for Fast Efficient & Lossless Image Compression System, is a lossless image compression algorithm that performs 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio.","[u'All articles needing additional references', u'Articles needing additional references from August 2011', u'Lossless compression algorithms', u'Lossy compression algorithms', u'Use dmy dates from July 2013']","[u'Binary code', u'Causal', u'Comparison of graphics file formats', u'Data compression ratio', u'Decorrelation', u'Entropy encoding', u'Exchangeable image file format', u'Exif', u'Felix (disambiguation)', u'GIMP', u'Geometric distribution', u'HiRISE', u'Image compression', u'Image file formats', u'JPEG-LS', u'Jeffrey Vitter', u'Lossless JPEG', u'Lossless data compression', u'Portable Network Graphics', u'Rice code']"
FLAME clustering,"Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.

","[u'Data clustering algorithms', u'Wikipedia articles with possible conflicts of interest from August 2010']","[u'Data clustering', u'Fuzzy clustering']"
FNN algorithm,"The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.

","[u'Algorithms and data structures stubs', u'All Wikipedia articles needing context', u'All pages needing cleanup', u'All stub articles', u'Computer science stubs', u'Dynamical systems', u'Nonlinear time series analysis', u'Statistical algorithms', u'Wikipedia articles needing context from October 2009', u'Wikipedia introduction cleanup from October 2009']","[u'Algorithm', u'Bibcode', u'Data structure', u'Digital object identifier', u'Embedding dimension', u'Nearest neighbor', u'PubMed Identifier', u'Time series']"
Fair-share scheduling,"Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes.
One common method of logically implementing the fair-share scheduling strategy is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.) The time quantum required by round-robin is arbitrary, as any equal division of time will produce the same results.
This was first developed by Judy Kay and Piers Lauder through their research at Sydney University in the 1980s.","[u'All articles needing additional references', u'Articles needing additional references from June 2012', u'Processor scheduling algorithms']","[u'Central processing unit', u'Completely Fair Scheduler', u'Operating systems', u'Round-robin scheduling', u'Scheduling algorithm']"
Fast Fourier transform,"A fast Fourier transform (FFT) algorithm computes the discrete Fourier transform (DFT) of a sequence, or its inverse. Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. A FFT rapidly computes such transformations by factorizing the DFT matrix into a product of sparse (mostly zero) factors. As a result, it manages to reduce the complexity of computing the DFT from , which arises if one simply applies the definition of DFT, to , where  is the data size.
Fast Fourier transforms are widely used for many applications in engineering, science, and mathematics. The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805. In 1994 Gilbert Strang described the FFT as ""the most important numerical algorithm of our lifetime"" and it was included in Top 10 Algorithms of 20th Century by the IEEE journal Computing in Science & Engineering.","[u'All articles needing additional references', u'Articles needing additional references from June 2015', u'Articles needing additional references from October 2015', u'Articles with inconsistent citation formats', u'Digital signal processing', u'Discrete transforms', u'FFT algorithms', u'Unsolved problems in computer science']","[u'2 Pallas', u'3 Juno', u'Algorithm', u'Approximation error', u'Arithmetic complexity of the discrete Fourier transform', u'Asymptotically optimal', u'Big O notation', u""Bluestein's FFT algorithm"", u""Bruun's FFT algorithm"", u'Butterfly diagram', u'Cache-oblivious', u'Cache (computing)', u'Carl Friedrich Gauss', u'Central processing unit', u'Charles E. Leiserson', u'Chebyshev approximation', u'Chinese Remainder Theorem', u'Chirp-z algorithm', u'Circulant matrix', u'Clifford Stein', u'Complex number', u'Composite number', u'Computational complexity theory', u'Connexions', u'Convolution', u'Convolution theorem', u'Cooley\u2013Tukey FFT algorithm', u'Coordinate vector', u'Coprime', u'Cornelius Lanczos', u'Cyclotomic polynomial', u'DFT matrix', u'Digital object identifier', u'Digital signal processing', u'Dirichlet series', u'Discrete Fourier transform', u'Discrete Hartley transform', u'Discrete cosine transform', u'Discrete sine transform', u'Distributed memory', u'Divide and conquer algorithm', u'Even and odd functions', u'FFT', u'FFTPACK', u'FFTW', u'FFT (disambiguation)', u'Factorization', u'Fast Folding Algorithm', u'Fast Walsh\u2013Hadamard transform', u'Fast multipole method', u'Finite field', u'Fixed-point FFT algorithms', u'Fixed-point arithmetic', u'Floating-point', u'Floating-point unit', u'Fourier analysis', u'Fourier transform on finite groups', u'Frank Yates', u'Frequency domain', u'G.C. Danielson', u'Generalized distributive law', u'Generating set of a group', u'Generating trigonometric tables', u'Gilbert Strang', u'Goertzel algorithm', u'Graph (mathematics)', u'Group (mathematics)', u'Group representation', u'Group theory', u'Hadamard transform', u'Hartley transform', u'International Standard Book Number', u'International Standard Serial Number', u'Introduction to Algorithms', u'J. W. Cooley', u'J. W. Tukey', u'JSTOR', u'James Cooley', u'John Tukey', u'Joseph Fourier', u'LIGO', u'List of unsolved problems in computer science', u'Lock-in amplifier', u'Math Kernel Library', u'Matrix (mathematics)', u'Matrix decomposition', u'Multidimensional transform', u'Multiplication algorithm', u'Non-uniform discrete Fourier transform', u'Number-theoretic transform', u'Number theory', u'Numerical analysis', u'Numerical stability', u'Nyquist\u2013Shannon sampling theorem', u'Odlyzko\u2013Sch\xf6nhage algorithm', u'Orders of magnitude', u'Orthogonality', u'Out-of-core', u'Overlap add', u'Overlap save', u'Pairwise summation', u'Parallel computing', u'Partial differential equation', u'Piotr Indyk', u'Pipeline (computing)', u'Polynomial', u'Power of two', u'Prime-factor FFT algorithm', u'Prime number', u'Primitive root of unity', u'Proof by exhaustion', u'Proportionality (mathematics)', u'Quantum Fourier transform', u'Quick Fourier transform algorithm', u""Rader's FFT algorithm"", u'Recurrence relation', u'Recursion', u'Richard Garwin', u'Ronald L. Rivest', u'Root mean square', u'Roots of unity', u'Round-off error', u'SIAM Journal on Scientific Computing', u'Satisfiability Modulo Theories', u'Sequence', u'Shmuel Winograd', u'Sinusoidal functions', u'Sparse matrix', u'Spectral estimation', u'Spectral music', u'Spectrum analyzer', u'Spherical harmonics', u'Split-radix FFT', u'Split-radix FFT algorithm', u'Thomas H. Cormen', u'Thomas J. Watson Research Center', u'Time series', u'Toeplitz matrix', u'Transpose', u'Trigonometric function', u'Twiddle factor', u'Upper bound', u'Vector-radix FFT algorithm', u'Wavelet', u'Wilkinson Microwave Anisotropy Probe', u'Winograd FFT algorithm']"
Fast folding algorithm,"In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data. It computes superpositions of the signal modulo various window sizes simultaneously.
The FFA is best known for its use in the detection of pulsars, as popularised by SETI@home and Astropulse.","[u'Algorithms and data structures stubs', u'All stub articles', u'Computer science stubs', u'Signal processing']","[u'Algorithm', u'Astropulse', u'Data structure', u'Periodic function', u'Pulsar', u'SETI@home', u'Signal processing', u'Time series']"
Faugère F4 algorithm,"In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring. The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.
The Faugère F5 algorithm first calculates the Gröbner basis of a pair of generator polynomials of the ideal. Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis:

If Gprev is an already computed Gröbner basis (f2, …, fm) and we want to compute a Gröbner basis of (f1) + Gprev then we will construct matrices whose rows are m f1 such that m is a monomial not divisible by the leading term of an element of Gprev.

This strategy allows the algorithm to apply two new criteria based on what Faugère calls signatures of polynomials. Thanks to these criteria, the algorithm can compute Gröbner bases for a large class of interesting polynomial systems, called regular sequences, without ever simplifying a single polynomial to zero—the most time-consuming operation in algorithms that compute Gröbner bases. It is also very effective for a large number of non-regular sequences.","[u'Algorithms and data structures stubs', u'All articles with unsourced statements', u'All stub articles', u'Articles with unsourced statements from February 2013', u'Computer algebra', u'Computer science stubs']","[u'Algorithm', u'ArXiv', u'Buchberger algorithm', u'Computer algebra', u'Data structure', u'Digital object identifier', u'Gr\xf6bner basis', u'Hidden Field Equations', u'Ideal (ring theory)', u'International Standard Book Number', u'International Standard Serial Number', u'Jean-Charles Faug\xe8re', u'Magma computer algebra system', u'Maple (software)', u'Maple computer algebra system', u'Polynomial ring', u'Regular sequence', u'SINGULAR', u'Sage (mathematics software)', u'Sparse matrix']"
Featherstone's algorithm,"Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an ""open kinematic chain"") such as a skeleton used in ragdoll physics.
The Featherstone's algorithm uses a reduced coordinate representation. This is in contrast to the more popular Lagrange multiplier method, which uses maximal coordinates. Brian Mirtich's PhD Thesis has a very clear and detailed description of the algorithm. Baraff's paper ""Linear-time dynamics using Lagrange multipliers"" has a discussion and comparison of both algorithms.","[u'Algorithms and data structures stubs', u'All stub articles', u'Computational physics', u'Computer physics engines', u'Computer science stubs', u'Mechanics']","[u'Algorithm', u'Data structure', u'International Standard Book Number', u'Kinematic chain', u'Lagrange multiplier method', u'Ragdoll physics', u'Skeleton']"
Fibonacci search technique,"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. Compared to binary search, Fibonacci search examines locations whose addresses have lower dispersion. Therefore, when the elements being searched have non-uniform access memory storage (i.e., the time needed to access a storage location varies depending on the location previously accessed), the Fibonacci search has an advantage over binary search in slightly reducing the average time needed to access a storage location. The typical example of non-uniform access storage is that of a magnetic tape, where the time to access a particular element is proportional to its distance from the element currently under the tape's head. Note, however, that large arrays not fitting in CPU cache or even in RAM can also be considered as non-uniform access examples. Fibonacci search has a complexity of O(log(n)) (see Big O notation).
Fibonacci search was first devised by Jack Kiefer (1953) as a minimax search for the maximum (minimum) of a unimodal function in an interval.","[u'All articles needing expert attention', u'All articles that are too technical', u'Articles needing expert attention from July 2013', u'Search algorithms', u'Wikipedia articles that are too technical from July 2013']","[u'Big O notation', u'Binary search algorithm', u'CPU cache', u'Computer science', u'Divide and conquer algorithm', u'Fibonacci number', u'Golden section search', u'Jack Kiefer (statistician)', u'Magnetic tape', u'Minimax', u'RAM', u'Search algorithms', u'Sorted array', u'The Art of Computer Programming', u'Unimodal function']"
Fisher–Yates shuffle,"The Fisher–Yates shuffle (named after Ronald Fisher and Frank Yates), also known as the Knuth shuffle (after Donald Knuth), is an algorithm for generating a random permutation of a finite set—in plain terms, for randomly shuffling the set. A variant of the Fisher–Yates shuffle, known as Sattolo's algorithm, may be used to generate random cyclic permutations of length n instead. The Fisher–Yates shuffle is unbiased, so that every permutation is equally likely. The modern version of the algorithm is also rather efficient, requiring only time proportional to the number of items being shuffled and no additional storage space.
Fisher–Yates shuffling is similar to randomly picking numbered tickets (combinatorics: distinguishable objects) out of a hat without replacement until there are none left.","[u'CS1 errors: external links', u'Combinatorial algorithms', u'Monte Carlo methods', u'Permutations', u'Randomness']","[u'-yllion', u'AMS Euler', u'Algorithm', u'Array data structure', u'Bernoulli process', u'Biased sample', u'Binomial distribution', u'C. R. Rao', u'CWEB', u'Combinatorics', u'Computer Modern', u'Computers and Typesetting', u'Concrete Mathematics', u'Concrete Roman', u'Cycle notation', u'Cyclic permutation', u'Dancing Links', u'Digital object identifier', u""Dijkstra's algorithm"", u'Donald E. Knuth', u'Donald Knuth', u'Expected value', u'Factorial', u'Finite set', u'Floating-point', u'Font', u'Frank Yates', u'GNU MIX Development Kit', u'International Standard Book Number', u'International Standard Serial Number', u""Knuth's Algorithm X"", u""Knuth's Simpath algorithm"", u'Knuth Prize', u'Knuth reward check', u'Knuth\u2013Bendix completion algorithm', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Linear congruential generator', u'Literate programming', u'METAFONT', u'MIX', u'MMIX', u'Man or boy test', u'Merge sort', u'Modulo operator', u'National Institute of Standards and Technology', u'OCLC', u'Off-by-one error', u'Pivot element', u'Playing card', u'Potrzebie', u'Power of 2', u'Prime factor', u'Pseudorandom', u'Pseudorandom number generator', u'Python (programming language)', u'Quater-imaginary base', u'Quicksort', u'RC4', u'Radix sort', u'Random number generator', u'Random permutation', u'Reservoir sampling', u'Richard Durstenfeld', u'Robinson\u2013Schensted\u2013Knuth correspondence', u'Ronald A. Fisher', u'Ronald Fisher', u'Sandra Sattolo', u'Selected papers series of Knuth', u'Sentinel value', u'Shuffling', u'Software', u'Surreal Numbers (book)', u'TeX', u'The Art of Computer Programming', u'The Complexity of Songs', u'Things a Computer Scientist Rarely Talks About', u'Time complexity', u'Trabb Pardo\u2013Knuth algorithm', u'Transitive relation', u'Uniform distribution (discrete)', u'WEB']"
Fitness proportionate selection,"Fitness proportionate selection, also known as roulette wheel selection, is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination.
In fitness proportionate selection, as in all selection methods, the fitness function assigns a fitness to possible solutions or chromosomes. This fitness level is used to associate a probability of selection with each individual chromosome. If  is the fitness of individual  in the population, its probability of being selected is , where  is the number of individuals in the population.
This could be imagined similar to a Roulette wheel in a casino. Usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value. This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. Then a random selection is made similar to how the roulette wheel is rotated.
While candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be. Contrast this with a less sophisticated selection algorithm, such as truncation selection, which will eliminate a fixed percentage of the weakest candidates. With fitness proportionate selection there is a chance some weaker solutions may survive the selection process; this is an advantage, as though a solution may be weak, it may include some component which could prove useful following the recombination process.
The analogy to a roulette wheel can be envisaged by imagining a roulette wheel in which each candidate solution represents a pocket on the wheel; the size of the pockets are proportionate to the probability of selection of the solution. Selecting N chromosomes from the population is equivalent to playing N games on the roulette wheel, as each candidate is drawn independently.
Other selection techniques, such as stochastic universal sampling or tournament selection, are often used in practice. This is because they have less stochastic noise, or are fast, easy to implement and have a constant selection pressure [Blickle, 1996].
The naive implementation is carried out by first generating the cumulative probability distribution (CDF) over the list of individuals using a probability proportional to the fitness of the individual. A uniform random number from the range [0,1) is chosen and the inverse of the CDF for that number gives an individual. This corresponds to the roulette ball falling in the bin of an individual with a probability proportional to its width. The ""bin"" corresponding to the inverse of the uniform random number can be found most quickly by using a binary search over the elements of the CDF. It takes in the O(log n) time to choose an individual. A faster alternative that generates individuals in O(1) time will be to use the alias method.
Recently, a very simple O(1) algorithm was introduced that is based on ""stochastic acceptance"". The algorithm randomly selects an individual (say ) and accepts the selection with probability , where  is the maximum fitness in the population. Certain analysis indicates that the stochastic acceptance version has a considerably better performance than versions based on linear or binary search, especially in applications where fitness values might change during the run.",[u'Genetic algorithms'],"[u'Alias method', u'Big O notation', u'Binary search algorithm', u'Chromosome', u'Cumulative probability distribution function', u'Fitness function', u'Genetic algorithm', u'Genetic operator', u'Probability', u'Reward-based selection', u'Stochastic universal sampling', u'Tournament selection', u'Truncation selection', u'Uniform distribution (continuous)']"
Flashsort,"Flashsort is a distribution sorting algorithm showing linear computational complexity  for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.

",[u'Sorting algorithms'],"[u'Adaptive sort', u'American flag sort', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'O notation', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Probability distribution', u'Proxmap sort', u'Quantile', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Fletcher's checksum,The Fletcher checksum is an algorithm for computing a position-dependent checksum devised by John G. Fletcher (1934-2012) at Lawrence Livermore Labs in the late 1970s. The objective of the Fletcher checksum was to provide error-detection properties approaching those of a cyclic redundancy check but with the lower computational effort associated with summation techniques.,"[u'All articles with unsourced statements', u'Articles with unsourced statements from May 2012', u'Checksum algorithms']","[u'Adler-32', u'Algorithm', u'Arithmetic overflow', u'Array data structure', u'Binary data', u'Byte', u'C (programming language)', u'Checksum', u'Cyclic redundancy check', u'Digital object identifier', u'End-around carry', u'For loop', u'IPv4 header checksum', u'Lawrence Livermore National Laboratory', u'Mark Adler', u'Modular Arithmetic', u'Modulo operation', u'Signed number representations', u'Subroutine', u'Triangular number', u'Variable (programming)']"
Flood fill,"Flood fill, also called seed fill, is an algorithm that determines the area connected to a given node in a multi-dimensional array. It is used in the ""bucket"" fill tool of paint programs to fill connected, similarly-colored areas with a different color, and in games such as Go and Minesweeper for determining which pieces are cleared. When applied on an image to fill a particular bounded area with color, it is also known as boundary fill.","[u'All articles lacking sources', u'Articles lacking sources from August 2009', u'Articles with example pseudocode', u'Computer graphics algorithms']","[u'Algorithm', u'Alpha compositing', u'Array data structure', u'Boundary value problem', u'Connected-component labeling', u""Dijkstra's algorithm"", u'Glossary of graph theory', u'Go (game)', u'Graph traversal', u'Graphic Adventure Creator', u'Inkscape', u'Java applet', u'Minesweeper (video game)', u'Paint program', u'Pixel connectivity', u'Queue (abstract data type)', u'Queue (data structure)', u'Recursion', u'Stack (data structure)']"
Floyd's cycle-finding algorithm,"In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values.
For any function ƒ that maps a finite set S to itself, and any initial value x0 in S, the sequence of iterated function values

must eventually use the same value twice: there must be some i ≠ j such that xi = xj. Once this happens, the sequence must continue periodically, by repeating the same sequence of values from xi to xj−1. Cycle detection is the problem of finding i and j, given ƒ and x0.

","[u'Articles with example Python code', u'Combinatorial algorithms', u'Fixed points (mathematics)']","[u'Abelian group', u'Alan Sherman', u'Algorithm', u'Andrew Yao', u'Average case analysis', u'Burt Kaliski', u'Celestial mechanics', u'Cellular automaton', u'Claus P. Schnorr', u'Common Lisp', u'Computational group theory', u'Computer program', u'Computer science', u'Computer simulation', u'Cryptographic hash function', u'Cryptography', u'Cycle detection (graph theory)', u'Cycle graph', u'Data Encryption Standard', u'Data structure', u'Digital object identifier', u'Directed graph', u'Discrete logarithm', u'Donald Knuth', u'Eric Allender', u'Faith Ellen', u'Finite set', u'Function (mathematics)', u'Functional graph', u'Glossary of graph theory', u'Graph theory', u'Greatest common divisor', u'Hash collision', u'Hash table', u'Hendrik Lenstra', u'Infinite loop', u'Integer factorization', u'Iterated function', u'JSTOR', u'Linear congruential generator', u'Linked list', u'Link\xf6ping University', u'Maria Klawe', u'Mathematical folklore', u'Number theory', u'Oscillator (cellular automaton)', u'Periodic sequence', u'Phase space', u'Pointer algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's rho algorithm"", u'Power of two', u'Pseudorandom number generator', u'Python (programming language)', u'Reachability', u'Rho (letter)', u'Richard Brent (scientist)', u'Robert Sedgewick (computer scientist)', u'Robert W. Floyd', u'Ron Rivest', u'S-expression', u'Sequence', u'Shape analysis (software)', u'Space complexity', u'Stack (data structure)', u'The Tortoise and the Hare', u'William Kahan']"
Floyd–Steinberg dithering,"Floyd–Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg. It is commonly used by image manipulation software, for example when an image is converted into GIF format that is restricted to a maximum of 256 colors.
The algorithm achieves dithering using error diffusion, meaning it pushes (adds) the residual quantization error of a pixel onto its neighboring pixels, to be dealt with later. It spreads the debt out according to the distribution (shown as a map of the neighboring pixels):

The pixel indicated with a star (*) indicates the pixel currently being scanned, and the blank pixels are the previously-scanned pixels. The algorithm scans the image from left to right, top to bottom, quantizing pixel values one by one. Each time the quantization error is transferred to the neighboring pixels, while not affecting the pixels that already have been quantized. Hence, if a number of pixels have been rounded downwards, it becomes more likely that the next pixel is rounded upwards, such that on average, the quantization error is close to zero.
The diffusion coefficients have the property that if the original pixel values are exactly halfway in between the nearest available colors, the dithered result is a checkerboard pattern. For example 50% grey data could be dithered as a black-and-white checkerboard pattern. For optimal dithering, the counting of quantization errors should be in sufficient accuracy to prevent rounding errors from affecting the result.
In some implementations, the horizontal direction of scan alternates between lines; this is called ""serpentine scanning"" or boustrophedon transform dithering.
In pseudocode:

for each y from top to bottom
   for each x from left to right
      oldpixel  := pixel[x][y]
      newpixel  := find_closest_palette_color(oldpixel)
      pixel[x][y]  := newpixel
      quant_error  := oldpixel - newpixel
      pixel[x+1][y  ] := pixel[x+1][y  ] + quant_error * 7/16
      pixel[x-1][y+1] := pixel[x-1][y+1] + quant_error * 3/16
      pixel[x  ][y+1] := pixel[x  ][y+1] + quant_error * 5/16
      pixel[x+1][y+1] := pixel[x+1][y+1] + quant_error * 1/16

When converting 16 bit greyscale to 8 bit, find_closest_palette_color() may perform just a simple rounding, for example:

find_closest_palette_color(oldpixel) = floor(oldpixel / 256)

","[u'Articles with example code', u'Articles with example pseudocode', u'Computer graphics algorithms', u'Image processing']","[u'Boustrophedon transform', u'David (Michelangelo)', u'Dithering', u'Error diffusion', u'GIF', u'List of monochrome and RGB palettes', u'Louis Steinberg', u'Pixel', u'Pseudocode', u'Quantization error', u'Robert W. Floyd']"
Floyd–Warshall algorithm,"In computer science, the Floyd–Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of vertices, though it does not return details of the paths themselves. Versions of the algorithm can also be used for finding the transitive closure of a relation , or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.","[u'Articles with example pseudocode', u'Commons category without a link on Wikidata', u'Dynamic programming', u'Graph algorithms', u'Polynomial-time problems', u'Routing algorithms']","[u'A* search algorithm', u'Algorithm', u'All-pairs shortest path problem', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Bernard Roy', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u'Big theta', u'Binary heap', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'C++', u'C. R. Acad. Sci. Paris', u'C Sharp (programming language)', u'Charles E. Leiserson', u'Claude Elwood Shannon', u'Communications of the ACM', u'Computational complexity theory', u'Computer science', u'Cycle (graph theory)', u'D*', u'Dense graph', u'Depth-first search', u'Depth-limited search', u'Deterministic finite automaton', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dorit S. Hochbaum', u'Dynamic programming', u""Edmonds' algorithm"", u'Eric W. Weisstein', u'Fast matrix multiplication', u'Finite automaton', u""Floyd's algorithm"", u""Floyd's cycle-finding algorithm"", u'Floyd\u2013Steinberg dithering', u'Fringe search', u'Gauss\u2013Jordan elimination', u'Graph (data structure)', u'Graph traversal', u'Hill climbing', u'International Standard Book Number', u'Introduction to Algorithms', u'Invertible matrix', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Java (programming language)', u'John McCarthy (computer scientist)', u""Johnson's algorithm"", u'Journal of the ACM', u'Jump point search', u""Kleene's algorithm"", u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Logical conjunction', u'Logical disjunction', u'MATLAB', u'MathWorld', u'Matrix (mathematics)', u'NetworkX', u'PDF', u'Pathfinder network', u'Perl', u""Prim's algorithm"", u'Programming language', u'Python (programming language)', u'R programming language', u'Real number', u'Recursion', u'Regular expression', u'Regular language', u'Robert Floyd', u'Robert W. Floyd', u'Ron Rivest', u'SIAM Journal on Computing', u'SMA*', u'Schulze method', u'SciPy', u'Search game', u'Shortest-path tree', u'Shortest path problem', u'Sparse graph', u'Stephen Cole Kleene', u'Stephen Warshall', u'Thomas H. Cormen', u'Timothy M. Chan', u'Transitive closure', u'Tree traversal', u'University of California, Berkeley', u'Uri Zwick', u'Weighted graph', u'Widest path problem']"
Force-based algorithms (graph drawing),"Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically pleasing way. Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space so that all the edges are of more or less equal length and there are as few crossing edges as possible, by assigning forces among the set of edges and the set of nodes, based on their relative positions, and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.
While graph drawing can be a difficult problem, force-directed algorithms, being physical simulations, usually require no special knowledge about graph theory such as planarity.

","[u'Articles with example pseudocode', u'Graph algorithms', u'Graph drawing', u'Pages containing cite templates with deprecated parameters']","[u'Algorithm', u'Angular resolution (graph drawing)', u'ArXiv', u'Barnes\u2013Hut simulation', u'Centrality', u'Circular arc', u'Connected component (graph theory)', u'Convex position', u""Coulomb's law"", u'Cytoscape', u'David Eppstein', u'David Harel', u'Digital object identifier', u'Dorothea Wagner', u'Edward Reingold', u'Electric charge', u'Euclidean distance', u'Genetic algorithm', u'Gephi', u'Global optimization', u'Graph (mathematics)', u'Graph drawing', u'Graphviz', u""Hooke's law"", u'International Standard Book Number', u'LearnDiscovery \u2013 mindmap of Wikipedia (software)', u'Limit of a sequence', u'Local minimum', u'Mechanical equilibrium', u'Michael T. Goodrich', u'Multidimensional scaling', u'N-body', u'N-body problem', u'N-body simulation', u'Online algorithm', u'Optimization (mathematics)', u'Peter Eades', u'Planar graph', u'Polyhedral graph', u'Prefuse', u'Roberto Tamassia', u'Running time', u'Simulated annealing', u'Spline curve', u'Spring (device)', u'Stress majorization', u'Tulip (software)', u'Tutte embedding', u'W. T. Tutte']"
Ford–Fulkerson algorithm,"The Ford–Fulkerson method or Ford–Fulkerson algorithm (FFA) is an algorithm that computes the maximum flow in a flow network. It is called a ""method"" instead of an ""algorithm"" as the approach to finding augmenting paths in a residual graph is not fully specified or it is specified in several implementations with different running times. It was published in 1956 by L. R. Ford, Jr. and D. R. Fulkerson. The name ""Ford–Fulkerson"" is often also used for the Edmonds–Karp algorithm, which is a specialization of Ford–Fulkerson.
The idea behind the algorithm is as follows: as long as there is a path from the source (start node) to the sink (end node), with available capacity on all edges in the path, we send flow along one of the paths. Then we find another path, and so on. A path with available capacity is called an augmenting path.","[u'Articles with example pseudocode', u'Graph algorithms', u'Network flow']","[u'Algorithm', u'Approximate max-flow min-cut theorem', u'Augmenting path', u'Big O notation', u'Breadth-first search', u'Canadian Journal of Mathematics', u'Charles E. Leiserson', u'Clifford Stein', u'D. R. Fulkerson', u'Depth-first search', u'Digital object identifier', u'Edmonds\u2013Karp algorithm', u'Flow network', u'International Standard Book Number', u'Introduction to Algorithms', u'L. R. Ford, Jr.', u'Max-flow min-cut theorem', u'Maximum flow problem', u'Oreilly Media', u'Ronald L. Rivest', u'Theoretical Computer Science (journal)', u'Thomas H. Cormen']"
Forward-backward algorithm,"The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e. it computes, for all hidden state variables , the distribution . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to compute efficiently the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.
The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.","[u'Dynamic programming', u'Error detection and correction', u'Machine learning algorithms', u'Markov models']","[u'Algorithm', u'BCJR algorithm', u'Baum-Welch algorithm', u""Bayes' rule"", u'Belief propagation', u'Conditional independence', u'Dynamic programming', u'Hidden Markov model', u'Hidden Markov models', u'IEEE', u'Inference', u'International Standard Book Number', u'Island algorithm', u'Lawrence Rabiner', u'Marginal probability', u'Posterior probability', u'Python programming language', u'Time complexity', u'Viterbi algorithm']"
Fractal compression,"Fractal compression is a lossy compression method for digital images, based on fractals. The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image. Fractal algorithms convert these parts into mathematical data called ""fractal codes"" which are used to recreate the encoded image.","[u'All articles with unsourced statements', u'Articles with unsourced statements from August 2009', u'Articles with unsourced statements from March 2008', u'Fractals', u'Image compression', u'Lossy compression algorithms', u'Pages containing cite templates with deprecated parameters']","[u'A-law algorithm', u'ACM SIGGRAPH', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithms', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bicubic interpolation', u'Bilinear interpolation', u'Binary image', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'CD-ROM', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Compact space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Contraction mapping', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital image', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Encarta', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fiasco (image format)', u'Fibonacci coding', u'Film frame', u'Fixed point iteration', u'Fourier transform', u'Fractal', u'Fractal transform', u'Frame rate', u'Genuine Fractals', u'Golomb coding', u'Graph of a function', u'Grayscale', u'Huffman coding', u'Hutchinson operator', u'IEEE Geoscience and Remote Sensing Society', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'International Standard Book Number', u'Interpolant', u'Iterated function system', u'JPEG', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Linux Journal', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Michael Barnsley', u'Microsoft', u'Mitsubishi', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Morgan Kaufmann Publishers', u'Motion compensation', u'Move-to-front transform', u'Netpbm', u'Nyquist\u2013Shannon sampling theorem', u'OnOne Software', u'PAQ', u'Patent', u'Peak signal-to-noise ratio', u'Photoshop', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'RealVideo', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Spectrum Holobyte', u'Speech coding', u'Standard test image', u'Star Trek: The Next Generation A Final Unity', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'University of South Carolina', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet', u'Wavelet compression', u'Windows Media Player', u'\u039c-law algorithm']"
Freivalds' algorithm,"Freivalds' algorithm (named after Rusins Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. Given three n × n matrices A, B, and C, a general problem is to verify whether A × B = C. A naïve algorithm would compute the product A × B explicitly and compare term by term whether this product equals C. However, the best known matrix multiplication algorithm runs in O(n2.3729) time. Freivalds' algorithm utilizes randomization in order to reduce this time bound to O(n2)  with high probability. In O(kn2) time the algorithm can verify a matrix product with probability of failure less than .","[u'Articles containing proofs', u'Matrix multiplication algorithms', u'Matrix theory', u'Randomized algorithms']","[u'Algorithm', u'Basic Linear Algebra Subprograms', u""Bayes' Theorem"", u'Big O notation', u'CPU cache', u'Cache-oblivious algorithm', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Coppersmith\u2013Winograd algorithm', u'Deterministic algorithm', u'Digital object identifier', u'Error bound', u'Floating point', u'Matrix (mathematics)', u'Matrix decomposition', u'Matrix multiplication', u'Matrix multiplication algorithm', u'Multiprocessing', u'Numerical linear algebra', u'Numerical stability', u'One-sided error', u'Probabilistic algorithm', u'Probability', u'Random', u'Randomization', u'Randomized algorithm', u'Randomized algorithms', u'SIMD', u'Schwartz\u2013Zippel lemma', u'Sparse matrix', u'System of linear equations', u'Translation lookaside buffer', u'Vector (geometric)', u'With high probability']"
Fuzzy clustering,"Data clustering is the process of dividing data elements into classes or clusters so that items in the same class are as similar as possible, and items in different classes are as dissimilar as possible. Depending on the nature of the data and the purpose for which clustering is being used, different measures of similarity may be used to place items into classes, where the similarity measure controls how the clusters are formed. Some examples of measures that can be used as in clustering include distance, connectivity, and intensity.
In hard clustering, data is divided into distinct clusters, where each data element belongs to exactly one cluster. In fuzzy clustering (also referred to as soft clustering), data elements can belong to more than one cluster, and associated with each element is a set of membership levels. These indicate the strength of the association between that data element and a particular cluster. Fuzzy clustering is a process of assigning these membership levels, and then using them to assign data elements to one or more clusters.
One of the most widely used fuzzy clustering algorithms is the Fuzzy C-Means (FCM) Algorithm (Bezdek 1981). The FCM algorithm attempts to partition a finite collection of  elements  into a collection of c fuzzy clusters with respect to some given criterion. Given a finite set of data, the algorithm returns a list of  cluster centres  and a partition matrix , where each element  tells the degree to which element  belongs to cluster . Like the K-means clustering, the FCM aims to minimize an objective function:

where:

This differs from the k-means objective function by the addition of the membership values  and the fuzzifier , with . The fuzzifier  determines the level of cluster fuzziness. A large  results in smaller memberships  and hence, fuzzier clusters. In the limit , the memberships  converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge,  is commonly set to 2.","[u'All articles lacking in-text citations', u'All articles needing cleanup', u'Articles lacking in-text citations from August 2009', u'Articles needing cleanup from October 2011', u'Articles with inconsistent citation formats', u'Cleanup tagged articles with a reason field from October 2011', u'Data clustering algorithms', u'Wikipedia pages needing cleanup from October 2011']","[u'Cluster Analysis', u'Data clustering', u'Determining the number of clusters in a data set', u'Digital object identifier', u'Expectation-maximization algorithm', u'FLAME Clustering', u'Fuzzy logic', u'Hard clustering', u'International Standard Book Number', u'K-means clustering', u'PubMed Identifier', u'Soft K-means']"
Fürer's algorithm,"Fürer's algorithm is an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity. It was created in 2007 by Swiss mathematician Martin Fürer of Pennsylvania State University as an asymptotically faster (when analysed on a multitape Turing machine) algorithm than its predecessor, the Schönhage–Strassen algorithm published in 1971.
The predecessor to the Fürer algorithm, the Schönhage–Strassen algorithm, used fast Fourier transforms to compute integer products in time  (in big O notation) and its authors, Arnold Schönhage and Volker Strassen, also conjectured a lower bound for the problem of . Here,  denotes the total number of bits in the two input numbers. Fürer's algorithm reduces the gap between these two bounds: it can be used to multiply binary integers of length  in time  (or by a circuit with that many logic gates), where log*n represents the iterated logarithm operation. However, the difference between the  and  factors in the time bounds of the Schönhage–Strassen algorithm and the Fürer algorithm for realistic values of  is very small.
In 2008, Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi gave a similar algorithm that relies on modular arithmetic instead of complex arithmetic to achieve the same running time.
In 2014, David Harvey, Joris van der Hoeven, and Grégoire Lecerf showed that an optimized version of Fürer's algorithm achieves a running time of , making the implied constant in the  exponent explicit. They also gave a modification to Fürer's algorithm that achieves 
In 2015 Svyatoslav Covanov and Emmanuel Thomé provided another modifications that achieves same running time. Yet, as all the other implementation, it's still not practical.

","[u'Algorithms and data structures stubs', u'All articles with unsourced statements', u'All stub articles', u'Articles with unsourced statements from June 2015', u'Computer arithmetic algorithms', u'Computer science stubs']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'ArXiv', u'Arnold Sch\xf6nhage', u'Asymptotic complexity', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big O notation', u'Binary GCD algorithm', u'Boolean circuit', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Data structure', u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u'Fast Fourier transform', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer multiplication algorithm', u'Integer square root', u'Iterated logarithm', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lower bound', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Martin F\xfcrer', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Multiplication algorithm', u'Number-theoretic transform', u'Number theory', u'Pennsylvania State University', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Switzerland', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Turing machine', u'Volker Strassen', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
GLR parser,"A GLR parser (GLR standing for ""generalized LR"", where L stands for ""left-to-right"" and R stands for ""rightmost (derivation)"") is an extension of an LR parser algorithm to handle nondeterministic and ambiguous grammars. The theoretical foundation was provided in a 1974 paper by Bernard Lang (along with other general Context-Free parsers such as GLL). It describes a systematic way to produce such algorithms, and provides uniform results regarding correctness proofs, complexity with respect to grammar classes, and optimization techniques. The first actual implementation of GLR was described in a 1984 paper by Masaru Tomita, it has also been referred to as a ""parallel parser"". Tomita presented five stages in his original work, though in practice it is the second stage that is recognized as the GLR parser.
Though the algorithm has evolved since its original forms, the principles have remained intact. As shown by an earlier publication, Lang was primarily interested in more easily used and more flexible parsers for extensible programming languages. Tomita's goal was to parse natural language text thoroughly and efficiently. Standard LR parsers cannot accommodate the nondeterministic and ambiguous nature of natural language, and the GLR algorithm can.","[u'All articles covered by WikiProject Wikify', u'All articles lacking in-text citations', u'All articles with unsourced statements', u'All pages needing cleanup', u'Articles covered by WikiProject Wikify from February 2015', u'Articles lacking in-text citations from May 2011', u'Articles with unsourced statements from May 2011', u'Parsing algorithms', u'Wikipedia introduction cleanup from February 2015']","[u'ASF+SDF Meta Environment', u'Ambiguous grammar', u'Breadth-first search', u'CYK algorithm', u'Comparison of parser generators', u'DMS Software Reengineering Toolkit', u'Digital object identifier', u'Directed acyclic graph', u'Earley algorithm', u'GNU Bison', u'International Standard Book Number', u'International Standard Serial Number', u'LALR', u'LR parser', u'LR parsers', u'Masaru Tomita', u'Mathematical optimization', u'Natural language', u'Nondeterministic grammar', u'Parser generator', u'State transition']"
Gaussian elimination,"In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the associated matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and in every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.

Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.","[u'All articles to be merged', u'All articles with specifically marked weasel-worded phrases', u'Articles to be merged from March 2013', u'Articles with example pseudocode', u'Articles with specifically marked weasel-worded phrases from January 2014', u'Exchange algorithms', u'Numerical linear algebra', u'Pages using citations with accessdate and no URL', u'Pages using web citations with no URL']","[u'Absolute value', u'Addison-Wesley', u'Algorithm', u'American Mathematical Monthly', u'ArXiv', u'Argmax', u'Arithmetic', u'Array data structure', u'Augmented matrix', u'BLAS', u'Bareiss algorithm', u'Basic Linear Algebra Subprograms', u'Basis (linear algebra)', u'Big O notation', u'Bit complexity', u'Block matrix', u'Carl Friedrich Gauss', u'Charles F. Van Loan', u'Column space', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Computer hardware', u""Cramer's rule"", u'Determinant', u'Diagonally dominant', u'Digital object identifier', u'Dot product', u'Dual space', u'Eigenvalues and eigenvectors', u'Elementary matrix', u'Elementary row operations', u'Euclidean vector', u'Field (mathematics)', u'Finite field', u'Floating point', u'Floating point number', u'Frobenius matrix', u'Gene H. Golub', u'Gram\u2013Schmidt process', u'Human computer', u'Identity matrix', u'Inner product space', u'Integer', u'International Standard Book Number', u'International Standard Serial Number', u'Invertible matrix', u'Isaac Newton', u'Iterative method', u'JSTOR', u'John Wiley & Sons', u'Kernel (linear algebra)', u'LU decomposition', u'Leading coefficient', u'Library (computing)', u'Linear algebra', u'Linear combination', u'Linear independence', u'Linear map', u'Linear span', u'Liu Hui', u'MATLAB', u'Matrix (mathematics)', u'Matrix decomposition', u'Matrix multiplication', u'McGraw-Hill', u'Minor (linear algebra)', u'Nicholas Higham', u'Numerical linear algebra', u'Numerical stability', u'O notation', u'Ordinary least squares', u'Orthogonality', u'Outer product', u'Pivot element', u'Positive-definite matrix', u'Prentice Hall', u'Projection (linear algebra)', u'Rank (linear algebra)', u'Rank of a matrix', u'Rational number', u'Reduced row echelon form', u'Rod calculus', u'Row-echelon form', u'Row echelon form', u'Row space', u'Scalar (mathematics)', u'Society for Industrial and Applied Mathematics', u'Sparse matrix', u'System of linear equations', u'Systems of linear equations', u'Tensors', u'The Nine Chapters on the Mathematical Art', u'Transformation matrix', u'Transpose', u'Triangular form', u'Triangular matrix', u'Vector projection', u'Vector space', u'Wiley-Interscience', u'Wilhelm Jordan (geodesist)', u'YouTube']"
Gauss–Jordan elimination,"In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the associated matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and in every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.

Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.","[u'All articles to be merged', u'All articles with specifically marked weasel-worded phrases', u'Articles to be merged from March 2013', u'Articles with example pseudocode', u'Articles with specifically marked weasel-worded phrases from January 2014', u'Exchange algorithms', u'Numerical linear algebra', u'Pages using citations with accessdate and no URL', u'Pages using web citations with no URL']","[u'Absolute value', u'Addison-Wesley', u'Algorithm', u'American Mathematical Monthly', u'ArXiv', u'Argmax', u'Arithmetic', u'Array data structure', u'Augmented matrix', u'BLAS', u'Bareiss algorithm', u'Basic Linear Algebra Subprograms', u'Basis (linear algebra)', u'Big O notation', u'Bit complexity', u'Block matrix', u'Carl Friedrich Gauss', u'Charles F. Van Loan', u'Column space', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Computer hardware', u""Cramer's rule"", u'Determinant', u'Diagonally dominant', u'Digital object identifier', u'Dot product', u'Dual space', u'Eigenvalues and eigenvectors', u'Elementary matrix', u'Elementary row operations', u'Euclidean vector', u'Field (mathematics)', u'Finite field', u'Floating point', u'Floating point number', u'Frobenius matrix', u'Gene H. Golub', u'Gram\u2013Schmidt process', u'Human computer', u'Identity matrix', u'Inner product space', u'Integer', u'International Standard Book Number', u'International Standard Serial Number', u'Invertible matrix', u'Isaac Newton', u'Iterative method', u'JSTOR', u'John Wiley & Sons', u'Kernel (linear algebra)', u'LU decomposition', u'Leading coefficient', u'Library (computing)', u'Linear algebra', u'Linear combination', u'Linear independence', u'Linear map', u'Linear span', u'Liu Hui', u'MATLAB', u'Matrix (mathematics)', u'Matrix decomposition', u'Matrix multiplication', u'McGraw-Hill', u'Minor (linear algebra)', u'Nicholas Higham', u'Numerical linear algebra', u'Numerical stability', u'O notation', u'Ordinary least squares', u'Orthogonality', u'Outer product', u'Pivot element', u'Positive-definite matrix', u'Prentice Hall', u'Projection (linear algebra)', u'Rank (linear algebra)', u'Rank of a matrix', u'Rational number', u'Reduced row echelon form', u'Rod calculus', u'Row-echelon form', u'Row echelon form', u'Row space', u'Scalar (mathematics)', u'Society for Industrial and Applied Mathematics', u'Sparse matrix', u'System of linear equations', u'Systems of linear equations', u'Tensors', u'The Nine Chapters on the Mathematical Art', u'Transformation matrix', u'Transpose', u'Triangular form', u'Triangular matrix', u'Vector projection', u'Vector space', u'Wiley-Interscience', u'Wilhelm Jordan (geodesist)', u'YouTube']"
Gauss–Legendre algorithm,"The Gauss–Legendre algorithm is an algorithm to compute the digits of π. It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of π. However, the drawback is that it is memory intensive and it is therefore sometimes not used over Machin-like formulas.
The method is based on the individual work of Carl Friedrich Gauss (1777–1855) and Adrien-Marie Legendre (1752–1833) combined with modern algorithms for multiplication and square roots. It repeatedly replaces two numbers by their arithmetic and geometric mean, in order to approximate their arithmetic-geometric mean.
The version presented below is also known as the Gauss–Euler, Brent–Salamin (or Salamin–Brent) algorithm; it was independently discovered in 1975 by Richard Brent and Eugene Salamin. It was used to compute the first 206,158,430,000 decimal digits of π on September 18 to 20, 1999, and the results were checked with Borwein's algorithm.",[u'Pi algorithms'],"[u'Adrien-Marie Legendre', u'Algorithm', u'Arithmetic-geometric mean', u'Arithmetic mean', u'Arithmetic\u2013geometric mean', u""Borwein's algorithm"", u'Carl Friedrich Gauss', u'Elliptic integral', u'Eugene Salamin (mathematician)', u'Geometric mean', u'International Standard Serial Number', u'Machin-like formulas', u'Numerical approximations of \u03c0', u'Pi', u'Richard Brent (scientist)', u'Square root']"
Gauss–Newton algorithm,"The Gauss–Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss–Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.
Non-linear least squares problems arise for instance in non-linear regression, where parameters in a model are sought such that the model is in good agreement with available observations.
The method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton.

","[u'Least squares', u'Optimization algorithms and methods', u'Statistical algorithms']","[u'Abraham de Moivre', u'Absolute time and space', u'An Historical Account of Two Notable Corruptions of Scripture', u'Analysis of covariance', u'Analysis of variance', u'Approximation algorithm', u'Approximation theory', u'Arithmetica Universalis', u'Augmented Lagrangian method', u'BFGS method', u'Barrier function', u'Bayesian experimental design', u'Bellman\u2013Ford algorithm', u'Benjamin Pulleyn', u'Binomial regression', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Bucket argument', u'Calculus', u'Calibration curve', u'Carl Friedrich Gauss', u'Catherine Barton', u'Chebyshev nodes', u'Chebyshev polynomials', u'Cholesky decomposition', u'Classical mechanics', u'Column vectors', u'Combinatorial optimization', u'Comparison of optimization software', u'Computational statistics', u'Confounding', u'Conjugate gradient', u'Convex minimization', u'Convex optimization', u'Copernican Revolution', u'Corpuscular theory of light', u'Correlation and dependence', u'Cranbury Park', u'Criss-cross algorithm', u'Curve fitting', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'De analysi per aequationes numero terminorum infinitas', u'De motu corporum in gyrum', u'Descent direction', u'Design of experiments', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Direction (geometry, geography)', u'Dynamic programming', u'Dynamics (mechanics)', u'Early life of Isaac Newton', u'Edmonds\u2013Karp algorithm', u'Elements of the Philosophy of Newton', u'Ellipsoid method', u'Errors and residuals in statistics', u'Evolutionary algorithm', u'Exchange algorithm', u'Finite difference', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gaussian quadrature', u'Gauss\u2013Markov theorem', u'General Scholium', u'General linear model', u'Generalized Gauss\u2013Newton method', u'Generalized least squares', u'Generalized linear model', u'Golden section search', u'Goodness of fit', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Gravitational constant', u'Greedy algorithm', u'Growth curve (statistics)', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Hypotheses non fingo', u'Ill-conditioned', u'Impact depth', u'Inertia', u'Integer programming', u'International Standard Book Number', u'Isaac Barrow', u'Isaac Newton', u""Isaac Newton's occult studies"", u'Isaac Newton S/O Philipose', u'Isaac Newton in popular culture', u'Isotonic regression', u'Iterative method', u'Iteratively reweighted least squares', u'Jacobian matrix', u'John Conduitt', u'John Keill', u'John Wiley & Sons', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u'Kendall tau rank correlation coefficient', u""Kepler's laws of planetary motion"", u'Kissing number problem', u""Kruskal's algorithm"", u'Later life of Isaac Newton', u'Least squares', u'Leibniz\u2013Newton calculus controversy', u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear approximation', u'Linear least squares (mathematics)', u'Linear programming', u'Linear regression', u'List of statistics articles', u'List of things named after Isaac Newton', u'Local convergence', u'Local regression', u'Local search (optimization)', u'Logistic regression', u""Mallows's Cp"", u'Mathematical optimization', u'Matrix transpose', u'Matroid', u'Maxima and minima', u'Mean and predicted response', u'Metaheuristic', u'Method of Fluxions', u'Minimum mean-square error', u'Minimum spanning tree', u'Model selection', u'Moving least squares', u'Multivariate analysis of variance', u'Nelder\u2013Mead method', u""Newton's cannonball"", u""Newton's cradle"", u""Newton's identities"", u""Newton's inequalities"", u""Newton's law of cooling"", u""Newton's law of universal gravitation"", u""Newton's laws of motion"", u""Newton's metal"", u""Newton's method"", u""Newton's method in optimization"", u""Newton's reflector"", u""Newton's rings"", u""Newton's theorem about ovals"", u""Newton's theorem of revolving orbits"", u'Newton (Blake)', u'Newton (unit)', u'Newton disc', u'Newton fractal', u'Newton polygon', u'Newton polynomial', u'Newton scale', u'Newtonian dynamics', u'Newtonian fluid', u'Newtonian potential', u'Newtonian telescope', u'Newtonianism', u'Newton\u2013Cartan theory', u'Newton\u2013Cotes formulas', u'Newton\u2013Euler equations', u'Newton\u2013Okounkov body', u'Newton\u2013Pepys problem', u'Non-linear least squares', u'Non-linear regression', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Nonlinear regression', u'Nonparametric regression', u'Notes on the Jewish Temple', u'Numerical analysis', u'Numerical integration', u'Numerical smoothing and differentiation', u'Opticks', u'Optimal design', u'Optimization algorithm', u'Ordinary least squares', u'Orthogonal polynomials', u'Outline of statistics', u'Parameterized post-Newtonian formalism', u'Partial correlation', u'Partial least squares', u'Partition of sums of squares', u'Pearson product-moment correlation coefficient', u'Penalty method', u'Philosophi\xe6 Naturalis Principia Mathematica', u'Poisson regression', u'Polynomial regression', u'Post-Newtonian expansion', u""Powell's method"", u'Power number', u'Problem of Apollonius', u'Push\u2013relabel maximum flow algorithm', u'QR factorization', u'Quadratic programming', u'Quaestiones quaedam philosophicae', u'Quantile regression', u'Quasi-Newton method', u'Rank correlation', u'Rate of convergence', u'Regression analysis', u'Regression model validation', u'Religious views of Isaac Newton', u'Residual (statistics)', u'Response surface methodology', u'Revised simplex algorithm', u'Ridge regression', u'Robust regression', u'Rotating spheres', u'Schr\xf6dinger\u2013Newton equation', u'Scientific revolution', u'Segmented regression', u'Semiparametric regression', u'Sequential quadratic programming', u'Sextant', u'Simple linear regression', u'Simplex algorithm', u'Simulated annealing', u'Solar mass', u""Spearman's rank correlation coefficient"", u'Standing on the shoulders of giants', u'Stationary point', u'Statistical model', u'Steepest descent', u'Stepwise regression', u'Structural coloration', u'Studentized residual', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'System identification', u'Table of Newtonian series', u'Tabu search', u""Taylor's theorem"", u'The Chronology of Ancient Kingdoms Amended', u'The Mysteryes of Nature and Art', u'The Queries', u'Total least squares', u'Truncated Newton method', u'Trust region', u'Weighted least squares', u'William Clarke (apothecary)', u'William Jones (mathematician)', u'William Stukeley', u'Wolfe conditions', u'Woolsthorpe Manor', u'Writing of Principia Mathematica']"
Gene expression programming,"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype-phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.","[u'Evolutionary algorithms', u'Evolutionary computation', u'Gene expression programming', u'Genetic algorithms', u'Genetic programming', u'Wikipedia articles with possible conflicts of interest from November 2012']","[u'Artificial intelligence', u'Artificial neural network', u'Boolean algebra (logic)', u'C (programming language)', u'Chromosome', u'Computer program', u'Computer programming', u'Confusion matrix', u'Correlation and dependence', u'Crossover (genetic algorithm)', u'DNA double helix', u'Decision trees', u'Evolution strategies', u'Evolutionary algorithm', u'Evolutionary algorithms', u'Exclusive or', u'Expression tree', u'F-measure', u'Fitness function', u'Fitness landscape', u'Fortran', u'Gene', u'GeneXproTools', u'Gene expression programming', u'Genetic algorithms', u'Genetic operator', u'Genetic operators', u'Genetic programming', u'Genotype', u'Genotype-phenotype distinction', u'Gepsoft', u'Google Code', u'Hinge loss', u'International Standard Book Number', u'Jaccard similarity', u'Java (programming language)', u'Karva notation', u'Logic synthesis', u'Logistic regression', u'Loss function', u'Machine learning', u'Matthews correlation coefficient', u'Maximum likelihood estimation', u'Mean absolute error', u'Mean squared error', u'Mutation (genetic algorithm)', u'Parse tree', u'Pearson product-moment correlation coefficient', u'Phenotype', u'Precision and recall', u'Predictive analytics', u'Python (programming language)', u'R-square', u'Receiver operating characteristic', u'Recursion (computer science)', u'Regression analysis', u'Root mean squared error', u'Roulette-wheel selection', u'Sensitivity and specificity', u'SourceForge', u'Statistical classification', u'Time series prediction', u'Transposition (genetics)', u'Tree structure']"
General number field sieve,"In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 100 digits. Heuristically, its complexity for factoring an integer n (consisting of  bits) is of the form

(in L-notation), where ln is the natural logarithm. It is a generalization of the special number field sieve: while the latter can only factor numbers of a certain special form, the general number field sieve can factor any number apart from prime powers (which are trivial to factor by taking roots). When the term number field sieve (NFS) is used without qualification, it refers to the general number field sieve.
The principle of the number field sieve (both special and general) can be understood as an improvement to the simpler rational sieve or quadratic sieve. When using such algorithms to factor a large number n, it is necessary to search for smooth numbers (i.e. numbers with small prime factors) of order n1/2. The size of these values is exponential in the size of n (see below). The general number field sieve, on the other hand, manages to search for smooth numbers that are subexponential in the size of n. Since these numbers are smaller, they are more likely to be smooth than the numbers inspected in previous algorithms. This is the key to the efficiency of the number field sieve. In order to achieve this speed-up, the number field sieve has to perform computations and factorizations in number fields. This results in many rather complicated aspects of the algorithm, as compared to the simpler rational sieve.
Note that log2 n is the number of bits in the binary representation of n, that is the size of the input to the algorithm, so any element of the order nc for a constant c is exponential in log n. The running time of the number field sieve is super-polynomial but sub-exponential in the size of the input.

",[u'Integer factorization algorithms'],"[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algebraic number', u'Algebraic number field', u'Algorithm', u'Algorithmic efficiency', u'Ancient Egyptian multiplication', u'Arjen Lenstra', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'Block Lanczos algorithm for nullspace of a matrix over a finite field', u'Block Wiedemann algorithm', u'Carl Pomerance', u'Centrum Wiskunde & Informatica', u'Chakravala method', u""Cipolla's algorithm"", u'Computational complexity theory', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cunningham project', u'Degree of a polynomial', u'Digital object identifier', u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Field norm', u'Function field sieve', u""F\xfcrer's algorithm"", u'GPL', u'Gaussian elimination', u'Generating primes', u'Greatest common divisor', u'Hendrik Lenstra', u'Heuristic', u'Homomorphism', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Internet', u'Irreducible polynomial', u'Jason Papadopoulos', u'Karatsuba algorithm', u'L-notation', u'Lattice sieving', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Multiplication algorithm', u'Natural logarithm', u'Number field', u'Number theory', u'Paul Leyland', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial', u'Primality test', u'Prime power', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Radix', u'Rational number', u'Rational sieve', u'Ring of integers', u'Root of a function', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Thorsten Kleinjung', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'United Kingdom', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Genetic algorithms,"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover.","[u'All articles needing additional references', u'All articles with dead external links', u'All articles with unsourced statements', u'Articles needing additional references from May 2011', u'Articles with dead external links from September 2011', u'Articles with unsourced statements from August 2007', u'Articles with unsourced statements from December 2011', u'CS1 errors: dates', u'CS1 errors: external links', u'Cybernetics', u'Digital organisms', u'Genetic algorithms', u'Mathematical optimization', u'Optimization algorithms and methods', u'Pages using citations with accessdate and no URL', u'Search algorithms', u'Use dmy dates from July 2013']","[u'Alex Fraser (scientist)', u'Algorithm', u'Ant colony optimization', u'Artificial evolution', u'Artificial intelligence', u'Artificial selection', u'Associative array', u'Average information', u'Bacteriologic algorithm', u'Bin packing problem', u'Bit array', u'CMA-ES', u'Candidate solution', u'Cellular automata', u'Chromosomal inversion', u'Chromosome', u'Cluster analysis', u'Computational fluid dynamics', u'Computer simulation', u'Cross-entropy method', u'Crossover (genetic algorithm)', u'Cultural algorithm', u'Data structure', u'David B. Fogel', u'Decision problem', u'Differential Search Algorithm', u'Digital object identifier', u'Edge recombination operator', u'Emanuel Falkenauer', u'Engineering', u'Ergodicity', u'Evolution strategy', u'Evolutionary Computation', u'Evolutionary algorithm', u'Evolutionary algorithms', u'Evolutionary computing', u'Evolutionary ecology', u'Evolutionary programming', u'Evolved antenna', u'Evolver (software)', u'Extremal optimization', u'Fitness (biology)', u'Fitness approximation', u'Fitness function', u'Fitness landscape', u'Floating point', u'Gaussian adaptation', u'Gene expression programming', u'Genetic drift', u'Genetic operator', u'Genetic programming', u'Genetic representation', u'Genotype', u'Global optimization', u'Global optimum', u'Gray coding', u'Grouping genetic algorithm', u'Hans-Joachim Bremermann', u'Hans-Paul Schwefel', u'Harmony search', u'Heredity', u'Heuristic (computer science)', u'Hill climbing', u""Holland's Schema Theorem"", u'Ingo Rechenberg', u'Institute for Advanced Study', u'Integer', u'Integer linear programming', u'Intelligent Water Drops', u'Interactive evolutionary algorithm', u'Interactive evolutionary computation', u'International Conference on Machine Learning', u'International Standard Book Number', u'Iteration', u'Jean-Marc J\xe9z\xe9quel', u'John Henry Holland', u'John Koza', u'John Markoff', u'Knapsack problem', u'Lawrence J. Fogel', u'Linked list', u'List (computing)', u'List of genetic algorithm applications', u'Local optima', u'Local optimum', u'Local search (optimization)', u'Markov chain', u'Mean fitness', u'Meme', u'Memetic algorithm', u'Metaheuristic', u'Metaheuristics', u'Mutation (genetic algorithm)', u'National Diet Library', u'Natural selection', u'Nils Aall Barricelli', u'No free lunch in search and optimization', u'Object (computer science)', u'Objective function', u'Optimization (mathematics)', u'Particle filter', u'Particle swarm optimization', u'Phenotype', u'Pittsburgh, Pennsylvania', u'Population', u'Princeton, New Jersey', u'Propagation of schema', u'Reactive search optimization', u'Schema (genetic algorithms)', u'Search algorithm', u'Selection (genetic algorithm)', u'Simulated annealing', u'Space Technology 5', u'Springer Science+Business Media', u'Steven Skiena', u'Stochastic optimization', u'Stochastics', u'Swarm intelligence', u'Tabu search', u'The New York Times', u'Timeline', u'Travelling salesman problem', u'Tree (data structure)', u'Universal Darwinism', u'University of Michigan']"
Gift wrapping algorithm,"In computational geometry, the gift wrapping algorithm is an algorithm for computing the convex hull of a given set of points.

","[u'Convex hull algorithms', u'Polytopes']","[u'Algorithm', u'Arithmetic precision', u'Big O notation', u""Chan's algorithm"", u'Charles E. Leiserson', u'Clifford Stein', u'Collinear', u'Computational geometry', u'Convex hull', u'Coordinates (elementary mathematics)', u'Degenerate case', u'Digital object identifier', u'Extreme point', u'General position', u'Graham scan', u'Information Processing Letters', u'International Standard Book Number', u'Introduction to Algorithms', u'Linear time', u'Output-sensitive algorithm', u'Polar coordinates', u'Ron Rivest', u'Thomas H. Cormen', u'Time complexity']"
Gilbert–Johnson–Keerthi distance algorithm,"The Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, but instead relies solely on a support function to iteratively generate closer simplices to the correct answer using the Minkowski sum (CSO) of two convex shapes.
""Enhanced GJK"" algorithms use edge information to speed up the algorithm by following edges when looking for the next simplex. This improves performance substantially for polytopes with large numbers of vertices.
GJK algorithms are often used incrementally in simulation systems and video games. In this mode, the final simplex from a previous solution is used as the initial guess in the next iteration, or ""frame"". If the positions in the new frame are close to those in the old frame, the algorithm will converge in one or two iterations. This yields collision detection systems which operate in near-constant time.
The algorithm's stability, speed, and small storage footprint make it popular for realtime collision detection, especially in physics engines for video games.","[u'All stub articles', u'Applied mathematics stubs', u'Convex geometry', u'Geometric algorithms']","[u'Algorithm', u'Applied mathematics', u'Collision detection', u'Convex set', u'Minkowski sum', u'Physics engine', u'Simplex', u'Support (mathematics)', u'Video games']"
Girvan–Newman algorithm,"The Girvan–Newman algorithm (named after Michelle Girvan and Mark Newman) is a hierarchical method used to detect communities in complex systems.

","[u'Graph algorithms', u'Network analysis', u'Networks']","[u'Betweenness centrality', u'Centrality', u'Closeness (mathematics)', u'Community structure', u'Complex system', u'Dendrogram', u'Hierarchical clustering', u'Mark Newman', u'Modularity (networks)']"
Global illumination,"Global illumination (shortened as GI) or indirect illumination is a general name for a group of algorithms used in 3D computer graphics that are meant to add more realistic lighting to 3D scenes. Such algorithms take into account not only the light which comes directly from a light source (direct illumination), but also subsequent cases in which light rays from the same source are reflected by other surfaces in the scene, whether reflective or not (indirect illumination).
Theoretically reflections, refractions, and shadows are all examples of global illumination, because when simulating them, one object affects the rendering of another object (as opposed to an object being affected only by a direct light). In practice, however, only the simulation of diffuse inter-reflection or caustics is called global illumination.
Images rendered using global illumination algorithms often appear more photorealistic than images rendered using only direct illumination algorithms. However, such images are computationally more expensive and consequently much slower to generate. One common approach is to compute the global illumination of a scene and store that information with the geometry, e.g., radiosity. That stored data can then be used to generate images from different viewpoints for generating walkthroughs of a scene without having to go through expensive lighting calculations repeatedly.
Radiosity, ray tracing, beam tracing, cone tracing, path tracing, Metropolis light transport, ambient occlusion, photon mapping, and image based lighting are examples of algorithms used in global illumination, some of which may be used together to yield results that are not fast, but accurate.
These algorithms model diffuse inter-reflection which is a very important part of global illumination; however most of these (excluding radiosity) also model specular reflection, which makes them more accurate algorithms to solve the lighting equation and provide a more realistically illuminated scene.
The algorithms used to calculate the distribution of light energy between surfaces of a scene are closely related to heat transfer simulations performed using finite-element methods in engineering design.
In real-time 3D graphics, the diffuse inter-reflection component of global illumination is sometimes approximated by an ""ambient"" term in the lighting equation, which is also called ""ambient lighting"" or ""ambient color"" in 3D software packages. Though this method of approximation (also known as a ""cheat"" because it's not really a global illumination method) is easy to perform computationally, when used alone it does not provide an adequately realistic effect. Ambient lighting is known to ""flatten"" shadows in 3D scenes, making the overall visual effect more bland. However, used properly, ambient lighting can be an efficient way to make up for a lack of processing power.","[u'All articles needing additional references', u'Articles needing additional references from May 2013', u'Global illumination algorithms']","[u'3D computer graphics', u'Algorithm', u'Ambient occlusion', u'Beam tracing', u'Caustic (optics)', u'Cone tracing', u'Diffuse inter-reflection', u'Distributed ray tracing', u'Finite element analysis', u'Heat transfer', u'High dynamic range imaging', u'Image-based lighting', u'Image based lighting', u'International Standard Book Number', u'Lightcuts', u'Metropolis light transport', u'Path tracing', u'Photon mapping', u'Point Based Global Illumination', u'Radiosity (computer graphics)', u'Ray tracing (graphics)', u'Real-time rendering', u'Rendering equation', u'Specular reflection', u'Spherical harmonic lighting']"
Gnome sort,"Gnome sort (or Stupid sort) is a sorting algorithm originally proposed by Dr. Hamid Sarbazi-Azad (Professor of Computer Engineering at Sharif University of Technology) in 2000 and called ""stupid sort"" (not to be confused with bogosort), and then later on described by Dick Grune and named ""gnome sort"" from the observation that it is ""how a gnome sorts a line of flower pots."" It is a sorting algorithm which is similar to insertion sort, except that moving an element to its proper place is accomplished by a series of swaps, as in bubble sort. It is conceptually simple, requiring no nested loops. The average, or expected, running time is O(n2), but tends towards O(n) if the list is initially almost sorted. In practice the algorithm can run as fast as insertion sort.
The algorithm always finds the first place where two adjacent elements are in the wrong order, and swaps them. It takes advantage of the fact that performing a swap can introduce a new out-of-order adjacent pair only next to the two swapped elements. It does not assume that elements forward of the current position are sorted, so it only needs to check the position directly previous to the swapped elements.","[u'All articles needing additional references', u'All articles with unsourced statements', u'Articles needing additional references from August 2010', u'Articles with unsourced statements from April 2012', u'Comparison sorts', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Array data type', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Dick Grune', u'Flashsort', u'Hamid Sarbazi-Azad', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Sharif University of Technology', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Teleportation', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Goertzel algorithm,"The Goertzel algorithm is a Digital Signal Processing (DSP) technique that provides a means for efficient evaluation of individual terms of the Discrete Fourier Transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad. The algorithm was first described by Gerald Goertzel in 1958.
Like the DFT, the Goertzel algorithm analyses one selectable frequency component from a discrete signal. Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, using real-valued arithmetic for real-valued input sequences. For covering a full spectrum, the Goertzel algorithm has a higher order of complexity than Fast Fourier Transform (FFT) algorithms; but for computing a small number of selected frequency components, it is more numerically efficient. The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications, though not limited to these.
The Goertzel algorithm can also be used ""in reverse"" as a sinusoid synthesis function, which requires only 1 multiplication and 1 subtraction per generated sample.","[u'All articles needing additional references', u'All articles needing cleanup', u'All articles with specifically marked weasel-worded phrases', u'All articles with unsourced statements', u'Articles needing additional references from February 2014', u'Articles needing cleanup from February 2014', u'Articles with specifically marked weasel-worded phrases from February 2014', u'Articles with unsourced statements from February 2014', u'Cleanup tagged articles with a reason field from February 2014', u'Digital signal processing', u'FFT algorithms', u'Pages using citations with accessdate and no URL', u'Pages using web citations with no URL', u'Wikipedia pages needing cleanup from February 2014']","[u'Aliasing', u'Array data type', u'Big O notation', u""Bluestein's FFT algorithm"", u'Computational complexity theory', u'Digital Signal Processing', u'Digital filter', u'Digital object identifier', u'Discrete Fourier Transform', u'Discrete Fourier transform', u'Discrete signal', u'Dual-tone multi-frequency signaling', u'Fast Fourier Transform', u'Fast Fourier transform', u'Finite impulse response', u'Frequency-shift keying', u'Gerald Goertzel', u'Infinite impulse response', u'International Standard Serial Number', u'Marginal stability', u'Numerical stability', u'Nyquist\u2013Shannon sampling theorem', u'Object-oriented programming', u'Phase-shift keying', u'Pole (complex analysis)', u'Pseudocode', u'Z transform']"
Golden section search,The golden section search is a technique for finding the extremum (minimum or maximum) of a strictly unimodal function by successively narrowing the range of values inside which the extremum is known to exist. The technique derives its name from the fact that the algorithm maintains the function values for triples of points whose distances form a golden ratio. The algorithm is the limit of Fibonacci search (also described below) for a large number of function evaluations. Fibonacci search and Golden section search were discovered by Kiefer (1953). (see also Avriel and Wilde (1966)).,"[u'Articles with example Java code', u'Fibonacci numbers', u'Golden ratio', u'Optimization algorithms and methods']","[u'Absolute value', u'Algorithm', u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u'Binary search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u""Brent's method"", u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Extremum', u'Fibonacci Quarterly', u'Fibonacci number', u'Fibonacci search', u'Fibonacci search technique', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Golden angle', u'Golden ratio', u'Golden ratio base', u'Golden rectangle', u'Golden rhombus', u'Golden spiral', u'Golden triangle (mathematics)', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'International Standard Book Number', u'Iterative method', u'JSTOR', u'Jack Kiefer (mathematician)', u'Jack Kiefer (statistician)', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u'Kepler triangle', u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical Reviews', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Metallic mean', u'Minimax', u'Minimum spanning tree', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization algorithm', u'Pell number', u'Penalty method', u""Powell's method"", u'Proceedings of the American Mathematical Society', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Revised simplex algorithm', u'Sequence', u'Sequential quadratic programming', u'Silver ratio', u'Simplex algorithm', u'Simulated annealing', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Ternary search', u'Truncated Newton method', u'Trust region', u'Unimodal function', u'Wolfe conditions']"
Goldschmidt division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output.","[u'All articles to be expanded', u'All articles with unsourced statements', u'All pages needing factual verification', u'Articles to be expanded from September 2012', u'Articles with example pseudocode', u'Articles with unsourced statements from February 2012', u'Articles with unsourced statements from February 2014', u'Binary arithmetic', u'Computer arithmetic', u'Computer arithmetic algorithms', u'Division (mathematics)', u'Wikipedia articles needing clarification from July 2015', u'Wikipedia articles needing factual verification from June 2015']","[u'AMD', u'Algorithm', u'Analysis of algorithms', u'Approximation', u'Barrett reduction', u'Binomial theorem', u'Chunking (division)', u'Cryptography', u'Digital object identifier', u'Division (mathematics)', u'Double precision', u'Equioscillation theorem', u""Euclid's Elements"", u'Euclidean division', u'Extended precision', u'Fixed point arithmetic', u'Floating point', u'Fused multiply\u2013add', u'Greatest common divisor', u'Hexadecimal', u'Integer (computer science)', u'International Standard Book Number', u'Karatsuba algorithm', u'Long division', u'Lookup table', u'Microprocessor', u'Modular arithmetic', u'Multiplication algorithm', u'Multiplicative inverse', u""Newton's method"", u'OCLC', u'Original Intel Pentium (P5 microarchitecture)', u'Output-sensitive algorithm', u'Pentium FDIV bug', u'Power of two', u'Precision (computer science)', u'Quotient', u'Radix', u'Rate of convergence', u'Relative error', u'Remainder', u'Remez algorithm', u'Round-off error', u'Sch\xf6nhage\u2013Strassen algorithm', u'Short division', u'Single precision', u'Toom\u2013Cook multiplication']"
Gradient descent,"Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.
Gradient descent is also known as steepest descent, or the method of steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.","[u'Articles with example Python code', u'First order methods', u'Gradient methods', u'Optimization algorithms and methods']","[u'Accuracy', u'Algorithm', u'Artificial neural network', u'Augustin Cauchy', u'BFGS method', u'Backpropagation', u'Big O notation', u'Bowl (vessel)', u'Concentric circles', u'Conjugate gradient', u'Conjugate gradient method', u'Constraint (mathematics)', u'Contour line', u'Convex function', u'Convex programming', u'Coursera', u'Curvature', u'Defined and undefined', u'Delta rule', u'Differentiable function', u'Digital object identifier', u'Eigenvalues', u'Euclidean norm', u""Euler's method"", u'First-order method', u'Forward\u2013backward algorithm (convex programming)', u'Function space', u'Geoffrey Hinton', u'Gradient', u'Gradient descent', u'Gradient flow', u'G\xe2teaux derivative', u'Hessian matrix', u'International Standard Serial Number', u'Isosurface', u'Jacobian matrix', u'Limited-memory BFGS', u'Line search', u'Linear least squares (mathematics)', u'Lipschitz continuity', u'Local maximum', u'Local minimum', u'Mathematical Reviews', u'Mathematical analysis', u'Mathematical optimization', u'Method of steepest descent', u'Nelder\u2013Mead method', u'Neural Networks (journal)', u""Newton's method in optimization"", u'Ordinary differential equations', u'Orthogonal', u'Preconditioner', u'Preconditioning', u'Projection (linear algebra)', u'Python (programming language)', u'Regina S. Burachik', u'Rosenbrock function', u'Rprop', u'Stochastic gradient descent', u'Subgradient method', u'Variational inequality', u'Willamette University', u'Wolfe conditions', u'YouTube', u'Yurii Nesterov']"
Graham scan,"Graham's scan is a method of finding the convex hull of a finite set of points in the plane with time complexity O(n log n). It is named after Ronald Graham, who published the original algorithm in 1972. The algorithm finds all vertices of the convex hull ordered along its boundary.","[u'Articles with example pseudocode', u'Convex hull algorithms']","[u'All nearest smaller values', u'Big O notation', u'Charles E. Leiserson', u'Clifford Stein', u'Convex hull', u'Cross product', u'Digital object identifier', u'Dot product', u'Heapsort', u'International Standard Book Number', u'Interval (mathematics)', u'Introduction to Algorithms', u'Robert Sedgewick (computer scientist)', u'Ron Rivest', u'Ronald Graham', u'Sorting algorithm', u'Springer Science+Business Media', u'Thomas H. Cormen', u'Time complexity', u'Uzi Vishkin', u'Vector (geometric)']"
Grover's algorithm,"Grover's algorithm is a quantum algorithm that finds with high probability the unique input to a black box function that produces a particular output value, using just O(N1/2) evaluations of the function, where N is the size of the function's domain.
The analogous problem in classical computation cannot be solved in fewer than O(N) evaluations (because, in the worst case, the correct input might be the last one that is tried). At roughly the same time that Grover published his algorithm, Bennett, Bernstein, Brassard, and Vazirani published a proof that no quantum solution to the problem can evaluate the function fewer than O(N1/2) times, so Grover's algorithm is asymptotically optimal.
Unlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, Grover's algorithm provides only a quadratic speedup. However, even quadratic speedup is considerable when N is large. Grover's algorithm could brute force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. As a result, it is sometimes suggested that symmetric key lengths be doubled to protect against future quantum attacks.
Like many quantum algorithms, Grover's algorithm is probabilistic in the sense that it gives the correct answer with a probability of less than 1. Though there is technically no upper bound on the number of repetitions that might be needed before the correct answer is obtained, the expected number of repetitions is a constant factor that does not grow with N.
Grover's original paper described the algorithm as a database search algorithm, and this description is still common. The database in this analogy is a table of all of the function's outputs, indexed by the corresponding input.","[u'Quantum algorithms', u'Search algorithms']","[u'Adiabatic quantum computation', u'Algorithmic cooling', u'Amplitude amplification', u'ArXiv', u'BQP', u'Bibcode', u'Black box', u'Brute-force attack', u'Cavity quantum electrodynamics', u'Charge qubit', u'Circuit quantum electrodynamics', u'Classical capacity', u'Cluster state', u'Collision problem', u'Deutsch\u2013Jozsa algorithm', u'Digital object identifier', u'Domain of a function', u'EQP (complexity)', u'Entanglement-assisted classical capacity', u'Entanglement-assisted stabilizer formalism', u'Entanglement distillation', u'Flux qubit', u'Jordan form', u'Kane quantum computer', u'LOCC', u'Linear optical quantum computing', u'Loss\u2013DiVincenzo quantum computer', u'Mathematical formulation of quantum mechanics', u'Mean', u'Measurement in quantum mechanics', u'Median', u'NP (complexity class)', u'Nitrogen-vacancy center', u'Nuclear magnetic resonance quantum computer', u'One-way quantum computer', u'Optical lattice', u'Phase qubit', u'PostBQP', u'Probability', u'QMA', u'Quantum Fourier transform', u'Quantum Turing machine', u'Quantum algorithm', u'Quantum annealing', u'Quantum capacity', u'Quantum channel', u'Quantum circuit', u'Quantum complexity theory', u'Quantum computer', u'Quantum convolutional code', u'Quantum cryptography', u'Quantum decoherence', u'Quantum energy teleportation', u'Quantum error correction', u'Quantum gate', u'Quantum information', u'Quantum information science', u'Quantum key distribution', u'Quantum network', u'Quantum optics', u'Quantum phase estimation algorithm', u'Quantum programming', u'Quantum teleportation', u'Qubit', u'SIAM Journal on Computing', u""Shor's algorithm"", u""Simon's problem"", u'Spin (physics)', u'Stabilizer code', u'Subroutine', u'Superconducting quantum computing', u'Superdense coding', u'Timeline of quantum computing', u'Topological quantum computer', u'Trapped ion quantum computer', u'Ultracold atom', u'Unitary operator', u'Universal quantum simulator', u'Vladimir Korepin']"
Halley's method,"In numerical analysis, Halley’s method is a root-finding algorithm used for functions of one real variable with a continuous second derivative, i.e., C2 functions. It is named after its inventor Edmond Halley.
The algorithm is second in the class of Householder's methods, right after Newton's method. Like the latter, it produces iteratively a sequence of approximations to the root; their rate of convergence to the root is cubic. Multidimensional versions of this method exist.
Halley's method can be viewed as exactly finding the roots of a linear-over-linear Padé approximation to the function, in contrast to Newton's method/Secant method (approximates/interpolates the function linearly) or Cauchy's method/Muller's method (approximates/interpolates the function quadratically).","[u'Root-finding algorithms', u'Use dmy dates from July 2013']","[u'Bond (finance)', u'Bond Exchange of South Africa', u""Cauchy's method"", u'Differentiability class', u'Digital object identifier', u'Edmond Halley', u'Eric W. Weisstein', u""Householder's method"", u'Interpolation', u'MathWorld', u""Muller's method"", u""Newton's method"", u'Numerical analysis', u'Pad\xe9 approximant', u'Rate of convergence', u'Root-finding algorithm', u'Secant method', u'Second derivative', u""Taylor's theorem"", u'Yield to maturity']"
Harmony search,"In computer science and operations research, harmony search (HS) is a phenomenon-mimicking algorithm (also known as metaheuristic algorithm, soft computing algorithm or evolutionary algorithm) inspired by the improvisation process of musicians proposed by Zong Woo Geem in 2001. In the HS algorithm, each musician (= decision variable) plays (= generates) a note (= a value) for finding a best harmony (= global optimum) all together. Proponents claim the following merits:
HS does not require differential gradients, thus it can consider discontinuous functions as well as continuous functions.
HS can handle discrete variables as well as continuous variables.
HS does not require initial value setting for the variables.
HS is free from divergence.
HS may escape local optima.
HS may overcome the drawback of GA's building block theory which works well only if the relationship among variables in a chromosome is carefully considered. If neighbor variables in a chromosome have weaker relationship than remote variables, building block theory may not work well because of crossover operation. However, HS explicitly considers the relationship using ensemble operation.
HS has a novel stochastic derivative applied to discrete variables, which uses musician's experiences as a searching direction.
Certain HS variants do not require algorithm parameters such as HMCR and PAR, thus novice users can easily use the algorithm.","[u'All NPOV disputes', u'All articles lacking in-text citations', u'All articles with unsourced statements', u'Articles lacking in-text citations from April 2013', u'Articles with unsourced statements from April 2013', u'Combinatorial optimization', u'Evolutionary algorithms', u'NPOV disputes from April 2013', u'Optimization algorithms and methods', u'Wikipedia articles with possible conflicts of interest from April 2013']","[u'Algorithm', u'Ant colony optimization', u'Antwerp University', u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Combinatorial optimization', u'Comparison of optimization software', u'Computer science', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cross-entropy method', u'Cutting-plane method', u'Dalle Molle Institute for Artificial Intelligence Research', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolution Strategies', u'Evolutionary algorithm', u'Evolutionary algorithms', u'Evolutionary computation', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Genetic algorithm', u'Genetic algorithms', u'Genetic programming', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'Intelligent Water Drops', u'International Journal of Applied Metaheuristic Computing', u'International Transactions in Operational Research', u'Iterative method', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Minimum spanning tree', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Operations research', u'Optimization (mathematics)', u'Optimization algorithm', u'Particle swarm optimization', u'Penalty method', u""Powell's method"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Revised simplex algorithm', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Soft computing', u'Stochastic optimization', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Swarm Intelligence', u'Symmetric rank-one', u'Tabu search', u'Truncated Newton method', u'Trust region', u'Wolfe conditions', u'Zong Woo Geem']"
Hash join,"The hash join is an example of a join algorithm and is used in the implementation of a relational database management system.
The task of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which have that value.
Hash joins require an equijoin predicate (a predicate comparing values from one table with values from the other table using the equals operator '=').

","[u'Hashing', u'Join algorithms']","[u'Block nested loop', u'Database management system', u'Digital object identifier', u'Equijoin', u'Hash function', u'Hash table', u'Jim Gray (computer scientist)', u'Join (SQL)', u'Relational database', u'Symmetric Hash Join', u'Syntactic predicate', u'Tuple']"
Heap's algorithm,"Heap's algorithm generates all possible permutations of N objects. It was first proposed by B. R. Heap in 1963. The algorithm minimizes movement: it generates each permutation from the previous one by interchanging a single pair of elements; the other N−2 elements are not disturbed. In a 1977 review of permutation-generating algorithms, Robert Sedgewick concluded that it was at that time the most effective algorithm for generating permutations by computer.","[u'Combinatorial algorithms', u'Permutations']","[u'Algorithm', u'Digital object identifier', u'Permutation', u'Robert Sedgewick (computer scientist)', u'Steinhaus\u2013Johnson\u2013Trotter algorithm']"
Heapsort,"In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.
Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort.
Heapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.","[u'All articles with unsourced statements', u'Articles with example pseudocode', u'Articles with unsourced statements from June 2012', u'Articles with unsourced statements from September 2014', u'Comparison sorts', u'Heaps (data structures)', u'Sorting algorithms', u'Use dmy dates from July 2012']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary heap', u'Binary tree', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Branch prediction', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Communications of the ACM', u'Comparison of programming languages (array)', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Data cache', u'Digital object identifier', u'Donald Knuth', u'Edsger W. Dijkstra', u'External sorting', u'Flashsort', u'Fundamenta Informaticae', u'Gnome sort', u'Heap (data structure)', u'Hybrid algorithm', u'In-place algorithm', u'Ingo Wegener', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'J. W. J. Williams', u'JSort', u'Kurt Mehlhorn', u'Library sort', u'Linear speedup', u'Linked list', u'List (computing)', u'Locality of reference', u'Loop invariant', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel algorithm', u'Patience sorting', u'Peter Sanders (computer scientist)', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Robert Floyd', u'Robert W. Floyd', u'Ronald L. Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Steven Skiena', u'Stooge sort', u'Strand sort', u'Svante Carlsson', u'Ternary heapsort', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Treesort']"
Hindley-Milner type inference,"In type theory and functional programming, Hindley–Milner (HM) (also known as Damas–Milner or Damas–Hindley–Milner) is a classical type system for the lambda calculus with parametric polymorphism, first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.
Among HM's more notable properties is completeness and its ability to deduce the most general type of a given program without the need of any type annotations or other hints supplied by the programmer. Algorithm W is a fast algorithm, performing type inference in almost linear time with respect to the size of the source, making it practically usable to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably by constrained types as used in Haskell.","[u'1969 in computer science', u'1978 in computer science', u'1985 in computer science', u'Algorithms', u'All accuracy disputes', u'Articles with disputed statements from October 2013', u'Formal methods', u'Lambda calculus', u'Theoretical computer science', u'Type inference', u'Type systems', u'Type theory']","[u'Alpha-renaming', u'Anonymous function', u'Assignment (mathematical logic)', u'Attribute grammar', u'Bounded types', u'CiteSeer', u'Completeness (logic)', u'Consistency', u'DEXPTIME', u'Deductive system', u'Digital object identifier', u'Disjoint-set data structure', u'Equivalence class', u'Fixed point combinator', u'Formal system', u'Free variables and bound variables', u'Functional language', u'Functional programming', u'Harry Mairson', u'Haskell (programming language)', u'Identity function', u'International Standard Book Number', u'J. Roger Hindley', u'JSTOR', u'John Alan Robinson', u'Judgment (mathematical logic)', u'Lambda calculus', u'Linear time', u'ML (programming language)', u'NP-hard', u'Occurs check', u'Parametric polymorphism', u'Partial order', u'Principal type', u'Procedure (computer science)', u'Prolog', u'Proof system', u'Robin Milner', u'Rule of inference', u'Side effect (computer science)', u'Syntax', u'Syntax-directed', u'Term (logic)', u'Type annotation', u'Type inference', u'Type rules', u'Type system', u'Type theory', u'Type variable', u'Undecidable problem', u'Unification (computer science)', u'Unification (computing)']"
Hirschberg's algorithm,"In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman–Wunsch algorithm. Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.","[u'Articles with example pseudocode', u'Bioinformatics algorithms', u'Dynamic programming', u'Sequence alignment algorithms']","[u'Aho\u2013Corasick algorithm', u'Algorithm', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Arg max', u'BLAST', u'Big O Notation', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Commentz-Walter algorithm', u'Communications of the ACM', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computational biology', u'Computer science', u'DNA', u'Damerau\u2013Levenshtein distance', u'Dan Hirschberg', u'Deterministic acyclic finite state automaton', u'Diff', u'Digital object identifier', u'Directed acyclic word graph', u'Divide and conquer algorithm', u'Dynamic programming', u'Edit distance', u'FASTA', u'Generalized suffix tree', u'Hamming distance', u'Heuristic (computer science)', u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common subsequence problem', u'Longest common substring', u'Mathematical Reviews', u'Needleman-Wunsch algorithm', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Parsing', u'Pattern matching', u'Protein', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Regular expression', u'Regular tree grammar', u'Rope (data structure)', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Wagner\u2013Fischer algorithm']"
Hopcroft–Karp algorithm,"In computer science, the Hopcroft–Karp algorithm is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching – a set of as many edges as possible with the property that no two edges share an endpoint. It runs in  time in the worst case, where  is set of edges in the graph, and  is set of vertices of the graph. In the case of dense graphs the time bound becomes , and for random graphs it runs in near-linear time.
The algorithm was found by John Hopcroft and Richard Karp (1973). As in previous methods for matching such as the Hungarian algorithm and the work of Edmonds (1965), the Hopcroft–Karp algorithm repeatedly increases the size of a partial matching by finding augmenting paths. However, instead of finding just a single augmenting path per iteration, the algorithm finds a maximal set of shortest augmenting paths. As a result, only  iterations are needed. The same principle has also been used to develop more complicated algorithms for non-bipartite matching with the same asymptotic running time as the Hopcroft–Karp algorithm.","[u'Graph algorithms', u'Matching']","[u'Algorithm', u'Algorithmica', u'Assignment problem', u'Augmenting path', u'Average case analysis', u'Best, worst and average case', u'Bipartite graph', u'Bipartite matching', u'Breadth-first search', u'Cardinality', u'Computer science', u'Dense graph', u'Depth first search', u'Digital object identifier', u""Dinic's algorithm"", u'Graph (data structure)', u'Hungarian algorithm', u'Jack Edmonds', u'James B. Orlin', u'John Hopcroft', u'Kurt Mehlhorn', u'Logarithm', u'Matching (graph theory)', u'Mathematical Reviews', u'Maximum flow problem', u'Push-relabel maximum flow algorithm', u'Rajeev Motwani', u'Random graph', u'Ravindra K. Ahuja', u'Richard Karp', u'Robert Tarjan', u'Silvio Micali', u'Sparse graph', u'Symmetric difference', u'Symposium on Foundations of Computer Science', u'Thomas L. Magnanti', u'Vijay Vazirani', u'Worst case analysis']"
Huang's algorithm,Huang's algorithm is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Shing-Tsaan Huang in 1989 in the Journal of Computers.,"[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Termination algorithms']","[u'Algorithm', u'Dijkstra-Scholten algorithm', u'Distributed system', u'Journal of Computers', u'Shing-Tsaan Huang', u'Termination analysis']"
Huffman coding,"In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Ph.D. student at MIT, and published in the 1952 paper ""A Method for the Construction of Minimum-Redundancy Codes"".
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in linear time to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.","[u'1952 in computer science', u'All articles lacking in-text citations', u'Articles lacking in-text citations from January 2011', u'Binary trees', u'Commons category with local link same as on Wikidata', u'Lossless compression algorithms', u'Wikipedia articles needing clarification from February 2012']","[u'A-law algorithm', u'ASCII', u'A Mathematical Theory of Communication', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Alan Tucker', u'Algebraic code-excited linear prediction', u'Arithmetic coding', u'Array data type', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bernoulli process', u'Big O notation', u'Binary search tree', u'Binary tree', u'Bit rate', u'Block code', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Charles E. Leiserson', u'Chroma subsampling', u'Claude Shannon', u'Clifford Stein', u'Code-excited linear prediction', u'Codec', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Computer science', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'DEFLATE (algorithm)', u'Data compression', u'David A. Huffman', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Doctor of Philosophy', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exam', u'Expected value', u'Exponential-Golomb coding', u'Fax machines', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Frequency table', u'Golomb coding', u'Greedy algorithm', u'Group 4 compression', u'Huffyuv', u'Image compression', u'Image resolution', u'Independent and identically distributed', u'Information entropy', u'Information theory', u'Interlaced video', u'Internal node', u'Introduction to Algorithms', u'JPEG', u'JSTOR', u'Jan van Leeuwen', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Leaf node', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear-time', u'Linear predictive coding', u'Linear time', u'Linearithmic', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'MIT', u'MP3', u'Macroblock', u'Massachusetts Institute of Technology', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Monoid', u'Morse code', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'PKZIP', u'Package-merge algorithm', u'Patent', u'Peak signal-to-noise ratio', u'Pixel', u'Polynomial time', u'Power of two', u'Prediction by partial matching', u'Prefix code', u'Priority queue', u'Probability mass function', u'Proceedings of the IRE', u'Proportionality (mathematics)', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Queue (data structure)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Robert M. Fano', u'Ronald L. Rivest', u'Run-length encoding', u'Sampling (signal processing)', u'Scientific American', u'Set partitioning in hierarchical trees', u""Shannon's source coding theorem"", u'Shannon-Fano coding', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'T. C. Hu', u'Term paper', u'Thomas H. Cormen', u'Timeline of information theory', u'Total order', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable-length code', u'Variable bitrate', u'Varicode', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'Weighted path length from the root', u'\u039c-law algorithm']"
ID3 algorithm,"In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.

","[u'Articles with example pseudocode', u'Classification algorithms', u'Decision trees']","[u'Algorithm', u'Backtracking', u'C4.5 algorithm', u'Classification and regression tree', u'Decision tree learning', u'Entropy (information theory)', u'Information gain in decision trees', u'Machine learning', u'Natural language processing', u'Overfitting', u'Ross Quinlan']"
Incremental encoding,"Incremental encoding, also known as front compression, back compression, or front coding, is a type of delta encoding compression algorithm whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated. This algorithm is particularly well-suited for compressing sorted data, e.g., a list of words from a dictionary.
For example:
The encoding used to store the common prefix length itself varies from application to application. Typical techniques are storing the value as a single byte; delta encoding, which stores only the change in the common prefix length; and various universal codes. It may be combined with other general lossless data compression techniques such as entropy encoding and dictionary coders to compress the remaining suffixes.","[u'All stub articles', u'Database index techniques', u'Lossless compression algorithms', u'Storage software stubs']","[u'Affix', u'Bigram', u'Compression algorithm', u'Computer storage', u'Data', u'Delta encoding', u'Dictionary', u'Dictionary coder', u'Entropy encoding', u'GNU locate', u'Index (search engine)', u'Lossless data compression', u'Prefix (linguistics)', u'Software', u'Sorting', u'Universal code (data compression)', u'Word']"
Incremental heuristic search,"Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has been studied at least since the late 1960s. Heuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.
So far, three main classes of incremental heuristic search algorithms have been developed:
The first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*).
The second class updates the h-values from the previous search during the current search to make them more informed (example: Generalized Adaptive A*).
The third class updates the g-values from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite).
All three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.","[u'Artificial intelligence', u'Robot control', u'Search algorithms']","[u'A*', u'A* search algorithm', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'D*', u'Depth-first search', u'Depth-limited search', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph traversal', u'Hill climbing', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Narsingh Deo', u""Prim's algorithm"", u'Robotics', u'SMA*', u'Search game', u'Topology', u'Tree traversal']"
Insertion sort,"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:
Simple implementation: Bentley shows a three-line C version, and a five-line optimized version
Efficient for (quite) small data sets
More efficient in practice than most other simple quadratic (i.e., O(n2)) algorithms such as selection sort or bubble sort are usually faster in practice than asymptotically faster algorithms for small data sets
Adaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(nk) when each element in the input is no more than k places away from its sorted position
Stable; i.e., does not change the relative order of elements with equal keys
In-place; i.e., only requires a constant amount O(1) of additional memory space
Online; i.e., can sort a list as it receives it
When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.","[u'All articles with unsourced statements', u'Articles with example pseudocode', u'Articles with unsourced statements from September 2011', u'Articles with unsourced statements from September 2014', u'Commons category with local link same as on Wikidata', u'Comparison sorts', u'Online sorts', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary search algorithm', u'Binary tree', u'Binary tree sort', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'C (programming language)', u'Canada', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Data structure', u'Digital object identifier', u'Divide-and-conquer algorithm', u'Donald Knuth', u'Donald Shell', u'EEPROM', u'Farach-Colton', u'Flash memory', u'Flashsort', u'Gnome sort', u'Heap (data structure)', u'Heap sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Integer sorting', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'Iteration', u'JSort', u'Jon Bentley', u'Library sort', u'Linked list', u'List (computing)', u'Martin Farach-Colton', u'Mathematical Reviews', u'Merge sort', u'Mergesort', u'Odd\u2013even sort', u'Online algorithm', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Robert Sedgewick (computer scientist)', u'Ron Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Skip list', u'Smoothsort', u'Sorted array', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Time complexity', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'United Kingdom', u'Zero-based numbering']"
Inside-outside algorithm,"In computer science, the inside–outside algorithm is a way of re-estimating production probabilities in a probabilistic context-free grammar. It was introduced James K. Baker in 1979 as a generalization of the forward–backward algorithm for parameter estimation on hidden Markov models to stochastic context-free grammars. It is used to compute expectations, for example as part of the expectation–maximization algorithm (an unsupervised learning algorithm).

","[u'Algorithms and data structures stubs', u'All Wikipedia articles needing context', u'All pages needing cleanup', u'All stub articles', u'Computer science stubs', u'Parsing algorithms', u'Wikipedia articles needing context from June 2012', u'Wikipedia introduction cleanup from June 2012']","[u'Algorithm', u'Computer science', u'Data structure', u'Expectation\u2013maximization algorithm', u'Forward\u2013backward algorithm', u'Hidden Markov model', u'International Standard Book Number', u'James K. Baker', u'Karim Lari', u'Probabilistic context-free grammar', u'Steve J. Young', u'Stochastic context-free grammar']"
Integer factorization,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.","[u'Computational hardness assumptions', u'Integer factorization algorithms', u'Unsolved problems in computer science']","[u'AKS primality test', u'Abundant number', u'Achilles number', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Advanced Micro Devices', u'Algebraic-group factorisation algorithms', u'Algebraic number theory', u'Algorithm', u'Aliquot sequence', u'Almost perfect number', u'Amicable numbers', u'Ancient Egyptian multiplication', u'Arithmetic number', u'BQP', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Betrothed numbers', u'Big O notation', u'Binary GCD algorithm', u'Binary search', u'Bit', u'Canonical representation of a positive integer', u'Carl Pomerance', u'Chakravala method', u""Cipolla's algorithm"", u'Co-NP', u'Co-NP-complete', u'Colossally abundant number', u'Complexity class', u'Composite number', u'Computational complexity theory', u'Computer science', u'Congruence of squares', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cryptography', u'David Bressoud', u'Decision problem', u'Deficient number', u'Digital object identifier', u'Discrete logarithm', u'Discriminant of a quadratic form', u'Divisor', u'Divisor function', u""Dixon's algorithm"", u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve', u'Elliptic curve method', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Empty product', u'Equidigital number', u'Erd\u0151s\u2013Nicolas number', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u'Extravagant number', u'FNP (complexity)', u'FP (complexity)', u'Factorization', u'Fast Fourier transform', u""Fermat's factorization method"", u'Fermat primality test', u'Friendly number', u'Frugal number', u'Function field sieve', u'Function problem', u'Fundamental theorem of arithmetic', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generalized Riemann hypothesis', u'Generating primes', u'Generating set of a group', u'Greatest common divisor', u'Group (mathematics)', u'Harmonic divisor number', u'Hemiperfect number', u'Highly abundant number', u'Highly composite number', u'Hyperperfect number', u'Ideal class group', u'Index calculus algorithm', u'Integer factorization records', u'Integer square root', u'International Association for Cryptologic Research', u'International Standard Book Number', u'Karatsuba algorithm', u'Kronecker symbol', u'L-notation', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'List of unsolved problems in computer science', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Manindra Agrawal', u'Mathematical Reviews', u'Mathematics', u'Maurice Kraitchik', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Multiplicative partition', u'Multiply perfect number', u'NP-complete', u'NP-intermediate', u'NP (complexity)', u'Nature (journal)', u'Number theory', u'Opteron', u'P (complexity)', u'Partition (number theory)', u'Perfect number', u'Perfect power', u'Peter Shor', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial time', u'Powerful number', u'Practical number', u'Primality test', u'Prime decomposition', u'Prime decomposition (3-manifold)', u'Prime factor', u'Prime number', u'Primitive abundant number', u'Probabilistic algorithm', u'Pronic number', u""Proth's theorem"", u'Public-key', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic form', u'Quadratic residue', u'Quadratic sieve', u'Quantum computer', u'Quasiperfect number', u'RSA-768', u'RSA (algorithm)', u'RSA number', u'RSA problem', u'Randomized algorithm', u'Rational sieve', u'Regular number', u'Richard Crandall', u'Rough number', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Semiperfect number', u'Semiprime', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Sociable number', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Sphenic number', u'Square-free integer', u'Stan Wagon', u'Sublime number', u'Superabundant number', u'Superior highly composite number', u'Superperfect number', u'Sylow theorems', u'The Art of Computer Programming', u'Time complexity', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'UP (complexity)', u'Unitary divisor', u'Unitary perfect number', u'Untouchable number', u'Unusual number', u'Weird number', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'YouTube']"
Interior point method,"Interior point methods (also referred to as barrier methods) are a certain class of algorithms that solves linear and nonlinear convex optimization problems.

John von Neumann suggested an interior point method of linear programming which was neither a polynomial time method nor an efficient method in practice. In fact, it turned out to be slower in practice compared to simplex method which is not a polynomial time method. In 1984, Narendra Karmarkar developed a method for linear programming called Karmarkar's algorithm which runs in provably polynomial time and is also very efficient in practice. It enabled solutions of linear programming problems which were beyond the capabilities of simplex method. Contrary to the simplex method, it reaches a best solution by traversing the interior of the feasible region. The method can be generalized to convex programming based on a self-concordant barrier function used to encode the convex set.
Any convex optimization problem can be transformed into minimizing (or maximizing) a linear function over a convex set by converting to the epigraph form. The idea of encoding the feasible set using a barrier and designing barrier methods was studied by Anthony V. Fiacco, Garth P. McCormick, and others in the early 1960s. These ideas were mainly developed for general nonlinear programming, but they were later abandoned due to the presence of more competitive methods for this class of problems (e.g. sequential quadratic programming).
Yurii Nesterov and Arkadi Nemirovski came up with a special class of such barriers that can be used to encode any convex set. They guarantee that the number of iterations of the algorithm is bounded by a polynomial in the dimension and accuracy of the solution.
Karmarkar's breakthrough revitalized the study of interior point methods and barrier problems, showing that it was possible to create an algorithm for linear programming characterized by polynomial complexity and, moreover, that was competitive with the simplex method. Already Khachiyan's ellipsoid method was a polynomial time algorithm; however, it was too slow to be of practical interest.
The class of primal-dual path-following interior point methods is considered the most successful. Mehrotra's predictor-corrector algorithm provides the basis for most implementations of this class of methods.","[u'CS1 errors: chapter ignored', u'Optimization algorithms and methods', u'Use dmy dates from February 2011']","[u'Algorithm', u'Approximation algorithm', u'Arkadi Nemirovski', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Cambridge University Press', u'Candidate solution', u'Claude Lemar\xe9chal', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Convex set', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Diagonal matrix', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Epigraph', u'Evolutionary algorithm', u'Exchange algorithm', u'Feasible region', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'International Standard Book Number', u'Iteration', u'Iterative method', u'Jacobian matrix and determinant', u'John von Neumann', u""Johnson's algorithm"", u'KKT conditions', u""Karmarkar's algorithm"", u'Karush\u2013Kuhn\u2013Tucker conditions', u""Kruskal's algorithm"", u'Lagrange multiplier', u""Lemke's algorithm"", u'Leonid Khachiyan', u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear function', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical Reviews', u'Mathematical optimization', u'Matroid', u'Mehrotra predictor-corrector method', u'Metaheuristic', u'Minimum spanning tree', u'Narendra Karmarkar', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Newton method', u'Nonlinear conjugate gradient method', u'Nonlinear optimization', u'Nonlinear programming', u'Optimization algorithm', u'Penalty method', u'Polynomial time', u""Powell's method"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Revised simplex algorithm', u'Self-concordant', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Truncated Newton method', u'Trust region', u'Wolfe conditions', u'Yurii Nesterov']"
Interpolation search,"Interpolation search (sometimes referred to as extrapolation search) is an algorithm for searching for a given key value in an indexed array that has been ordered by the values of the key. It parallels how humans search through a telephone book for a particular name, the key value by which the book's entries are ordered. In each search step it calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.
By comparison, the binary search always chooses the middle of the remaining search space, discarding one half or the other, again depending on the comparison between the key value found at the estimated position and the key value sought. The remaining search space is reduced to the part before or after the estimated position. The linear search uses equality only as it compares elements one-by-one from the start, ignoring any sorting.
On average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.
In interpolation-sequential search, interpolation is used to find an item near the one being searched for, then linear search is used to find the exact item.",[u'Search algorithms'],"[u'Algorithm', u'B-tree', u'Big-O notation', u'Big O notation', u'Binary search', u'Binary search algorithm', u'C++', u'Collation', u'Exponential search', u'Hash table', u'Linear search', u'Mathematical optimization', u'Online algorithm', u'Search algorithm', u'Ternary search', u'Three-way comparison']"
Intersection algorithm,"The intersection algorithm is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources, it forms part of the modern Network Time Protocol. It is a modified form of Marzullo's algorithm.
While Marzullo's algorithm will return the smallest interval consistent with the largest number of sources, the returned interval does not necessarily include the center point (calculated offset) of all the sources in the intersection. The Intersection algorithm returns an interval that includes that returned by Marzullo's algorithm but may be larger since it will include the center points. This larger interval allows using additional statistical data to select a point within the interval, reducing the jitter in repeated execution.",[u'Agreement algorithms'],"[u'Agreement algorithm', u'Confidence band', u'Jitter', u""Marzullo's algorithm"", u'Network Time Protocol', u'Noise']"
Johnson algorithm,"Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in a sparse, edge weighted, directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.
A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.",[u'Graph algorithms'],"[u'A* search algorithm', u'All-pairs shortest path problem', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Charles E. Leiserson', u'Clifford Stein', u'Cycle (graph theory)', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Directed graph', u'Donald B. Johnson', u'Dynamic programming', u'Edge (graph theory)', u""Edmonds' algorithm"", u'Fibonacci heap', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph (data structure)', u'Graph traversal', u'Hill climbing', u'International Standard Book Number', u'Introduction to Algorithms', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Job Shop Scheduling', u'Journal of the ACM', u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'National Institute of Standards and Technology', u'Negative number', u""Prim's algorithm"", u'Ronald L. Rivest', u'SMA*', u'Search game', u'Shortest path', u'Sparse graph', u""Suurballe's algorithm"", u'Thomas H. Cormen', u'Time complexity', u'Tree traversal', u'Vertex (graph theory)', u'Weighted graph']"
Jump-and-Walk algorithm,"Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.
Jump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).
Jump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.

","[u'Algorithms', u'Triangulation (geometry)']","[u'Algorithm', u'John R. Rice (professor)', u'Point location', u'Triangulation']"
Jump point search,"In computer science, jump point search is an optimization to the A* search algorithm pathfinding algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long ""jumps"" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.
Jump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude.","[u'Algorithms and data structures stubs', u'All stub articles', u'Computer science stubs', u'Game artificial intelligence', u'Graph algorithms', u'Search algorithms']","[u'A* search algorithm', u'Algorithm', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Computer science', u'D*', u'Data structure', u'Depth-first search', u'Depth-limited search', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph traversal', u'Hill climbing', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'List of algorithms', u'Pathfinding', u""Prim's algorithm"", u'SMA*', u'Search game', u'Tree traversal']"
Jump search,"In computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items Lkm, where  and m is the block size, until an item is found that is larger than the search key. To find the exact position of the search key in the list a linear search is performed on the sublist L[(k-1)m, km].
The optimal value of m is √n, where n is the length of the list L. Because both steps of the algorithm look at, at most, √n items the algorithm runs in O(√n) time. This is better than a linear search, but worse than a binary search. The advantage over the latter is that a jump search only needs to jump backwards once, while a binary can jump backwards up to log n times. This can be important if a jumping backwards takes significantly more time than jumping forward.
The algorithm can be modified by performing multiple levels of jump search on the sublists, before finally performing the linear search. For an k-level jump search the optimum block size ml for the lth level (counting from 1) is n(k-l)/k. The modified algorithm will perform k backward jumps and runs in O(kn1/(k+1)) time.",[u'Search algorithms'],"[u'Algorithm', u'Ben Shneiderman', u'Binary search', u'Computer science', u'Dictionary of Algorithms and Data Structures', u'Interpolation search', u'Jump list', u'Linear search', u'List (computing)', u'National Institute of Standards and Technology', u'Search algorithm', u'Search key', u'Sublist']"
K-means++,"In data mining, k-means++ is an algorithm for choosing the initial values (or ""seeds"") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)","[u'All articles with dead external links', u'Articles with dead external links from May 2013', u'Data clustering algorithms', u'Statistical algorithms']","[u'Data mining', u'Digital object identifier', u'ELKI', u'GNU R', u'GraphLab', u'K-means clustering', u'Leonard Schulman', u""Lloyd's algorithm"", u'NP-hard', u'OpenCV', u'Scikit-learn', u'Vanilla (computing)', u'Weka (machine learning)']"
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.
The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.
The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.","[u'All articles with dead external links', u'All articles with unsourced statements', u'Articles with dead external links from January 2013', u'Articles with unsourced statements from March 2014', u'CS1 French-language sources (fr)', u'Data clustering algorithms', u'Statistical algorithms']","[u'Anomaly detection', u'Apache Mahout', u'Apache Spark', u'ArXiv', u'Artificial neural network', u'Association for Computational Linguistics', u'Association rule learning', u'Astronomy', u'Autoencoder', u'BIRCH', u'Bayesian network', u'Bell Labs', u'Bias-variance dilemma', u'Bilateral filter', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Canonical correlation analysis', u'Centroid', u'Centroidal Voronoi tessellation', u'Centroids', u'CiteSeer', u'Cluster analysis', u'Color quantization', u'Computational learning theory', u'Computer vision', u'Conditional random field', u'Convolutional neural network', u'CrimeStat', u'DBSCAN', u'Data Mining in Agriculture', u'Data mining', u'David MacKay (scientist)', u'David Mount', u'Decision tree learning', u'Deep learning', u'Determining the number of clusters in a data set', u'Dictionary learning', u'Digital object identifier', u'Dimensionality reduction', u'Discrete and Computational Geometry', u'ELKI', u'EM clustering', u'Empirical risk minimization', u'Ensemble learning', u'Environment for DeveLoping KDD-Applications Supported by Index-Structures', u'Euclidean distance', u'Expectation-maximization algorithm', u'Expectation\u2013maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Fuzzy clustering', u'GNU Octave', u'Gaussian distribution', u'Geostatistics', u'Grammar induction', u'Graphical model', u'Head/tail Breaks', u'Heuristic algorithm', u'Hidden Markov model', u'Hierarchical clustering', u'Hugo Steinhaus', u'IChrome Ltd.', u'IEEE Transactions on Information Theory', u'Independent component analysis', u'Integer lattice', u'International Standard Book Number', u'Iris (plant)', u'Iris flower data set', u'JSTOR', u'Jenks natural breaks optimization', u'Journal of the Royal Statistical Society, Series C', u'Julia language', u'K-SVD', u'K-means++', u'K-means clustering', u'K-medians clustering', u'K-medoids', u'K-nearest neighbor', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'K q-flats', u'Kd-tree', u'Learning to rank', u'Least-squares estimation', u'Lecture Notes in Computer Science', u'Linde\u2013Buzo\u2013Gray algorithm', u'Linear classifier', u'Linear discriminant analysis', u'Linear regression', u""Lloyd's algorithm"", u'Local optimum', u'Local outlier factor', u'Logistic regression', u'MATLAB', u'MLPACK (C++ library)', u'Machine Learning (journal)', u'Machine learning', u'MapReduce', u'Market segmentation', u'Mathematica', u'Mathematical Reviews', u'Mean', u'Mean-shift', u'Mean shift', u'Medoids', u'Metric (mathematics)', u'Mixture model', u'Multilayer perceptron', u'NP-hard', u'Naive Bayes classifier', u'Named entity recognition', u'Nathan Netanyahu', u'Natural language processing', u'Nearest centroid classifier', u'Non-negative matrix factorization', u'OPTICS algorithm', u'Online machine learning', u'OpenCV', u'Partition of a set', u'Perceptron', u'Principal component analysis', u'Probably approximately correct learning', u'Proceedings of the Royal Society A', u'Prototype', u'Pulse-code modulation', u'R (programming language)', u'Radial basis function', u'Radial basis function network', u'Random forest', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'SAS System', u'Sampling (statistics)', u'SciPy', u'Scikit-learn', u'Self-organizing map', u'Semi-supervised learning', u'Silhouette (clustering)', u'Smoothed analysis', u'Spherical k-means', u'Stata', u'Statistical classification', u'Statistical learning theory', u'Structured prediction', u'Supervised learning', u'Support vector machine', u'Symposium on Computational Geometry', u'T-distributed stochastic neighbor embedding', u'Taxicab geometry', u'Torch (machine learning)', u'Triangle inequality', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Variance', u'Vector quantization', u'Voronoi cell', u'Voronoi diagram', u'Weka (machine learning)', u'Whitening transformation', u'X-means clustering', u'Zentralblatt MATH']"
K-medoids,"The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary matrix of distances between datapoints instead of . This method was proposed in 1987 for the work with  norm and other distances.
k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.
It is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.
A medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster.","[u'All articles needing style editing', u'All articles with unsourced statements', u'Articles with unsourced statements from May 2015', u'Data clustering algorithms', u'Statistical algorithms', u'Wikipedia articles needing style editing from September 2015']","[u'Algorithm', u'Data clustering', u'ELKI', u'Julia language', u'K-means', u""Lloyd's algorithm"", u'MATLAB', u'Manhattan distance', u'Medoid', u'Medoids', u'Minkowski distance', u'R (programming language)', u'RapidMiner', u'Silhouette (clustering)']"
K-nearest neighbors,"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.

In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.

k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.
Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.
The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A shortcoming of the k-NN algorithm is that it is sensitive to the local structure of the data. The algorithm has nothing to do with and is not to be confused with k-means, another popular machine learning technique.","[u'All articles with unsourced statements', u'Articles with unsourced statements from December 2008', u'Articles with unsourced statements from March 2013', u'Classification algorithms', u'Machine learning algorithms', u'Search algorithms']","[u'Analytica Chimica Acta', u'Annals of Statistics', u'Anomaly detection', u'Artificial neural network', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Bayes error rate', u'Bayesian network', u'Belur V. Dasarathy', u'Bias-variance dilemma', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Canonical correlation', u'Canonical correlation analysis', u'Closest pair of points problem', u'Cluster analysis', u'Computational learning theory', u'Computer vision', u'Conditional random field', u'Confusion matrix', u'Consistency (statistics)', u'Continuous variable', u'Convolutional neural network', u'Curse of Dimensionality', u'DBSCAN', u'Data mining', u'Decision boundary', u'Decision tree learning', u'Deep learning', u'Digital object identifier', u'Dimension reduction', u'Dimensionality reduction', u'Embedding', u'Empirical risk minimization', u'Ensemble learning', u'Euclidean distance', u'Evolutionary algorithm', u'Expectation-maximization algorithm', u'Facial recognition system', u'Factor analysis', u'Feature (machine learning)', u'Feature engineering', u'Feature extraction', u'Feature learning', u'Feature scaling', u'Feature selection', u'Feature space', u'Feature vector', u'Forest Ecology and Management', u'GPU', u'Grammar induction', u'Graphical model', u'Haar wavelet', u'Hamming distance', u'Heuristic (computer science)', u'Hidden Markov model', u'Hierarchical clustering', u'Hyperparameter optimization', u'Independent component analysis', u'Instance-based learning', u'Integer', u'International Standard Book Number', u'K-means', u'K-means clustering', u'K-nearest neighbors classification', u'Kernel (statistics)', u'Large Margin Nearest Neighbor', u'Large margin nearest neighbor', u'Lazy learning', u'Learning to rank', u'Likelihood-ratio test', u'Linear discriminant analysis', u'Linear regression', u'Local outlier factor', u'Locality Sensitive Hashing', u'Logistic regression', u'MIT Press', u'Machine learning', u'Mahalanobis distance', u'Mean-shift', u'Metric (mathematics)', u'Multilayer perceptron', u'Mutual information', u'Naive Bayes classifier', u'Nearest centroid classifier', u'Nearest neighbor search', u'Neighbourhood components analysis', u'Non-negative matrix factorization', u'Non-parametric statistics', u'OPTICS algorithm', u'Online machine learning', u'OpenCV', u'Pattern recognition', u'Perceptron', u'Peter E. Hart', u'Principal Component Analysis', u'Principal component analysis', u'Probably approximately correct learning', u'Pseudo-metric', u'PubMed Identifier', u'RMSE', u'Random forest', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Self-organizing map', u'Semi-supervised learning', u'Statistical classification', u'Statistical learning theory', u'Structured prediction', u'Supervised learning', u'Support vector machine', u'T-distributed stochastic neighbor embedding', u'Thomas M. Cover', u'Time series', u'Unsupervised learning', u'VLDB', u'Vapnik\u2013Chervonenkis theory', u'Variable kernel density estimation']"
Kabsch algorithm,"The Kabsch algorithm, named after Wolfgang Kabsch, is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points. It is useful in graphics, cheminformatics to compare molecular structures, and also bioinformatics for comparing protein structures (in particular, see root-mean-square deviation (bioinformatics)).
The algorithm only computes the rotation matrix, but it also requires the computation of a translation vector. When both the translation and rotation are actually performed, the algorithm is sometimes called partial Procrustes superimposition (see also orthogonal Procrustes problem).

",[u'Bioinformatics algorithms'],"[u'Bioinformatics', u'Centroid', u'Cheminformatics', u'Coordinate system', u'Covariance matrix', u'Digital object identifier', u'Eigen (C++ library)', u'Matlab', u'Matrix (mathematics)', u'Orthogonal Procrustes problem', u'Procrustes superimposition', u'Protein', u'PubMed Identifier', u'PyMol', u'Python (programming language)', u'Quaternion', u'RMSD', u'Root-mean-square deviation (bioinformatics)', u'Root mean square', u'Rotation matrix', u'Singular value decomposition', u'Visual Molecular Dynamics', u""Wahba's problem"", u'Wolfgang Kabsch']"
Kadane's algorithm,"In computer science, the maximum subarray problem is the task of finding the contiguous subarray within a one-dimensional array of numbers (containing at least one positive number) which has the largest sum. For example, for the sequence of values −2, 1, −3, 4, −1, 2, 1, −5, 4; the contiguous subarray with the largest sum is 4, −1, 2, 1, with sum 6.
The problem was first posed by Ulf Grenander of Brown University in 1977, as a simplified model for maximum likelihood estimation of patterns in digitized images. A linear time algorithm was found soon afterwards by Jay Kadane of Carnegie-Mellon University (Bentley 1984).","[u'Articles with example Python code', u'Dynamic programming', u'Optimization algorithms and methods']","[u'Algorithm', u'Array data structure', u'Brown University', u'Carnegie-Mellon University', u'Communications of the ACM', u'Computer science', u'Digital object identifier', u'Dynamic programming', u'Empty sum', u'Jay Kadane', u'Jon Bentley', u'Linear time', u'Maximum likelihood', u'Python (programming language)', u'Subset sum problem', u'Ulf Grenander']"
Karatsuba algorithm,"The Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatolii Alexeevitch Karatsuba in 1960 and published in 1962. It reduces the multiplication of two n-digit numbers to at most  single-digit multiplications in general (and exactly  when n is a power of 2). It is therefore faster than the classical algorithm, which requires n2 single-digit products. For example, the Karatsuba algorithm requires 310 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (n = 1024 = 210), whereas the classical algorithm requires (210)2 = 1,048,576.
The Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic ""grade school"" algorithm. The Toom–Cook algorithm is a faster generalization of Karatsuba's method, and the Schönhage–Strassen algorithm is even faster, for sufficiently large n.","[u'Computer arithmetic algorithms', u'Multiplication', u'Pages with syntax highlighting errors']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Anatolii Alexeevitch Karatsuba', u'Ancient Egyptian multiplication', u'Andrey Kolmogorov', u'Asymptotically optimal', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big-O notation', u'Big O notation', u'Binary GCD algorithm', u'Carry-save adder', u'Chakravala method', u'Charles Babbage', u""Cipolla's algorithm"", u'Computational complexity theory', u'Computer platform', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cybernetics', u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Eric W. Weisstein', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Master theorem', u'MathWorld', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Moscow State University', u'Multiplication ALU', u'Multiplication algorithm', u'Number theory', u'Physics-Doklady', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u'Proceedings of the USSR Academy of Sciences', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Radix', u'Rational sieve', u'Recurrence relation', u'Recursion', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'The Art of Computer Programming', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'Yuri Petrovich Ofman']"
Karger's algorithm,"In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph. It was invented by David Karger and first published in 1993.
The idea of the algorithm is based on the concept of contraction of an edge  in an undirected graph . Informally speaking, the contraction of an edge merges the nodes  and  into one, reducing the total number of nodes of the graph by one. All other edges connecting either  or  are ""reattached"" to the merged node, effectively producing a multigraph. Karger's basic algorithm iteratively contracts randomly chosen edges until only two nodes remain; those nodes represent a cut in the original graph. By iterating this basic algorithm a sufficient number of times, a minimum cut can be found with high probability.","[u'Graph algorithms', u'Graph connectivity']","[u'Adjacency list', u'Adjacency matrix', u'Clifford Stein', u'Computer science', u'Cut (graph theory)', u'Cycle graph', u'David Karger', u'Dense graph', u'Digital object identifier', u'Edge contraction', u'Graph (mathematics)', u'Graph theory', u'Kruskal\u2019s algorithm', u'Mathematical Reviews', u'Max-flow min-cut theorem', u'Maximum flow', u'Minimum cut', u'Minimum spanning tree', u'Multigraph', u'Polynomial time', u'Push\u2013relabel maximum flow algorithm', u'Randomized algorithm']"
Karmarkar's algorithm,"Karmarkar's algorithm is an algorithm introduced by Narendra Karmarkar in 1984 for solving linear programming problems. It was the first reasonably efficient algorithm that solves these problems in polynomial time. The ellipsoid method is also polynomial time but proved to be inefficient in practice.
Where  is the number of variables and  is the number of bits of input to the algorithm, Karmarkar's algorithm requires  operations on  digit numbers, as compared to  such operations for the ellipsoid algorithm. The runtime of Karmarkar's algorithm is thus

using FFT-based multiplication (see Big O notation).
Karmarkar's algorithm falls within the class of interior point methods: the current guess for the solution does not follow the boundary of the feasible set as in the simplex method, but it moves through the interior of the feasible region, improving the approximation of the optimal solution by a definite fraction with every iteration, and converging to an optimal solution with rational data.","[u'Articles with example pseudocode', u'Optimization algorithms and methods', u'Pages containing cite templates with deprecated parameters', u'Software patent law']","[u'AT&T', u'Algorithm', u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u'Big O notation', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Combinatorial optimization', u'Combinatorica', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Feasible set', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Foundation for a Free Information Infrastructure', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Gilbert Strang', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'Interior point method', u'International Standard Serial Number', u'Iterative method', u""Johnson's algorithm"", u'KORBX', u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear Programming', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical Reviews', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Minimum spanning tree', u'Narendra Karmarkar', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Numerical analysis', u'Optimization algorithm', u'Penalty method', u'Philip Gill', u'Polynomial time', u""Powell's method"", u'Prior art', u'Projected Newton barrier method', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'RSA (algorithm)', u'Revised simplex algorithm', u'Robert J. Vanderbei', u'Ronald Rivest', u'Sch\xf6nhage\u2013Strassen algorithm', u'Sequential quadratic programming', u'Simplex algorithm', u'Simplex method', u'Simulated annealing', u'Software patent', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'The Mathematical Intelligencer', u'The New York Times', u'The Pentagon', u'Truncated Newton method', u'Trust region', u'Wolfe conditions']"
Karn's Algorithm,"Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages when using the Transmission Control Protocol (TCP) in computer networking. The algorithm was proposed by Phil Karn in 1987.
Accurate round trip estimates in TCP can be difficult to calculate because of an ambiguity created by retransmitted segments. The round trip time is estimated as the difference between the time that a segment was sent and the time that its acknowledgment was returned to the sender, but when packets are re-transmitted there is an ambiguity: the acknowledgment may be a response to the first transmission of the segment or to a subsequent re-transmission.
Karn's Algorithm ignores retransmitted segments when updating the round trip time estimate. Round trip time estimation is based only on unambiguous acknowledgments, which are acknowledgments for segments that were sent only once.
This simplistic implementation of Karn's algorithm can lead to problems as well. Consider what happens when TCP sends a segment after a sharp increase in delay. Using the prior round trip time estimate, TCP computes a timeout and retransmits a segment. If TCP ignores the round trip time of all retransmitted packets, the round trip estimate will never be updated, and TCP will continue retransmitting every segment, never adjusting to the increased delay.
A solution to this problem is to incorporate transmission timeouts with a timer backoff strategy. The timer backoff strategy computes an initial timeout. If the timer expires and causes a retransmission, TCP increases the timeout generally by a factor of 2. This algorithm has proven to be extremely effective in networks with high packet loss.","[u'Networking algorithms', u'Transmission Control Protocol', u'Wikipedia articles needing page number citations from March 2015']","[u'Computer network', u'Douglas E. Comer', u'Phil Karn', u'PostScript', u'Round-trip time', u'Transmission Control Protocol']"
Kirkpatrick–Seidel algorithm,"The Kirkpatrick–Seidel algorithm, called by its authors ""the ultimate planar convex hull algorithm"" is an algorithm for computing the convex hull of a set of points in the plane, with O(n log h) time complexity, where n is the number of input points and h is the number of points in the hull. Thus, the algorithm is output-sensitive: its running time depends on both the input size and the output size. Another output-sensitive algorithm, the gift wrapping algorithm, was known much earlier, but the Kirkpatrick–Seidel algorithm has an asymptotic running time that is significantly smaller and that always improves on the O(n log n) bounds of non-output-sensitive algorithms. The Kirkpatrick–Seidel algorithm is named after its inventors, David G. Kirkpatrick and Raimund Seidel.
Although the algorithm is asymptotically very efficient, it is not very practical for moderate-sized problems.

",[u'Convex hull algorithms'],"[u'Algorithm', u'Analysis of algorithms', u'Bitangent', u""Chan's algorithm"", u'Convex hull', u'David G. Kirkpatrick', u'Digital object identifier', u'Divide-and-conquer algorithm', u'Franco P. Preparata', u'Gift wrapping algorithm', u'Median', u'Output-sensitive algorithm', u'Raimund Seidel', u'Recursively', u'SIAM Journal on Computing']"
Knuth–Morris–Pratt algorithm,"In computer science, the Knuth–Morris–Pratt string searching algorithm (or KMP algorithm) searches for occurrences of a ""word"" W within a main ""text string"" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.
The algorithm was conceived in 1974 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. The three published it jointly in 1977.","[u'All articles needing additional references', u'Articles needing additional references from October 2009', u'Articles with example pseudocode', u'Donald Knuth', u'String matching algorithms']","[u'-yllion', u'AMS Euler', u'Aho\u2013Corasick algorithm', u'Algorithm', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Big-O notation', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'CWEB', u'Charles E. Leiserson', u'Clifford Stein', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computational complexity theory', u'Computer Modern', u'Computer science', u'Computers and Typesetting', u'Concrete Mathematics', u'Concrete Roman', u'Damerau\u2013Levenshtein distance', u'Dancing Links', u'David Eppstein', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u""Dijkstra's algorithm"", u'Directed acyclic word graph', u'Donald Knuth', u'Edit distance', u'Fisher\u2013Yates shuffle', u'Font', u'GNU MIX Development Kit', u'Generalized suffix tree', u'Hamming distance', u""Hirschberg's algorithm"", u'International Standard Book Number', u'Introduction to Algorithms', u'James H. Morris', u'Jaro\u2013Winkler distance', u""Knuth's Algorithm X"", u""Knuth's Simpath algorithm"", u'Knuth Prize', u'Knuth reward check', u'Knuth\u2013Bendix completion algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'Lexicographically minimal string rotation', u'Linear time', u'List of regular expression software', u'Literate programming', u'Longest common subsequence', u'Longest common substring', u'METAFONT', u'MIX', u'MMIX', u'Man or boy test', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Parsing', u'Pattern matching', u'Potrzebie', u'Pseudocode', u'Quater-imaginary base', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Real-time computing', u'Regular expression', u'Regular tree grammar', u'Robinson\u2013Schensted\u2013Knuth correspondence', u'Ronald L. Rivest', u'Rope (data structure)', u'Selected papers series of Knuth', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'Software', u'String (computer science)', u'String metric', u'String searching algorithm', u'Substring', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Surreal Numbers (book)', u'TeX', u'Ternary search tree', u'The Art of Computer Programming', u'The Complexity of Songs', u'Things a Computer Scientist Rarely Talks About', u'Thomas H. Cormen', u""Thompson's construction"", u'Trabb Pardo\u2013Knuth algorithm', u'Trie', u'Vaughan Pratt', u'WEB', u'Wagner\u2013Fischer algorithm', u'Wojciech Rytter', u'Zentralblatt MATH', u'Zero-based numbering']"
Kosaraju's algorithm,"In computer science, Kosaraju's algorithm (also known as the Kosaraju–Sharir algorithm) is a linear time algorithm to find the strongly connected components of a directed graph. Aho, Hopcroft and Ullman credit it to an unpublished paper from 1978 by S. Rao Kosaraju. The same algorithm was independently discovered by Micha Sharir and published by him in 1981. It makes use of the fact that the transpose graph (the same graph with the direction of every edge reversed) has exactly the same strongly connected components as the original graph.","[u'Graph algorithms', u'Graph connectivity']","[u'Adjacency list', u'Adjacency matrix', u'Alfred V. Aho', u'Algorithm', u'Asymptotically optimal', u'Breadth-first search', u'Charles E. Leiserson', u'Clifford Stein', u'Computer science', u'Depth-first search', u'Directed graph', u'Introduction to Algorithms', u'Jeffrey D. Ullman', u'John E. Hopcroft', u'Linear time', u'Micha Sharir', u'Path-based strong component algorithm', u'Ronald L. Rivest', u'S. Rao Kosaraju', u'Stack (data structure)', u'Strongly connected component', u""Tarjan's strongly connected components algorithm"", u'Thomas H. Cormen', u'Transpose graph']"
Kruskal's algorithm,"Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. It is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component).
This algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48–50 in 1956, and was written by Joseph Kruskal.
Other algorithms for this problem include Prim's algorithm, Reverse-delete algorithm, and Borůvka's algorithm.","[u'All articles lacking in-text citations', u'Articles containing proofs', u'Articles lacking in-text citations from June 2013', u'Articles with example pseudocode', u'Graph algorithms', u'Spanning tree']","[u'A* search algorithm', u'Ackermann function', u'Alpha\u2013beta pruning', u'Arbitrary', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u'Big-O notation', u'Binary logarithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Charles E. Leiserson', u'Clifford Stein', u'Comparison sort', u'Connected component (graph theory)', u'Connectivity (graph theory)', u'Counting sort', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Disjoint-set data structure', u'Dynamic programming', u'Edge (graph theory)', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Glossary of graph theory', u'Graph theory', u'Graph traversal', u'Greedy algorithm', u'Hill climbing', u'International Standard Book Number', u'Introduction to Algorithms', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'JSTOR', u""Johnson's algorithm"", u'Joseph Kruskal', u'Jump point search', u'Lexicographic breadth-first search', u'List of algorithms', u'Mathematical induction', u'Michael T. Goodrich', u'Minimum spanning tree', u'Nonempty', u""Prim's algorithm"", u'Proceedings of the American Mathematical Society', u'Radix sort', u'Reverse-delete algorithm', u'Roberto Tamassia', u'Ronald L. Rivest', u'SMA*', u'Search game', u'Single-linkage clustering', u'Spanning tree', u'Thomas H. Cormen', u'Tree (graph theory)', u'Tree traversal', u'Vertex (graph theory)']"
LR parser,"In computer science, LR parsers are a type of bottom-up parsers that efficiently handle deterministic context-free languages in guaranteed linear time. The LALR parsers and the SLR parsers are common variants of LR parsers. LR parsers are often mechanically generated from a formal grammar for the language by a parser generator tool. They are very widely used for the processing of computer languages, more than other kinds of generated parsers.
The name LR is an acronym. The L means that the parser reads input text in one direction without backing up; that direction is typically Left to right within each line, and top to bottom across the lines of the full input file. (This is true for most parsers.) The R means that the parser produces a reversed Rightmost derivation; it does a bottom-up parse, not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR(k). To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR.
LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages. But LR parsers are not suited for human languages which need more flexible but slower methods. Other parser methods (CYK algorithm, Earley parser, and GLR parser) that backtrack or yield multiple parses may take O(n2), O(n3) or even exponential time when they guess badly.
The above properties of L, R, and k are actually shared by all shift-reduce parsers, including precedence parsers. But by convention, the LR name stands for the form of parsing invented by Donald Knuth, and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser). LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing. This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. LR is also better at error reporting. It detects syntax errors as early in the input stream as possible.","[u'All articles with unsourced statements', u'Articles with unsourced statements from June 2012', u'Parsing algorithms']","[u'Alphabet (formal languages)', u'Ambiguous grammar', u'Backtracking', u'Boldface', u'Bottom-up parsing', u'CYK algorithm', u'Cambridge University Press', u'Canonical LR parser', u'Computer language', u'Computer science', u'Concatenation', u'Context-free grammar', u'Context-free language', u'Dangling else', u'Deterministic context-free language', u'Digital object identifier', u'Donald Knuth', u'Earley parser', u'Finite state automaton', u'Formal grammar', u'Frank DeRemer', u'GLR parser', u'GNU Bison', u'Generalized LR parser', u'International Standard Book Number', u'LALR', u'LALR parser', u'LL parser', u'LL parsing', u'Left corner parser', u'Lexical analysis', u'Nonterminal symbol', u'Operator-precedence parser', u'Parse tree', u'Parser', u'Parser generator', u'Parsing', u'Prefix (formal languages)', u'Prolog', u'Recursive ascent parser', u'Recursive descent parser', u'Rightmost derivation', u'SLR grammar', u'SLR parser', u'Semantics', u'Shift-reduce parser', u'Simple LR parser', u'Simple precedence parser', u'Stack (abstract data type)', u'Terminal symbol', u'Top-down parser', u'Top-down parsing', u'Yacc']"
LZ77 and LZ78,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978. They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG.
They are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, they are only equivalent when the entire data is intended to be decompressed. LZ78 decompression allows random access to the input as long as the entire dictionary is available, while LZ77 decompression must always start at the beginning of the input.
The algorithms were named an IEEE Milestone in 2004.","[u'All accuracy disputes', u'All articles containing potentially dated statements', u'Articles containing potentially dated statements from 2008', u'Articles with disputed statements from November 2010', u'Lossless compression algorithms', u'Use dmy dates from August 2012']","[u'A-law algorithm', u'Abraham Lempel', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'CiteSeer', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Electronic Arts', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Endianness', u'Entropy (information theory)', u'Entropy encoding', u'Explicit dictionary', u'Exponential-Golomb coding', u'Faculty of Electrical Engineering and Computing, University of Zagreb', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'GIF', u'Golomb coding', u'Huffman coding', u'IEEE Transactions on Information Theory', u'Image compression', u'Image resolution', u'Information theory', u'Institute of Electrical and Electronics Engineers', u'Interlaced video', u'Jacob Ziv', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZJB', u'LZRW', u'LZSS', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel-Ziv-Markov chain algorithm', u'Lempel-Ziv-Stac', u'Lempel-Ziv-Storer-Szymanski', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'List of IEEE milestones', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'PalmDoc', u'Peak signal-to-noise ratio', u'Peter Shor', u'Pixel', u'Portable Network Graphics', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sliding window', u'Sliding window compression', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Usenet newsgroup', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
LZJB,"LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS. It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel–Ziv family of compression algorithms. The name LZJB is derived from its parent algorithm and its creator—Lempel Ziv Jeff Bonwick. Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator.","[u'All stub articles', u'Computer science stubs', u'Lossless compression algorithms', u'Sun Microsystems software']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Computer science', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Jeff Bonwick', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Slab allocation', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'ZFS', u'\u039c-law algorithm']"
LZRW,"Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques. This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991.
The variants are:
LZRW1
LZRW1-A
LZRW2
LZRW3
LZRW3-A
LZRW4
LZRW5
The LZJB algorithm used in ZFS is derived from LZRW1.","[u'All stub articles', u'Computer science stubs', u'Free data compression software', u'Lossless compression algorithms']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithms', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Computer science', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Hash tables', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77', u'LZ77 and LZ78', u'LZJB', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Ross Williams (computer scientist)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'ZFS', u'\u039c-law algorithm']"
LZWL,LZWL is a syllable-based variant of the character-based LZW compression algorithm that can work with syllables obtained by all algorithms of decomposition into syllables. The algorithm can be used for words too.,"[u'All articles needing additional references', u'All articles needing style editing', u'Articles needing additional references from January 2013', u'Lossless compression algorithms', u'Wikipedia articles needing style editing from August 2009']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZW', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
LZX,"LZX is the name of an LZ77 family compression algorithm. It is also the name of a file archiver with the same name. Both were invented by Jonathan Forbes and Tomi Poutanen.

","[u'Amiga', u'Lossless compression algorithms']","[u'80x86', u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Amiga', u'Archive formats', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Cabinet (file format)', u'Canada', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Comparison of file archivers', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Jonathan Forbes (programmer)', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'List of archive formats', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Microsoft', u'Microsoft Compressed HTML Help', u'Microsoft Reader', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'OpenLaszlo', u'Operand', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Preprocessor', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Shareware', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tomi Poutanen', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'University of Waterloo', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'Windows 7', u'Windows Imaging Format', u'Windows Vista', u'Xbox Live Avatars', u'\u039c-law algorithm']"
Lamport's Bakery algorithm,"Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion.
In computer science, it is common for multiple threads to simultaneously access the same resources. Data corruption can occur if two or more threads try to write into the same memory location, or if one thread reads a memory location before another has finished writing into it. Lamport's bakery algorithm is one of many mutual exclusion algorithms designed to prevent concurrent threads entering critical sections of code concurrently to eliminate the risk of data corruption.","[u'All articles lacking in-text citations', u'All articles to be merged', u'Articles lacking in-text citations from December 2010', u'Articles to be merged from October 2013', u'Articles with example pseudocode', u'Concurrency control algorithms', u'Use dmy dates from December 2012']","[u'Algorithm', u'Analogy', u'Compare-and-swap', u'Computer science', u'Concurrency (computer science)', u'Cooperative multitasking', u'Critical section', u'Critical sections', u'Data corruption', u""Dekker's algorithm"", u'Eisenberg & McGuire algorithm', u""Lamport's Distributed Mutual Exclusion Algorithm"", u'Leslie Lamport', u'Lexicographical order', u'Memory (computers)', u'Mutual exclusion', u""Peterson's algorithm"", u'PlusCal', u'Pseudocode', u'Semaphore (programming)', u""Szymanski's Algorithm"", u'Thread (computer science)']"
Lamport's Distributed Mutual Exclusion Algorithm,"Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system.

","[u'All articles to be merged', u'All stub articles', u'Articles to be merged from October 2013', u'Computer science stubs', u'Concurrency control algorithms']","[u'Computer science', u'Distributed system', u""Lamport's bakery algorithm"", u'Lamport timestamps', u""Maekawa's Algorithm"", u'Mutual exclusion', u""Naimi-Trehel's Algorithm"", u""Raymond's Algorithm"", u'Ricart-Agrawala algorithm', u'Suzuki-Kasami algorithm']"
Least slack time scheduling,"Least slack time (LST) scheduling is a scheduling algorithm. It assigns priority based on the slack time of a process. Slack time is the amount of time left after a job if the job was started now. This algorithm is also known as least laxity first. Its most common use is in embedded systems, especially those with multiple processors. It imposes the simple constraint that each process on each available processor possesses the same run time, and that individual processes do not have an affinity to a certain processor. This is what lends it a suitability to embedded systems.","[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Scheduling algorithms']","[u'Earliest deadline first scheduling', u'Rate-monotonic scheduling', u'Scheduling algorithm']"
Lempel–Ziv,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978. They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG.
They are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, they are only equivalent when the entire data is intended to be decompressed. LZ78 decompression allows random access to the input as long as the entire dictionary is available, while LZ77 decompression must always start at the beginning of the input.
The algorithms were named an IEEE Milestone in 2004.","[u'All accuracy disputes', u'All articles containing potentially dated statements', u'Articles containing potentially dated statements from 2008', u'Articles with disputed statements from November 2010', u'Lossless compression algorithms', u'Use dmy dates from August 2012']","[u'A-law algorithm', u'Abraham Lempel', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'CiteSeer', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Electronic Arts', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Endianness', u'Entropy (information theory)', u'Entropy encoding', u'Explicit dictionary', u'Exponential-Golomb coding', u'Faculty of Electrical Engineering and Computing, University of Zagreb', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'GIF', u'Golomb coding', u'Huffman coding', u'IEEE Transactions on Information Theory', u'Image compression', u'Image resolution', u'Information theory', u'Institute of Electrical and Electronics Engineers', u'Interlaced video', u'Jacob Ziv', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZJB', u'LZRW', u'LZSS', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel-Ziv-Markov chain algorithm', u'Lempel-Ziv-Stac', u'Lempel-Ziv-Storer-Szymanski', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'List of IEEE milestones', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'PalmDoc', u'Peak signal-to-noise ratio', u'Peter Shor', u'Pixel', u'Portable Network Graphics', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sliding window', u'Sliding window compression', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Usenet newsgroup', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
Lempel–Ziv–Markov chain algorithm,"The Lempel–Ziv–Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression. It has been under development either since 1998 or 1996 and was first used in the 7z format of the 7-Zip archiver. This algorithm uses a dictionary compression scheme somewhat similar to the LZ77 algorithm published by Abraham Lempel and Jacob Ziv in 1977 and features a high compression ratio (generally higher than bzip2)) and a variable compression-dictionary size (up to 4 GB), while still maintaining decompression speed similar to other commonly used compression algorithms.
LZMA2 is a simple container format that can include both uncompressed data and LZMA data, possibly with multiple different LZMA encoding parameters. LZMA2 supports arbitrarily scalable multithreaded compression and decompression and efficient compression of data which is partially incompressible.","[u'All articles covered by WikiProject Wikify', u'All articles needing additional references', u'All articles needing style editing', u'All articles that may contain original research', u'All articles with unsourced statements', u'All pages needing cleanup', u'Articles covered by WikiProject Wikify from July 2014', u'Articles needing additional references from July 2010', u'Articles that may contain original research from April 2012', u'Articles with unsourced statements from June 2013', u'Lossless compression algorithms', u'Wikipedia articles needing style editing from July 2014', u'Wikipedia introduction cleanup from July 2014']","[u'.NET Compact Framework', u'.NET Framework', u'7-Zip', u'7-zip', u'7z', u'7z (file format)', u'A-law algorithm', u'ACE (compression file format)', u'ARC (file format)', u'ARJ', u'Abraham Lempel', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Android (operating system)', u'Android application package', u'Apple Disk Image', u'Ar (Unix)', u'Archive format', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'B1 (archive format)', u'BagIt', u'Binary tree', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Bzip2', u'C++', u'CFS (file format)', u'C (programming language)', u'C Sharp (programming language)', u'Cabinet (file format)', u'Canonical Huffman code', u'Central processing unit', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Common Public License', u'Compact Pro', u'Companding', u'Comparison of archive formats', u'Compress', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'Cpio', u'DEFLATE', u'DGCA (computing)', u'Data compression', u'Deb (file format)', u'Debian', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Doom WAD', u'Dpkg', u'Dynamic Markov compression', u'Dynamic programming', u'Dynamic range', u'EAR (file format)', u'EGG (file format)', u'EPUB', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Embedded system', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fedora', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'GHz', u'GNU Lesser General Public License', u'Gigabyte', u'Go (programming language)', u'Golomb coding', u'Gzip', u'Hash chain', u'Huffman coding', u'IOS', u'Igor Pavlov (programmer)', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'International Standard Book Number', u'JAR (file format)', u'Jacob Ziv', u'Java (programming language)', u'Java EE Connector Architecture', u'KGB Archiver', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LBR (file format)', u'LHA (file format)', u'LZ4 (compression algorithm)', u'LZ77', u'LZ77 and LZ78', u'LZHAM', u'LZJB', u'LZMA', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Linux kernel', u'List of archive formats', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Lzip', u'Lzop', u'MPQ', u'Mac OS', u'Macroblock', u'Markov chains', u'Martin Airport (Slovakia)', u'Megabyte per second', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Mono (software)', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'Open Packaging Conventions', u'Open eBook', u'Open source', u'PAQ', u'Package (OS X)', u'Package format', u'Pascal (programming language)', u'Patricia trie', u'PeaZip', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Public domain', u'Pyramid (image processing)', u'Python (programming language)', u'Quadruple D', u'Quantization (image processing)', u'Quantization (signal processing)', u'RAR (file format)', u'RPM', u'RPM Package Manager', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Rzip', u'SQX', u'SQ (program)', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Shar', u'Silverlight', u'Sliding window', u'Sound quality', u'SourceForge', u'Sourceforge', u'Speech coding', u'Springer Publishing', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'StuffIt', u'Sub-band coding', u'Tar (computing)', u'Thread (computer science)', u'Timeline of information theory', u'Tunstall coding', u'UHARC', u'Unary coding', u'Universal code (data compression)', u'Unix-like', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'WAR (file format)', u'Warped linear predictive coding', u'Wavelet compression', u'Windows Installer', u'Windows Phone', u'XZ Utils', u'Xamarin', u'Xar (archiver)', u'Xbox 360', u'Xz', u'ZIP (file format)', u'ZPAQ', u'Zip (file format)', u'Zoo (file format)', u'\u039c-law algorithm']"
Lempel–Ziv–Oberhumer,"Lempel–Ziv–Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.

","[u'All articles lacking reliable references', u'All articles needing additional references', u'All stub articles', u'Articles lacking reliable references from March 2015', u'Articles needing additional references from July 2014', u'C libraries', u'Free data compression software', u'Lossless compression algorithms', u'Software stubs']","[u'A-law algorithm', u'ANSI C', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Atari TOS', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Btrfs', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'Copyright', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Free software', u'GNU General Public License', u'GitHub', u'Golomb coding', u'Huffman coding', u'IBM AIX (operating system)', u'IRIX', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Java (programming language)', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Linux', u'Log area ratio', u'Lossless', u'Lossless compression', u'Lossy compression', u'Lzop', u'Mac OS', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nintendo 64', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Palm OS', u'Peak signal-to-noise ratio', u'Perl', u'Pixel', u'PlayStation', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Python (programming language)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Software', u'Solaris (operating system)', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'SunOS', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'VxWorks', u'Warped linear predictive coding', u'Wavelet compression', u'Wayback Machine', u'Wii', u'Win32', u'Zfs', u'\u039c-law algorithm']"
Lempel–Ziv–Stac,"Lempel–Ziv–Stac (LZS, or Stac compression) is a lossless data compression algorithm that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding. It was originally developed by Stac Electronics for tape compression, and subsequently adapted for hard disk compression and sold as the Stacker disk compression software. It was later specified as a compression algorithm for various network protocols. LZS is specified in the Cisco IOS stack.",[u'Lossless compression algorithms'],"[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Cisco IOS', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Discrete cosine transform', u'Display resolution', u'DoubleSpace', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Hard disk compression', u'Hifn', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'MS-DOS 6.0', u'Macroblock', u'Microsoft Point-to-Point Compression', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Stac Electronics', u'Stac v. Microsoft', u'Stacker (disk compression)', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
Lempel–Ziv–Storer–Szymanski,"Lempel–Ziv–Storer–Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James Storer and Thomas Szymanski. LZSS was described in article ""Data compression via textual substitution"" published in Journal of the ACM (pp. 928–951).
LZSS is a dictionary encoding technique. It attempts to replace a string of symbols with a reference to a dictionary location of the same string.
The main difference between LZ77 and LZSS is that in LZ77 the dictionary reference could actually be longer than the string it was replacing. In LZSS, such references are omitted if the length is less than the ""break even"" point. Furthermore, LZSS uses one-bit flags to indicate whether the next chunk of data is a literal (byte) or a reference to an offset/length pair.",[u'Lossless compression algorithms'],"[u'A-law algorithm', u'ARJ', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Allegro library', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Game Boy Advance', u'Golomb coding', u'Green Eggs and Ham', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'Journal of the ACM', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LHarc', u'LZ4 (compression algorithm)', u'LZ77', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'PKZip', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'RAR (file format)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Thomas Szymanski', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'Zoo (file format)', u'\u039c-law algorithm']"
Lempel–Ziv–Welch,"Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement, and has the potential for very high throughput in hardware implementations. It was the algorithm of the widely used Unix file compression utility compress, and is used in the GIF image format.","[u'Articles with example pseudocode', u'Lossless compression algorithms', u'Wikipedia articles needing clarification from October 2012']","[u'A-law algorithm', u'Abraham Lempel', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Adobe Acrobat Reader', u'Algebraic code-excited linear prediction', u'Algorithm', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Binary-to-text encoding', u'Bit', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compress', u'Compression artifact', u'Concatenation', u'Constant bitrate', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Data compression', u'Data compression ratio', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'English language', u'Entropy (information theory)', u'Entropy encoding', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'GIF', u'Golomb coding', u'Graphics Interchange Format', u'Gzip', u'Huffman coding', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'International Business Machines', u'Jacob Ziv', u'Karhunen\u2013Lo\xe8ve theorem', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel-Ziv-Markov chain algorithm', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossless data compression', u'Lossy compression', u'Macroblock', u'Mark N. Wegman', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'PDF', u'Patent', u'Peak signal-to-noise ratio', u'Pixel', u'Portable Network Graphics', u'Prediction by partial matching', u'Psychoacoustics', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Range encoding', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Sperry Corporation', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'TIFF', u'Terry Welch', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Uncompress', u'Unisys', u'United States', u'Universal code (data compression)', u'Unix', u'Variable bitrate', u'Victor S. Miller', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'\u039c-law algorithm']"
Lenstra elliptic curve factorization,"The Lenstra elliptic curve factorization or the elliptic curve factorization method (ECM) is a fast, sub-exponential running time algorithm for integer factorization which employs elliptic curves. For general purpose factoring, ECM is the third-fastest known factoring method. The second fastest is the multiple polynomial quadratic sieve and the fastest is the general number field sieve. The Lenstra elliptic curve factorization is named after Hendrik Lenstra.
Practically speaking, ECM is considered a special purpose factoring algorithm as it is most suitable for finding small factors. Currently, it is still the best algorithm for divisors not greatly exceeding 20 to 25 digits (64 to 83 bits or so), as its running time is dominated by the size of the smallest factor p rather than by the size of the number n to be factored. Frequently, ECM is used to remove small factors from a very large integer with many factors; if the remaining integer is still composite, then it has only large factors and is factored using general purpose techniques. The largest factor found using ECM so far has 83 digits and was discovered on 7 September 2013 by R. Propper. Increasing the number of curves tested improves the chances of finding a factor, but they are not linear with the increase in the number of digits.","[u'All articles containing potentially dated statements', u'Articles containing potentially dated statements from 2006', u'Finite fields', u'Integer factorization algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Annals of Mathematics', u'Arjen Lenstra', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big O notation', u'Binary GCD algorithm', u'Binary digit', u'Canfield\u2013Erd\u0151s\u2013Pomerance theorem', u'Carl Pomerance', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Decimal', u'Digital object identifier', u'Discrete logarithm', u'Divisor', u""Dixon's factorization method"", u'Edwards curve', u'Elliptic curve', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Exponential running time', u'Extended Euclidean algorithm', u'Factorial', u""Fermat's factorization method"", u""Fermat's little theorem"", u'Fermat primality test', u'Finite field', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'General purpose computer', u'Generating primes', u'Greatest common divisor', u'Group (mathematics)', u""Hasse's theorem on elliptic curves"", u'Hendrik Lenstra', u'Heuristic', u'Hyperelliptic curve', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Karatsuba algorithm', u'L-notation', u""Lagrange's theorem (group theory)"", u""Lehmer's GCD algorithm"", u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Linear', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Mathematical Reviews', u'Mathematics of Computation', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Modular inverse', u'Montgomery curve', u'Multiplication algorithm', u'Multiplicative group', u'Notices of the American Mathematical Society', u'Number theory', u'OCLC', u'PDF', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u'Point (geometry)', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u'Projective space', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u'Relatively prime', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Strong prime', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Torsion group', u'Trial division', u'UBASIC', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Level set method,"Level set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes. The advantage of the level set model is that one can perform numerical computations involving curves and surfaces on a fixed Cartesian grid without having to parameterize these objects (this is called the Eulerian approach). Also, the level set method makes it very easy to follow shapes that change topology, for example when a shape splits in two, develops holes, or the reverse of these operations. All these make the level set method a great tool for modeling time-varying objects, like inflation of an airbag, or a drop of oil floating in water.

The figure on the right illustrates several important ideas about the level set method. In the upper-left corner we see a shape; that is, a bounded region with a well-behaved boundary. Below it, the red surface is the graph of a level set function  determining this shape, and the flat blue region represents the xy-plane. The boundary of the shape is then the zero level set of , while the shape itself is the set of points in the plane for which  is positive (interior of the shape) or zero (at the boundary).
In the top row we see the shape changing its topology by splitting in two. It would be quite hard to describe this transformation numerically by parameterizing the boundary of the shape and following its evolution. One would need an algorithm able to detect the moment the shape splits in two, and then construct parameterizations for the two newly obtained curves. On the other hand, if we look at the bottom row, we see that the level set function merely translated downward. This is an example of when it can be much easier to work with a shape through its level set function than with the shape directly, where using the shape directly would need to consider and handle all the possible deformations the shape might undergo.
Thus, in two dimensions, the level set method amounts to representing a closed curve  (such as the shape boundary in our example) using an auxiliary function , called the level set function.  is represented as the zero level set of  by

and the level set method manipulates  implicitly, through the function . This function  is assumed to take positive values inside the region delimited by the curve  and negative values outside.
^ Osher, S.; Sethian, J. A. (1988), ""Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton–Jacobi formulations"" (PDF), J. Comput. Phys. 79: 12–49., Bibcode:1988JCoPh..79...12O, doi:10.1016/0021-9991(88)90002-2 
^ 
^","[u'Articles containing video clips', u'Computational fluid dynamics', u'Computer graphics algorithms', u'Image processing', u'Mathematical optimization', u'Numerical analysis']","[u'AUSM', u'Abstract additive Schwarz method', u'Additive Schwarz method', u'Airbag', u'Alternating direction implicit method', u'Analytic element method', u'BDDC', u'Balancing domain decomposition method', u'Bibcode', u'Boundary element method', u'Cambridge University Press', u'Cartesian grid', u'Closed curve', u'Collocation method', u'Computational fluid dynamics', u'Computational geometry', u'Computer graphics', u'Crank\u2013Nicolson method', u'Curve', u'Digital object identifier', u'Discontinuous Galerkin method', u'Domain decomposition methods', u'Eikonal equation', u'Essentially non-oscillatory', u'Euclidean norm', u'Extended finite element method', u'FETI', u'FETI-DP', u'FTCS scheme', u'Fictitious domain method', u'Finite-difference time-domain method', u'Finite difference', u'Finite difference method', u'Finite element method', u'Finite volume method', u""Godunov's scheme"", u'Hamilton\u2013Jacobi equation', u'High-resolution scheme', u'Hp-FEM', u'Hyperbolic partial differential equation', u'Image processing', u'Image segmentation', u'Immersed boundary method', u'International Standard Book Number', u'Isogeometric analysis', u'James Sethian', u'Joel H. Ferziger', u'Lax\u2013Friedrichs method', u'Lax\u2013Wendroff method', u'Level set', u'Level set (data structures)', u'Level set data structures', u'MUSCL scheme', u'MacCormack method', u'Material point method', u'Meshfree methods', u'Method of characteristics', u'Method of lines', u'Mortar methods', u'Multigrid method', u'Neumann\u2013Dirichlet method', u'Neumann\u2013Neumann methods', u'Numerical analysis', u'Numerical partial differential equations', u'Optimization (mathematics)', u'Parabolic partial differential equation', u'Parametric surface', u'Partial differential equation', u'Particle-in-cell', u'Poincar\xe9\u2013Steklov operator', u'Pseudo-spectral method', u'Riemann solver', u'Ronald Fedkiw', u'Schur complement method', u'Schwarz alternating method', u'Shape', u'Smoothed-particle hydrodynamics', u'Spectral element method', u'Spectral method', u'Springer-Verlag', u'Stanley Osher', u'Stochastic Eulerian Lagrangian method', u'Surface', u'Surfaces', u'Topology', u'Upwind scheme', u'Upwinding', u'Volume of fluid method']"
Levenberg–Marquardt algorithm,"In mathematics and computing, the Levenberg–Marquardt algorithm (LMA), also known as the damped least-squares (DLS) method, is used to solve non-linear least squares problems. These minimization problems arise especially in least squares curve fitting.
The LMA is used in many software applications for solving generic curve-fitting problems. However, as for many fitting algorithms, the LMA finds only a local minimum, which is not necessarily the global minimum. The LMA interpolates between the Gauss–Newton algorithm (GNA) and the method of gradient descent. The LMA is more robust than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. LMA can also be viewed as Gauss–Newton using a trust region approach.
The algorithm was first published in 1944 by Kenneth Levenberg, while working at the Frankford Army Arsenal. It was rediscovered in 1963 by Donald Marquardt who worked as a statistician at DuPont and independently by Girard, Wynn and Morrison.","[u'All articles with specifically marked weasel-worded phrases', u'Articles with specifically marked weasel-worded phrases from July 2015', u'Least squares', u'Optimization algorithms and methods', u'Pages with citations lacking titles', u'Statistical algorithms']","[u'.NET Framework', u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'C++', u'C (programming language)', u'C programming language', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Curve fitting', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Donald Marquardt', u'DuPont', u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Estimation theory', u'Evolutionary algorithm', u'Exchange algorithm', u'Fityk', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Fortran', u'Frankford Arsenal', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'GNU General Public License', u'GNU Octave', u'GNU Scientific Library', u'Gauss\u2013Newton algorithm', u'Global minimum', u'Gnuplot', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Haskell (programming language)', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'IDL (programming language)', u'IGOR Pro', u'Ill-posed problems', u'Integer programming', u'International Standard Book Number', u'Iteration', u'Iterative method', u'Jacobian matrix and determinant', u'Java (programming language)', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u'Kenneth Levenberg', u""Kruskal's algorithm"", u'LabVIEW', u'Least squares', u""Lemke's algorithm"", u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local minimum', u'Local search (optimization)', u'MATLAB', u'MEX file', u'MINPACK', u'Mathematica', u'Mathematical optimization', u'Mathematics', u'Matlab', u'Matroid', u'Metaheuristic', u'Minimum spanning tree', u'NMath', u'Nelder\u2013Mead method', u'NeuroSolutions', u""Newton's method in optimization"", u'Non-linear least squares', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization algorithm', u'Origin (software)', u'Penalty method', u'Perl', u'Perl Data Language', u""Powell's method"", u'Public domain', u'Push\u2013relabel maximum flow algorithm', u'Python (programming language)', u'Quadratic programming', u'Quasi-Newton method', u'R (programming language)', u'Revised simplex algorithm', u'Ridge regression', u'Robustness (computer science)', u'SAS (software)', u'SIAM Journal on Numerical Analysis', u'SciPy', u'Scipy', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Sparse matrix', u'Statistician', u'Statistics', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Tikhonov regularization', u'Truncated Newton method', u'Trust region', u'Walter Murray', u'Wolfe conditions']"
Lexicographic breadth-first search,"In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. The algorithm is different from breadth first search, but it produces an ordering that is consistent with breadth-first search.
The lexicographic breadth-first search algorithm is based on the idea of partition refinement and was first developed by Donald J. Rose, Robert E. Tarjan, and George S. Lueker (1976). A more detailed survey of the topic is presented by Corneil (2004). It has been used as a subroutine in other graph algorithms including the recognition of chordal graphs, and optimal coloring of distance-hereditary graphs.","[u'Graph algorithms', u'Search algorithms']","[u'A* search algorithm', u'Adjacency matrix', u'Alpha\u2013beta pruning', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'Breadth first search', u'British Museum algorithm', u'Chordal graph', u'Cograph', u'Comparability graph', u'Complement graph', u'Computer science', u'D*', u'Depth-first search', u'Depth-limited search', u'Depth first search', u'Derek Corneil', u'Digital object identifier', u""Dijkstra's algorithm"", u'Distance-hereditary graph', u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph coloring', u'Graph traversal', u'Greedy coloring', u'Hill climbing', u'Induced subgraph', u'International Standard Book Number', u'Interval graph', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographical order', u'Linear time', u'List of algorithms', u'Partition (set theory)', u'Partition refinement', u""Prim's algorithm"", u'Pseudocode', u'Queue (data structure)', u'Robert Tarjan', u'SIAM Journal on Computing', u'SIAM Journal on Discrete Mathematics', u'SMA*', u'Search game', u'Sorting algorithm', u'Tree traversal']"
Library sort,"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:

Suppose a librarian were to store his books alphabetically on a long shelf, starting with the A's at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Z's. If the librarian acquired a new book that belongs to the B section, once he finds the correct space in the B section, he will have to move every book over, from the middle of the B's all the way down to the Z's in order to make room for the new book. This is an insertion sort. However, if he were to leave a space after every letter, as long as there was still space after B, he would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.

The algorithm was proposed by Michael A. Bender, Martín Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.
Like the insertion sort it is based on, library sort is a stable comparison sort and can be run as an online algorithm; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). The mechanism used for this improvement is very similar to that of a skip list. There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.
Compared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + ε)n, but with no further recommendations on how to choose ε.
One weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.","[u'Comparison sorts', u'Online sorts', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary search', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Digital object identifier', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'List (computing)', u'Martin Farach-Colton', u'Mart\xedn Farach-Colton', u'Merge sort', u'Mergesort', u'Michael A. Bender', u'Miguel Mosteiro', u'Odd\u2013even sort', u'Online algorithm', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Skip list', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Linde–Buzo–Gray algorithm,"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.
It is similar to the k-means method in data clustering.

","[u'Algorithms and data structures stubs', u'All articles lacking reliable references', u'All stub articles', u'Articles lacking reliable references from June 2012', u'Artificial neural networks', u'Computer science stubs', u'Data clustering algorithms', u'Machine learning algorithms']","[u'Algorithm', u'Codebook', u'Data clustering', u'Data structure', u'Digital object identifier', u'IEEE Transactions on Communications', u'K-means', u""Lloyd's algorithm"", u'Robert M. Gray', u'Vector quantization']"
Line drawing algorithm,"A line drawing algorithm is a graphical algorithm for approximating a line segment on discrete graphical media. On discrete media, such as pixel-based displays and printers, line drawing requires such an approximation (in nontrivial cases). Basic algorithms rasterize lines in one color. A better representation with multiple color gradations requires an advanced process, anti-aliasing.
On continuous media, by contrast, no algorithm is necessary to draw a line. For example, oscilloscopes use natural phenomena to draw lines and curves.
The Cartesian slope-intercept equation for a straight line is  With m representing the slope of the line and b as the y intercept. Given that the two endpoints of the line segment are specified at positions  and . we can determine values for the slope m and y intercept b with the following calculations,  so, .","[u'All articles to be expanded', u'Articles to be expanded from December 2009', u'Computer graphics algorithms', u'Featured articles needing translation from German Wikipedia', u'Science articles needing translation from German Wikipedia']","[u'Algorithm', u'Anti-aliasing', u""Bresenham's line algorithm"", u'Computer display', u'Computer printer', u'Digital Differential Analyzer (graphics algorithm)', u'Gupta-Sproull algorithm', u'Oscilloscope', u'Pixel', u'Spatial anti-aliasing', u""Xiaolin Wu's line algorithm""]"
Line segment intersection,"In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect, or cross.
Simple algorithms examine each pair of segments. However, if a large number of possibly intersecting segments are to be checked, this becomes increasingly inefficient since most pairs of segments are not close to one another in a typical input sequence. The most common, more efficient way to solve this problem for a high number of segments is to use a sweep line algorithm, where we imagine a line sliding across the line segments and we track which line segments it intersects at each point in time using a dynamic data structure based on binary search trees. The Shamos–Hoey algorithm applies this principle to solve the line segment intersection detection problem, as stated above, of determining whether or not a set of line segments has an intersection; the Bentley–Ottmann algorithm works by the same principle to list all intersections in logarithmic time per intersection.","[u'Algorithms and data structures stubs', u'All stub articles', u'CS1 errors: chapter ignored', u'Computer science stubs', u'Geometric algorithms']","[u'Algorithm', u'Bentley\u2013Ottmann algorithm', u'Binary search tree', u'CGAL', u'Charles E. Leiserson', u'Clifford Stein', u'Computational geometry', u'Data structure', u'Digital object identifier', u'Euclidean plane', u'International Standard Book Number', u'Introduction to Algorithms', u'Line-line intersection', u'Line segment', u'Ronald L. Rivest', u'Shamos\u2013Hoey algorithm', u'Sweep line algorithm', u'Thomas H. Cormen', u'Washington University in St. Louis']"
Linear-time,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm.","[u'All articles with unsourced statements', u'Analysis of algorithms', u'Articles with unsourced statements from July 2014', u'Computational complexity theory', u'Computational resources', u'Use dmy dates from September 2010']","[u'2-EXPTIME', u'3SAT', u'AKS primality test', u'Abstract machine', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alan Cobham', u'Alexander Schrijver', u'Algorithm', u'Amortized time', u'Approximation algorithm', u'Approximation algorithms', u'ArXiv', u'Array data structure', u'Average-case complexity', u'Avi Wigderson', u'BPP (complexity)', u'BQP', u'Big O notation', u'Binary numeral system', u'Binary search', u'Binary search algorithm', u'Binary tree', u'Binary tree sort', u'Bounded-error probabilistic polynomial', u'Boyer\u2013Moore string search algorithm', u'Brute-force search', u'Bubble sort', u'Christos H. Papadimitriou', u""Cobham's thesis"", u'Cole-Vishkin algorithm', u'Comparison sort', u'Complexity Zoo', u'Complexity class', u'Computable function', u'Computation model', u'Computational complexity of mathematical operations', u'Computational complexity theory', u'Computer science', u'Conjunctive normal form', u'Content-addressable memory', u'Convolution theorem', u'DLOGTIME', u'DTIME', u'Decision problem', u'Deterministic Turing machine', u'Digital object identifier', u'Disjoint set data structure', u'Double exponential function', u'Dynamic programming', u'EXP', u'EXPTIME', u'E (complexity)', u'Element (math)', u'Elsevier', u'Euclidean algorithm', u'Exponential time hypothesis', u'Fast Fourier transform', u'Formal language', u'General number field sieve', u'Graph (mathematics)', u'Graph isomorphism problem', u'Greatest common divisor', u""Grover's algorithm"", u'Gr\xf6bner basis', u'Heapsort', u'In-place merge sort', u'Infra-exponential', u'Insertion sort', u'Instruction (computer science)', u'Integer factorization', u'International Standard Book Number', u'International Standard Serial Number', u'Introsort', u'Inverse Ackermann function', u'Iterated logarithm', u'Journal of Computer and System Sciences', u""Karmarkar's algorithm"", u'Kd-tree', u'L-notation', u'Lance Fortnow', u'Lecture Notes in Computer Science', u'Linear', u'Linear programming', u'Logarithm', u'Logarithmic growth', u'Logarithmic identities', u'L\xe1szl\xf3 Babai', u'L\xe1szl\xf3 Lov\xe1sz', u'Matrix chain multiplication', u'Maximum matching', u'Merge sort', u'Michael Sipser', u'Monge array', u'NP-complete', u'NP-hard', u'NP (complexity)', u'Noam Nisan', u'Non-deterministic Turing machine', u'Optimization (mathematics)', u'P (complexity)', u'P versus NP', u'P versus NP problem', u'P \u2260 NP', u'Parallel Random Access Machine', u'Parallel algorithm', u'Parallel computing', u'Parameterized complexity', u'Partial correlation', u'Patience sorting', u'Polygon triangulation', u'Polynomial expression', u'Presburger arithmetic', u'Priority queue', u'Probabilistic Turing machine', u'Product (mathematics)', u'Property testing', u'Pseudo-polynomial time', u'Quantifier elimination', u'Quantum Turing machine', u'Quantum algorithm', u'Quasilinear time', u'Quicksort', u'RP (complexity)', u'Raimund Seidel', u'Randomized algorithm', u'Real closed field', u'Recurrence relation', u'Reduction (complexity)', u'Repeated squaring', u'Robustness (computer science)', u'Running Time (film)', u'Running time', u'Russell Impagliazzo', u'Self-balancing binary search tree', u'Set cover', u'Shell sort', u'Smoothsort', u'Society for Industrial and Applied Mathematics', u'Sorting algorithm', u'Space complexity', u'Springer-Verlag', u'Steiner tree problem', u""Stirling's approximation"", u'String (computer science)', u'Traveling salesman problem', u'Travelling salesman problem', u'Turing machine', u""Ukkonen's Algorithm"", u'Upper bound', u'Worst-case complexity', u'ZPP (complexity)']"
Linear programming,"Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).
More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.
Linear programs are problems that can be expressed in canonical form as

where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and  is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second then we can say the first vector is less-than or equal-to the second vector.
Linear programming can be applied to various fields of study. It is widely used in business and economics, and is also utilized for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proved useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.","[u'All articles needing additional references', u'Articles containing Russian-language text', u'Articles needing additional references from October 2015', u'Convex optimization', u'Geometric algorithms', u'Linear programming', u'Mathematical and quantitative methods (economics)', u'Operations research', u'P-complete problems', u'Unsolved problems in computer science', u'Wikipedia external links cleanup from August 2010', u'Wikipedia spam cleanup from August 2010']","[u'.NET Framework', u'AIMMS', u'AMPL', u'API', u'APMonitor', u'Affine function', u'Albert W. Tucker', u'Algebraic modeling language', u'Algorithm', u'Apache License', u'Approximation algorithm', u'Approximation algorithms', u'ArXiv', u'Arrangement polytope', u'Assignment problem', u'Augmented Lagrangian method', u'BSD licenses', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Benders' decomposition"", u'Block diagram', u'Block matrix', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Branch and price', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'COIN-OR CLP', u'CPLEX', u'Canonical form', u'Cassowary constraint solver', u'Christos H. Papadimitriou', u'Coefficient', u'Combinatorial optimization', u'Comparability', u'Comparison of optimization software', u'Computers and Intractability: A Guide to the Theory of NP-Completeness', u'Concave function', u'Constraint (mathematics)', u'Convex function', u'Convex lattice polytope', u'Convex minimization', u'Convex optimization', u'Convex polytope', u'Convex programming', u'Convex set', u'Coopr', u'Copyleft', u'Covering problem', u'Covering problems', u'Criss-cross algorithm', u'Cutting-plane method', u'Dantzig-Wolfe decomposition', u'David S. Johnson', u'Davidon\u2013Fletcher\u2013Powell formula', u'Delayed column generation', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dominating set problem', u'Dual problem', u'Duality (optimization)', u'Dynamic programming', u'Dynamical system', u'Economics', u'Edge cover', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Euler', u'Evolutionary algorithm', u'Exchange algorithm', u'FICO Xpress', u'Feasible region', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'FortMP', u'Fourier\u2013Motzkin elimination', u'Fractional coloring', u'Frank Lauren Hitchcock', u'Frank\u2013Wolfe algorithm', u'FreeMat', u'Function (mathematics)', u'GNU Linear Programming Kit', u'GNU MathProg', u'Game theory', u'Gauss\u2013Newton algorithm', u'General Algebraic Modeling System', u'George B. Dantzig', u'George Dantzig', u'Gilbert Strang', u'Global maximum', u'Global minimum', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph (mathematics)', u'Graph algorithm', u'Graph diameter', u'Greedy algorithm', u'Gurobi', u'G\xfcnter M. Ziegler', u'Half-space (geometry)', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Hirsch conjecture', u'IMSL Numerical Libraries', u'Independent set (graph theory)', u'Independent set problem', u'Integer decomposition property', u'Integer programming', u'Interior-point method', u'Interior point method', u'International Standard Book Number', u'International Standard Serial Number', u'Intersection (mathematics)', u'Iterative method', u'JSTOR', u'Java (programming language)', u'Ji\u0159\xed Matou\u0161ek (mathematician)', u'Job shop scheduling', u'John von Neumann', u""Johnson's algorithm"", u'Joseph Fourier', u""Karmarkar's algorithm"", u""Karp's 21 NP-complete problems"", u'Katta G. Murty', u'Klee\u2013Minty cube', u""Kruskal's algorithm"", u'LINDO', u'LP-type problem', u""Lemke's algorithm"", u'Leonid Kantorovich', u'Leonid Khachiyan', u'Level set', u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear', u'Linear-fractional programming (LFP)', u'Linear complementarity problem', u'Linear equality', u'Linear function', u'Linear functional', u'Linear inequality', u'Linear programming', u'Linear programming relaxation', u'Lingo (programming language)', u'List of numerical analysis topics', u'List of unsolved problems in computer science', u'Local convergence', u'Local maximum', u'Local minimum', u'Local search (optimization)', u'LpSolve', u'MATLAB', u'MINTO', u'MOSEK', u'Maple (software)', u'Mark Overmars', u'Matching (graph theory)', u'Mathcad', u'Mathematica', u'Mathematical Reviews', u'Mathematical model', u'Mathematical optimization', u'Mathematical programming', u'Matrix (mathematics)', u'Matrix transpose', u'Matroid', u'Maximum principle', u'Metaheuristic', u'Michael R. Garey', u'Microeconomics', u'Microsoft Excel', u'Microsoft Solver Foundation', u'Minimum spanning tree', u'Mixed complementarity problem', u'Mixed linear complementarity problem', u'NAG Numerical Library', u'NMath Stats', u'NP-hard', u'Narendra Karmarkar', u'National Diet Library', u'Naum Z. Shor', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nobel prize in economics', u'Nonlinear complementarity problem', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Numerical Algorithms Group', u'O-Matrix', u'OR/MS Today', u'Objective function', u'Octave', u'Odysseus', u'OpenOpt', u'Operations research', u'OptimJ', u'Optimization (mathematics)', u'Optimization Toolbox', u'Optimization algorithm', u'Oriented matroid', u'PHP', u'P (complexity)', u'Packing problem', u'Packing problems', u'Path-following', u'Penalty method', u'Permissive free software licence', u'Plane (geometry)', u'Polygon', u'Polyhedron', u'Polynomial-time', u'Polynomial time', u'Polytope', u""Powell's method"", u'Proceedings of the USSR Academy of Sciences', u'Profit maximization', u'Projective method', u'Proprietary software', u'Push\u2013relabel maximum flow algorithm', u'Python (programming language)', u'Qoca', u'Quadratic programming', u'Quasi-Newton method', u'R', u'R-Project', u'Real number', u'Revised simplex algorithm', u'Robert J. Vanderbei', u'Routing', u'SAS System', u'SCIP (optimization software)', u'Sage (mathematics software)', u'Scheduling (production processes)', u'Scilab', u'Semidefinite programming', u'Sequential quadratic programming', u'Set cover problem', u'Set packing', u'Shadow price', u'Simple polygon', u'Simplex algorithm', u'Simulated annealing', u'Slack variable', u""Smale's problems"", u'Springer-Verlag', u'Stephen Smale', u'Stochastic programming', u'Strong duality', u'Subgradient method', u'Submodular', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Sysquake', u'TOMLAB', u'Tabu search', u'The Mathematical Intelligencer', u'Time complexity', u'Tjalling Koopmans', u'Total dual integrality', u'Totally unimodular', u'Totally unimodular matrix', u'Traveling salesman problem', u'Truncated Newton method', u'Trust region', u'Unit cube', u'Variable (programming)', u'Vector space', u'Vertex cover', u'Vertex cover problem', u'Vijay Vazirani', u'VisSim', u'Weak duality', u""What'sBest"", u'Wolfe conditions', u'World War II', u'Worst-case complexity', u'Yinyu Ye']"
Linear search,"In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted. The list need not be ordered.
Linear search is the simplest search algorithm; it is a special case of brute-force search. Its worst case cost is proportional to the number of elements in the list. Its expected cost is also proportional to the number of elements if all elements are searched equally. If the list has more than a few elements and is searched often, then more complicated search methods such as binary search or hashing may be appropriate. Those methods have faster search times but require additional resources to attain that speed.","[u'All articles needing additional references', u'Articles needing additional references from November 2010', u'Articles with example Java code', u'Articles with example pseudocode', u'Search algorithms']","[u'Array data structure', u'Array index', u'Assembly language', u'Asymptotic complexity', u'Average-case complexity', u'Big O notation', u'Binary search', u'Branch instruction', u'Brute-force search', u'Compiler', u'Computer science', u'Donald Knuth', u'Geometric distribution', u'Hash table', u'High-level programming languages', u'Instruction (computer science)', u'International Standard Book Number', u'Linear search problem', u'Linked list', u'List (computing)', u'Machine language', u'Null pointer', u'Pseudocode', u'Recursion', u'Reference (computer science)', u'Search algorithm', u'Search data structure', u'Sentinel value', u'Sort (computing)', u'Ternary search', u'Worst-case complexity']"
Linear time,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm.","[u'All articles with unsourced statements', u'Analysis of algorithms', u'Articles with unsourced statements from July 2014', u'Computational complexity theory', u'Computational resources', u'Use dmy dates from September 2010']","[u'2-EXPTIME', u'3SAT', u'AKS primality test', u'Abstract machine', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alan Cobham', u'Alexander Schrijver', u'Algorithm', u'Amortized time', u'Approximation algorithm', u'Approximation algorithms', u'ArXiv', u'Array data structure', u'Average-case complexity', u'Avi Wigderson', u'BPP (complexity)', u'BQP', u'Big O notation', u'Binary numeral system', u'Binary search', u'Binary search algorithm', u'Binary tree', u'Binary tree sort', u'Bounded-error probabilistic polynomial', u'Boyer\u2013Moore string search algorithm', u'Brute-force search', u'Bubble sort', u'Christos H. Papadimitriou', u""Cobham's thesis"", u'Cole-Vishkin algorithm', u'Comparison sort', u'Complexity Zoo', u'Complexity class', u'Computable function', u'Computation model', u'Computational complexity of mathematical operations', u'Computational complexity theory', u'Computer science', u'Conjunctive normal form', u'Content-addressable memory', u'Convolution theorem', u'DLOGTIME', u'DTIME', u'Decision problem', u'Deterministic Turing machine', u'Digital object identifier', u'Disjoint set data structure', u'Double exponential function', u'Dynamic programming', u'EXP', u'EXPTIME', u'E (complexity)', u'Element (math)', u'Elsevier', u'Euclidean algorithm', u'Exponential time hypothesis', u'Fast Fourier transform', u'Formal language', u'General number field sieve', u'Graph (mathematics)', u'Graph isomorphism problem', u'Greatest common divisor', u""Grover's algorithm"", u'Gr\xf6bner basis', u'Heapsort', u'In-place merge sort', u'Infra-exponential', u'Insertion sort', u'Instruction (computer science)', u'Integer factorization', u'International Standard Book Number', u'International Standard Serial Number', u'Introsort', u'Inverse Ackermann function', u'Iterated logarithm', u'Journal of Computer and System Sciences', u""Karmarkar's algorithm"", u'Kd-tree', u'L-notation', u'Lance Fortnow', u'Lecture Notes in Computer Science', u'Linear', u'Linear programming', u'Logarithm', u'Logarithmic growth', u'Logarithmic identities', u'L\xe1szl\xf3 Babai', u'L\xe1szl\xf3 Lov\xe1sz', u'Matrix chain multiplication', u'Maximum matching', u'Merge sort', u'Michael Sipser', u'Monge array', u'NP-complete', u'NP-hard', u'NP (complexity)', u'Noam Nisan', u'Non-deterministic Turing machine', u'Optimization (mathematics)', u'P (complexity)', u'P versus NP', u'P versus NP problem', u'P \u2260 NP', u'Parallel Random Access Machine', u'Parallel algorithm', u'Parallel computing', u'Parameterized complexity', u'Partial correlation', u'Patience sorting', u'Polygon triangulation', u'Polynomial expression', u'Presburger arithmetic', u'Priority queue', u'Probabilistic Turing machine', u'Product (mathematics)', u'Property testing', u'Pseudo-polynomial time', u'Quantifier elimination', u'Quantum Turing machine', u'Quantum algorithm', u'Quasilinear time', u'Quicksort', u'RP (complexity)', u'Raimund Seidel', u'Randomized algorithm', u'Real closed field', u'Recurrence relation', u'Reduction (complexity)', u'Repeated squaring', u'Robustness (computer science)', u'Running Time (film)', u'Running time', u'Russell Impagliazzo', u'Self-balancing binary search tree', u'Set cover', u'Shell sort', u'Smoothsort', u'Society for Industrial and Applied Mathematics', u'Sorting algorithm', u'Space complexity', u'Springer-Verlag', u'Steiner tree problem', u""Stirling's approximation"", u'String (computer science)', u'Traveling salesman problem', u'Travelling salesman problem', u'Turing machine', u""Ukkonen's Algorithm"", u'Upper bound', u'Worst-case complexity', u'ZPP (complexity)']"
List of algorithm general topics,"This is a list of algorithm general topics.
Analysis of algorithms
Ant colony algorithm
Approximation algorithm
Best and worst cases
Big O notation
Combinatorial search
Competitive analysis
Computability theory
Computational complexity theory
Embarrassingly parallel problem
Emergent algorithm
Evolutionary algorithm
Fast Fourier transform
Genetic algorithm
Graph exploration algorithm
Heuristic
Hill climbing
Implementation
Las Vegas algorithm
Lock-free and wait-free algorithms
Monte Carlo algorithm
Numerical analysis
Online algorithm
Polynomial time approximation scheme
Problem size
Pseudorandom number generator
Quantum algorithm
Random-restart hill climbing
Randomized algorithm
Running time
Sorting algorithm
Search algorithm
Stable algorithm (disambiguation)
Super-recursive algorithm
Tree search algorithm","[u'Algorithms', u'Mathematics-related lists']","[u'Algorithm', u'Analysis of algorithms', u'Ant colony algorithm', u'Approximation algorithm', u'Best and worst cases', u'Big O notation', u'Combinatorial search', u'Competitive analysis (online algorithm)', u'Complexity class', u'Computability theory', u'Computational complexity theory', u'Embarrassingly parallel', u'Emergent algorithm', u'Evolutionary algorithm', u'Fast Fourier transform', u'Genetic algorithm', u'Graph exploration algorithm', u'Heuristic', u'Hill climbing', u'Implementation', u'Las Vegas algorithm', u'List of algorithms', u'List of complexity classes', u'List of computability and complexity topics', u'List of data structures', u'Lock-free and wait-free algorithms', u'Monte Carlo algorithm', u'Numerical analysis', u'Online algorithm', u'Polynomial time approximation scheme', u'Problem size', u'Pseudorandom number generator', u'Quantum algorithm', u'Random-restart hill climbing', u'Randomized algorithm', u'Running time', u'Search algorithm', u'Sorting algorithm', u'Stable algorithm (disambiguation)', u'Super-recursive algorithm', u'Tree search algorithm']"
List of algorithms,The following is a list of algorithms along with one-line descriptions for each.,"[u'Algorithms', u'All articles needing additional references', u'Articles contradicting other articles', u'Articles needing additional references from April 2014', u'Mathematics-related lists']","[u'3Dc', u'A*', u'A-law algorithm', u'AC-3 algorithm', u'AKS primality test', u'ALOPEX', u'Abstract Algebra', u'AdaBoost', u'Adaptive-additive algorithm', u'Adaptive Huffman coding', u'Adaptive coding', u'Adaptive histogram equalization', u'Adaptive replacement cache', u'Addition-chain exponentiation', u'Adler-32', u'Advanced Encryption Standard', u'Affine transformation', u'Aho\u2013Corasick string matching algorithm', u'Algorithm X', u'Algorithms for Recovery and Isolation Exploiting Semantics', u'Algorithms for calculating variance', u'All pairs shortest path', u'Alpha-beta pruning', u'Alpha max plus beta min algorithm', u'Ambient occlusion', u'Analytical hierarchy', u'Ant colony optimization', u'Antiderivatives', u'Antipodal point', u'Approximate counting algorithm', u'Apriori algorithm', u'Arbitrary-precision arithmetic', u'Arithmetic coding', u'Arithmetical hierarchy', u'Arnoldi iteration', u'Artificial neural network', u'Assignment problem', u'Association rule learning', u'Astronomical algorithms', u'Asymmetric key algorithm', u'Audio data compression', u'B*', u'B-spline', u'BCH Code', u'BCJR algorithm', u'BFGS method', u'BKM algorithm', u'Baby-step giant-step', u'Backpropagation', u'Backtracking', u'Backward Euler method', u'Bailey\u2013Borwein\u2013Plouffe formula', u'Baillie-PSW primality test', u'Bandwidth (matrix theory)', u""Banker's algorithm"", u'Barnes\u2013Hut simulation', u'Basic Local Alignment Search Tool', u'Basis function', u'Baum\u2013Welch algorithm', u'Bayesian statistics', u'Bead sort', u'Beam search', u'Beam stack search', u'Beam tracing', u'Bees algorithm', u'Bellman\u2013Ford algorithm', u""Benson's algorithm"", u'Bentley\u2013Ottmann algorithm', u'Berkeley algorithm', u'Berlekamp\u2013Massey algorithm', u'Best-first search', u'Best Bin First', u'Biconjugate gradient method', u'Bicubic interpolation', u'Bidirectional search', u'Bilinear interpolation', u'Binary GCD algorithm', u'Binary search algorithm', u'Binary splitting', u'Bioinformatics', u'Bionics', u'Birkhoff interpolation', u'Bisection method', u'Bitap algorithm', u'Bitonic sorter', u'Block Truncation Coding', u'Block nested loop', u'Bloom filter', u'Blowfish (cipher)', u""Bluestein's FFT algorithm"", u'Blum Blum Shub', u'Boehm garbage collector', u'Bogosort', u'Boosting (meta-algorithm)', u""Booth's multiplication algorithm"", u'Bootstrap aggregating', u""Borwein's algorithm"", u""Bor\u016fvka's algorithm"", u'Bowyer\u2013Watson algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Branch and bound', u'Branch and cut', u'Breadth-first search', u""Bresenham's line algorithm"", u'Bron\u2013Kerbosch algorithm', u'BrownBoost', u'Bruss algorithm', u""Bruun's FFT algorithm"", u'Bubble sort', u""Buchberger's algorithm"", u'Bucket sort', u'Buddy memory allocation', u'Bully algorithm', u'Burrows\u2013Wheeler transform', u'Burst trie', u'Burstsort', u""Buzen's algorithm"", u'Byte pair encoding', u'Byzantine fault tolerance', u'B\xe9zier curve', u'C3 linearization', u'C4.5 algorithm', u'CHS conversion', u'CNF-SAT', u'CORDIC', u'CYK algorithm', u'Cache algorithms', u'Calculus', u""Cannon's algorithm"", u'Canny edge detector', u'Canonical LR parser', u'Canopy clustering algorithm', u'Cantor\u2013Zassenhaus algorithm', u'Causality', u'Chaff algorithm', u'Chain matrix multiplication', u""Chaitin's algorithm"", u'Chakravala method', u""Chan's algorithm"", u""Cheney's algorithm"", u""Chew's second algorithm"", u'Chien search', u'Cholesky decomposition', u'Chomsky normal form', u'Christofides algorithm', u'Clipping (computer graphics)', u'Clock synchronization', u'Clock with Adaptive Replacement', u'Closest pair problem', u'Cocktail sort', u'Code-excited linear prediction', u'Coding theory', u'Cohen\u2013Sutherland', u'Collision detection', u'Coloring algorithm', u'Comb sort', u'Combinatorial optimization', u'Combinatorics', u'Complete-linkage clustering', u'Computational complexity theory', u'Computational geometry', u'Computational linguistics', u'Computational mathematics', u'Computational physics', u'Computational science', u'Computational statistics', u'Computer algebra', u'Computer architecture', u'Computer graphics', u'Computer science', u'Computing \u03c0', u'Computus', u'Cone algorithm', u'Cone tracing', u'Congruence of squares', u'Conjugate gradient', u'Conjunctive normal form', u'Connected-component labeling', u'Constrained Delaunay triangulation', u'Constraint algorithm', u'Constraint satisfaction', u'Context-free grammar', u'Context tree weighting', u'Contour line', u'Convex hull', u'Convex hull algorithms', u'Convex polygon', u'Convex set', u'Cooley\u2013Tukey FFT algorithm', u'Coppersmith\u2013Winograd algorithm', u'Coset', u'Counting sort', u'Crank-Nicolson method', u""Cristian's algorithm"", u'Cross-entropy method', u'Cryptographic hash function', u'Cryptographically secure pseudo-random number generator', u'Cryptography', u'Cubic interpolation', u'Cuthill\u2013McKee algorithm', u'Cutting-plane method', u'Cycle detection', u'Cycle sort', u'Cyclic redundancy check', u'Cyrus\u2013Beck', u'D*', u'DBSCAN', u'DEFLATE (algorithm)', u'DPLL algorithm', u'DTMF', u'Daitch\u2013Mokotoff Soundex', u'Damerau\u2013Levenshtein distance', u'Damm algorithm', u'Dancing Links', u'Dantzig\u2013Wolfe decomposition', u'Data Encryption Standard', u'Data clustering', u'Data mining', u'Database', u'Davis\u2013Putnam algorithm', u'De Boor algorithm', u'De Bruijn graph', u""De Casteljau's algorithm"", u'Decision tree learning', u""Dekker's algorithm"", u'Delaunay triangulation', u'Delayed column generation', u'Delta encoding', u'Demon algorithm', u'Depth-first search', u'Deterministic automaton', u'Deutsch-Jozsa algorithm', u""Dice's coefficient"", u'Dictionary coder', u'Difference-map algorithm', u'Difference map algorithm', u'Differential equation', u'Differential evolution', u'Diffie\u2013Hellman key exchange', u'Digital Differential Analyzer (graphics algorithm)', u'Digital Signature Algorithm', u'Digital signal processing', u""Dijkstra's algorithm"", u'Dijkstra-Scholten algorithm', u""Dinic's algorithm"", u'Discrete Fourier transform', u""Discrete Green's Theorem"", u'Discrete logarithm', u'Disk scheduling', u'Distributed algorithm', u'Distributed systems', u'Dithering', u'Divide and conquer algorithm', u'Division algorithm', u""Dixon's algorithm"", u'Doomsday algorithm', u'Double Metaphone', u'Double dabble', u'Duality (mathematics)', u'Dynamic Markov compression', u'Dynamic Programming', u'Dynamic time warping', u'Dynamical system', u'ESC algorithm', u'Earley parser', u'Earliest deadline first scheduling', u'Edge detection', u""Edmonds's algorithm"", u'Edmonds\u2013Karp algorithm', u'Eigenvalue algorithm', u'ElGamal encryption', u'Elementary function (differential algebra)', u'Elevator algorithm', u'Elias delta coding', u'Elias gamma coding', u'Elias omega coding', u'Ellipsoid method', u'Elliptic curve cryptography', u'Embedded Zerotree Wavelet', u'Entropy', u'Entropy encoding', u'Error detection and correction', u'Error diffusion', u'Espresso heuristic logic minimizer', u'Estimation theory', u'Euclidean algorithm', u'Euclidean distance map', u'Euclidean minimum spanning tree', u'Euclidean shortest path problem', u'Euler integration', u'Euler method', u'Evolution strategy', u'Evolutionary computation', u'Exact cover', u'Expectation-maximization algorithm', u'Exponential-Golomb coding', u'Exponential backoff', u'Exponential function', u'Exponentiating by squaring', u'Extended Euclidean algorithm', u'Extrapolation', u'FELICS', u'FLAME clustering', u'FNN algorithm', u'Fair-share scheduling', u'False position method', u'Fast Cosine Transform', u'Fast Fourier transform', u'Fast clipping', u'Fast folding algorithm', u'Fast multipole method', u'Fatigue (material)', u'Faug\xe8re F4 algorithm', u'Fault-tolerant system', u""Featherstone's algorithm"", u'Feature detection (computer vision)', u'Feature space', u""Fermat's factorization method"", u'Fermat primality test', u'Fibonacci coding', u'Fibonacci numbers', u'Fibonacci search technique', u'Finite difference method', u'Fisher\u2013Yates shuffle', u'Fitness proportionate selection', u'Flashsort', u""Fletcher's checksum"", u'Flood fill', u'Flow network', u""Floyd's cycle-finding algorithm"", u'Floyd\u2013Steinberg dithering', u'Floyd\u2013Warshall algorithm', u'Force-based algorithms (graph drawing)', u'Ford\u2013Fulkerson algorithm', u'Formal grammar', u'Fortuna (PRNG)', u""Fortune's Algorithm"", u'Forward-backward algorithm', u'Forward error correction', u'Fowler\u2013Noll\u2013Vo hash function', u'Fractal compression', u'Fractal dimension', u""Freivalds' algorithm"", u'Fuzzy clustering', u""F\xfcrer's algorithm"", u'GLR parser', u'Garbage collection (computer science)', u'Gaussian elimination', u'Gauss\u2013Jordan elimination', u'Gauss\u2013Legendre algorithm', u'Gauss\u2013Newton algorithm', u'Gauss\u2013Seidel method', u'Gene expression programming', u'General Problem Solver', u'General number field sieve', u'Generalised Hough transform', u'Genetic algorithms', u'Geometric hashing', u'Geoscience', u'Gerchberg\u2013Saxton algorithm', u'Gibbs sampling', u'Gift wrapping algorithm', u'Gilbert\u2013Johnson\u2013Keerthi distance algorithm', u'Girvan\u2013Newman algorithm', u'Global illumination', u'Gnome sort', u'Goertzel algorithm', u'Golden section search', u'Goldschmidt division', u'Golomb coding', u'Gordon\u2013Newell theorem', u""Gosper's algorithm"", u'Gouraud shading', u'Gradient descent', u'Graham scan', u'Gram\u2013Schmidt process', u'Graph drawing', u'Graph search algorithm', u'Graph theory', u'Gray code', u'Greatest common divisor', u'Greedy randomized adaptive search procedure', u'Ground state', u""Grover's algorithm"", u'GrowCut algorithm', u'Gr\xf6bner basis', u'Half-toning', u""Halley's method"", u'Hamming(7,4)', u'Hamming code', u'Hamming distance', u'Hamming weight', u'Harmony search', u'Hash Function', u'Hash join', u'Hash tree (persistent data structure)', u""Heap's algorithm"", u'Heapsort', u'Hermite interpolation', u'Heuristic', u'Hidden Markov model', u'Hidden markov model', u'Hidden surface determination', u'Hindley-Milner type inference', u""Hirschberg's algorithm"", u'Histogram equalization', u'Hopcroft\u2013Karp algorithm', u'Hopfield net', u'Hough transform', u""Huang's algorithm"", u'Hubs and authorities', u'Huffman coding', u'Hungarian algorithm', u'Hungarian method', u'Hyperlink-Induced Topic Search', u'ID3 algorithm', u'Image-based lighting', u'Image Compression', u'Image compression', u'Image processing', u'Importance sampling', u'Incremental encoding', u'Incremental heuristic search', u'Index calculus algorithm', u'Information theory', u'Insertion sort', u'Inside-outside algorithm', u'Integer factorization', u'Integer linear programming', u'Interior point method', u'International Data Encryption Algorithm', u'Interpolation', u'Interpolation search', u'Intersection algorithm', u'Introsort', u'Inverse iteration', u'Isosurface', u'Iterative deepening depth-first search', u'Jaccard index', u'Jacobi eigenvalue algorithm', u'Jaro\u2013Winkler distance', u'Johnson algorithm', u'Join (SQL)', u'Jump-and-Walk algorithm', u'Jump point search', u'Jump search', u'K-means++', u'K-means clustering', u'K-medoids', u'K-nearest neighbors', u'Kabsch algorithm', u""Kadane's algorithm"", u'Kahan summation algorithm', u'Kalman filter', u'Karatsuba algorithm', u""Karger's algorithm"", u""Karmarkar's algorithm"", u""Karn's Algorithm"", u'Karplus-Strong string synthesis', u'Keyed-hash message authentication code', u'Kirkpatrick\u2013Seidel algorithm', u""Knight's Tour"", u'Knuth\u2013Bendix completion algorithm', u'Knuth\u2013Morris\u2013Pratt algorithm', u""Kosaraju's algorithm"", u""Kruskal's algorithm"", u'LL parser', u'LPBoost', u'LR parser', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX', u'Lagged Fibonacci generator', u'Lagrange interpolation', u'Lagrange polynomial', u""Lamport's Bakery algorithm"", u""Lamport's Distributed Mutual Exclusion Algorithm"", u'Lamport ordering', u'Lanczos iteration', u'Lanczos resampling', u'Laplacian smoothing', u'Lax-Wendroff method', u'Least slack time scheduling', u'Least squares', u'Lempel\u2013Ziv', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Lenstra elliptic curve factorization', u'Lesk algorithm', u'Level set method', u'Levenberg\u2013Marquardt algorithm', u'Levenshtein coding', u'Levenshtein distance', u'Levinson recursion', u'Lexical analysis', u'Lexicographic breadth-first search', u'Liang\u2013Barsky', u'Library sort', u'Linde\u2013Buzo\u2013Gray algorithm', u'Line clipping', u'Line drawing algorithm', u'Line search', u'Line segment intersection', u'Linear-time', u'Linear classifier', u'Linear congruential generator', u'Linear feedback shift register', u'Linear interpolation', u'Linear multistep method', u'Linear predictive coding', u'Linear programming', u'Linear search', u'Linear time', u'List of algorithm general topics', u'List of algorithms', u'List of data structures', u'List of machine learning algorithms', u'List of numerical analysis topics', u'List of terms relating to algorithms and data structures', u'List scheduling', u""Lloyd's algorithm"", u'Local search (optimization)', u'Locality-sensitive hashing', u'Logistic regression', u'LogitBoost', u'Long division', u'Longest common subsequence problem', u'Longest common substring problem', u'Longest increasing subsequence problem', u'Longest path problem', u'Longitudinal redundancy check', u'Look-ahead LR parser', u'Lossless data compression', u'Lowest common ancestor', u'Lucas primality test', u'Luhn algorithm', u'Luhn mod N algorithm', u'Lule\xe5 algorithm', u'MD5', u'MISER algorithm', u'Machine Learning', u'Machine learning', u""Maekawa's Algorithm"", u'Magnitude (mathematics)', u'Manning Criteria', u'Marching cubes', u'Marching squares', u'Marching tetrahedrons', u'Marching triangles', u'Mark-compact algorithm', u'Mark and sweep', u'Markov decision process', u'Marr\u2013Hildreth algorithm', u""Marzullo's algorithm"", u'Masaru Tomita', u'Match Rating Approach', u'Mathematical constant', u'Mathematical optimization', u'Matrix multiplication', u'Matrix multiplication algorithm', u'Maximal clique', u'Maximum a posteriori', u'Maximum cardinality matching', u'Maximum flow', u'Maximum flow problem', u'Maximum likelihood', u'Maximum parsimony (phylogenetics)', u'Medical algorithms', u'Medical imaging', u'Medoid', u'Memetic algorithm', u'Merge algorithm', u'Merge sort', u'Mersenne twister', u'Metaheuristic', u'Metaphone', u'Methods of computing square roots', u'Metric space', u'Metropolis light transport', u'Metropolis\u2013Hastings algorithm', u'Microcanonical ensemble', u'Midpoint circle algorithm', u'Miller\u2013Rabin primality test', u'Min conflicts algorithm', u'Minimax', u'Minimum bounding box', u'Minimum bounding box algorithms', u'Minimum cut', u'Minimum degree algorithm', u'Minimum spanning tree', u'Modular arithmetic', u'Monotone cubic interpolation', u'Monte Carlo method', u'Monte Carlo simulation', u'Montgomery reduction', u'Mu-law algorithm', u""Muller's method"", u'Multi level feedback queue', u'Multigrid methods', u'Multiplication algorithm', u'Multiplicative inverse', u'Multivariate division algorithm', u'Multivariate interpolation', u'Mutual exclusion', u'N-body problem', u'NIST', u'NTRUEncrypt', u""Nagle's algorithm"", u""Naimi-Trehel's log(n) Algorithm"", u'Natural language processing', u'Nearest-neighbor interpolation', u'Nearest neighbor search', u'Nearest neighbour algorithm', u'Needleman\u2013Wunsch algorithm', u'Nelder\u2013Mead method', u'Nested loop join', u'Nested sampling algorithm', u'Network congestion', u'Network scheduler', u'Network theory', u'Neural network', u""Neville's algorithm"", u'New York State Identification and Intelligence System', u""Newell's algorithm"", u""Newton's method"", u""Newton's method in optimization"", u'Newton\u2013Raphson division', u'Nicholl\u2013Lee\u2013Nicholl', u'Non-deterministic algorithm', u'Non-restoring division', u'Nonblocking Minimal Spanning Switch', u'Nondeterministic algorithm', u'Nonlinear optimization', u'Normal mapping', u'Nth root algorithm', u'Number theory', u'Numerical analysis', u'Numerical integration', u'Numerical linear algebra', u'OPTICS algorithm', u'Odd-even sort', u'Odds algorithm', u'Odlyzko\u2013Sch\xf6nhage algorithm', u'One-attribute rule', u'Online algorithm', u'Operating systems', u'Operator-precedence parser', u'Optimal substructure', u'Ordered dithering', u'Ordered subset expectation maximization', u'Overlapping subproblem', u'PPM compression algorithm', u'Package-merge algorithm', u'Packrat parser', u'PageRank', u'Page replacement algorithms', u""Painter's algorithm"", u'Pancake sorting', u'Pareto distribution', u'Pareto interpolation', u'Parity bit', u'Parsing', u'Parsing expression grammar', u'Partial differential equation', u'Partial least squares regression', u'Partial order', u'Partial ordering', u'Particle swarm optimization', u'Path-based strong component algorithm', u'Path tracing', u'Patience sorting', u'Paxos algorithm', u'Pearson hashing', u""Pell's equation"", u'Perceptron', u'Perfect matching', u'Permutation group', u'Permutations', u""Peterson's algorithm"", u'Peterson\u2013Gorenstein\u2013Zierler algorithm', u""Petrick's method"", u'Phonetic algorithm', u'Phong shading', u'Photon mapping', u'Pi', u'Pigeonhole sort', u'Pohlig\u2013Hellman algorithm', u'Point cloud', u'Point in polygon', u'Point set registration', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polygon triangulation', u'Polynomial', u'Polynomial interpolation', u'Polynomial long division', u'Polynomial time', u'Positron emission tomography', u'Postman sort', u'Power iteration', u'Powerset construction', u'Pratt parser', u""Prim's algorithm"", u'Primality test', u'Prime-factor FFT algorithm', u'Prime factorization algorithm', u'Prime number', u'Priority queue', u'Probabilistic context-free grammar', u'Probability distribution', u'Process scheduler', u'Process synchronization', u'Programming language theory', u'Pr\xfcfer sequence', u'Pseudorandom number generator', u'Pulmonary embolism', u'Pulse-coupled neural networks', u'Push\u2013relabel algorithm', u'Q-learning', u'QR algorithm', u'Quadratic sieve', u'Quantum algorithm', u'Quasitriangulation', u'Queuing theory', u'Quickhull', u'Quicksort', u'Quine\u2013McCluskey algorithm', u'RANSAC', u'RC4 (cipher)', u'RIPEMD-160', u'RMSD', u'RSA (algorithm)', u'RTR0', u'Rabin\u2013Karp string search algorithm', u""Rader's FFT algorithm"", u'Radial basis function network', u'Radiosity (3D computer graphics)', u'Radix sort', u'Radon transform', u'Rainflow-counting algorithm', u'Ramer\u2013Douglas\u2013Peucker algorithm', u'Random-restart hill climbing', u'Random forest', u'Random walker algorithm', u'Range encoding', u'Rate-monotonic scheduling', u'Ray tracing (graphics)', u'Rayleigh quotient iteration', u""Raymond's Algorithm"", u'Recurrent neural network', u'Recursive descent parser', u'Redundancy check', u'Reed\u2013Solomon error correction', u'Reference counting', u'Region growing', u'Reinforcement Learning', u'Relevance Vector Machine', u'Restoring division', u'Rete algorithm', u'Reverse-delete algorithm', u'Rewriting', u'Ricart-Agrawala Algorithm', u'Rice coding', u'Richardson\u2013Lucy deconvolution', u""Ridder's method"", u'Riemann zeta function', u'Riemersma dithering', u'Rijndael', u'Risch algorithm', u'Ritz method', u'Root-finding algorithm', u'Rotating calipers', u'Round-robin scheduling', u'Rounding functions', u'Run-length encoding', u""Runge's phenomenon"", u'Runge\u2013Kutta methods', u""Ruppert's algorithm"", u'SEQUITUR algorithm', u'SHA-1', u'SHA-2', u'SRT division', u'SSS*', u'SUBCLU', u'SURF', u'SURF (Speeded Up Robust Features)', u'Samplesort', u'Scale-invariant feature transform', u'Scanline rendering', u'Scheduling (computing)', u'Schensted algorithm', u'Schreier\u2013Sims algorithm', u'Sch\xf6nhage\u2013Strassen algorithm', u'Scoring algorithm', u'Seam carving', u'Secant method', u'Secret sharing', u'Seek time', u'Segmentation (image processing)', u'Selection algorithm', u'Selection sort', u'Self-organizing map', u'Semi-space collector', u'Sequence alignment', u'Sequence assembly', u'Sequences', u'Set (mathematics)', u'Set Partitioning in Hierarchical Trees', u'Sethi-Ullman algorithm', u'Shading', u""Shamir's Secret Sharing"", u'Shamos\u2013Hoey algorithm', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Shellsort', u'Shifting nth-root algorithm', u'Shoelace algorithm', u""Shor's algorithm"", u'Shortest common supersequence', u'Shortest job next', u'Shortest path problem', u'Shortest remaining time', u'Shortest seek first', u'Shunting yard algorithm', u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Signal processing', u""Simon's algorithm"", u'Simple LR parser', u'Simple precedence parser', u'Simplex algorithm', u'Simulated annealing', u'Single-linkage clustering', u'Single photon emission computed tomography', u'Slerp', u'Smith\u2013Waterman algorithm', u'Smoothsort', u'Snapshot algorithm', u'Software engineering', u'Sort-Merge Join', u'Sorted list', u'Sorting algorithm', u'Sorting algorithms', u'Sorting by signed reversals', u'Soundex', u'Spaghetti sort', u'Sparse matrix', u'Special functions', u'Special number field sieve', u'Spectral envelope', u'Spectral layout', u'Speech encoding', u'Spigot algorithm', u'Spline interpolation', u'Stable marriage problem', u'State-Action-Reward-State-Action', u'State space search', u'Statistical classification', u'Steinhaus\u2013Johnson\u2013Trotter algorithm', u'Stemming', u'Stochastic tunneling', u'Stochastic universal sampling', u""Stone's method"", u'Stooge sort', u'Strand sort', u'Strassen algorithm', u'Stress (physics)', u'String metrics', u'Strong generating set', u'Strongly connected components', u'Strongly polynomial', u'Structured SVM', u'Sublinear', u'Subsequence', u'Subset sum problem', u'Substring', u'Substring search', u'Successive over-relaxation', u'Suffix tree', u""Sukhotin's algorithm"", u'Summed area table', u'Supervised learning', u'Support Vector Machines', u'Sutherland\u2013Hodgman', u'Swarm intelligence', u'Sweep and prune', u'Sweep line algorithm', u'Symbolic Cholesky decomposition', u'Symmetric key algorithm', u'Symmetric matrices', u'System of linear equations', u'Tabu search', u""Tarjan's off-line least common ancestors algorithm"", u""Tarjan's strongly connected components algorithm"", u'Tarski\u2013Kuratowski algorithm', u'Telephone exchange', u'Temporal difference learning', u'Ternary search', u'Texas Medication Algorithm Project', u'Theory of computation', u'Threefish', u'Tiger (hash)', u'Timsort', u'Tiny Encryption Algorithm', u'Todd\u2013Coxeter algorithm', u'Toeplitz matrix', u'Tomasulo algorithm', u'Toom\u2013Cook multiplication', u'Top-down parsing', u'Top-nodes algorithm', u'Topics in cryptography', u'Topological sorting', u'Tournament selection', u'Transaction (database)', u'Transform coding', u'Transitive closure', u'Transposition table', u'Trapezoidal rule (differential equations)', u'Traveling salesman problem', u'Tree sort', u'Tree traversal', u'Trial division', u'Triangulation (geometry)', u'Tricubic interpolation', u'Tridiagonal matrix algorithm', u'Trie', u'Trigonometric interpolation', u'Trigram search', u'Truncated binary encoding', u'Truncated binary exponential backoff', u'Truncation selection', u'TrustRank', u'Twofish', u'UPGMA', u""Ukkonen's algorithm"", u'Unary coding', u'Unicode Collation Algorithm', u'Uniform-cost search', u'Uniform binary search', u'Universal code (data compression)', u'VEGAS algorithm', u'Variational method', u'Vatti clipping algorithm', u'Vector clocks', u'Vector optimization', u'Vector quantization', u'Velvet (algorithm)', u'Verhoeff algorithm', u'Verlet integration', u'Video compression', u""Vincenty's formulae"", u'Visual cortex', u'Viterbi algorithm', u'Voronoi diagram', u'WHIRLPOOL', u'Wang and Landau algorithm', u""Ward's method"", u'Warnock algorithm', u'Warped Linear Predictive Coding', u'Watershed (algorithm)', u'Wavelet compression', u'Weiler\u2013Atherton', u'Winnow algorithm', u'X-ray', u'X-ray crystallography', u""Xiaolin Wu's line algorithm"", u'Xor swap algorithm', u'Yamartino method', u'Yarrow algorithm', u'Young tableaux', u""Zeller's congruence"", u'Zhu\u2013Takaoka string matching algorithm', u'Ziggurat algorithm', u'Zobrist hashing']"
List of terms relating to algorithms and data structures,"The NIST Dictionary of Algorithms and Data Structures is a reference work maintained by the U.S. National Institute of Standards and Technology. It defines a large number of terms relating to algorithms and data structures. For algorithms and data structures not necessarily mentioned here, see list of algorithms and list of data structures.
This list of terms was originally derived from the index of that document, and is in the public domain, as it was compiled by a Federal Government employee as part of a Federal Government work. Some of the terms defined are:

","[u'Algorithms and data structures', u'Lists of computer terms', u'Mathematics-related lists']","[u'(a,b)-tree', u'0-ary function', u'0-based indexing', u'0/1 knapsack problem', u'2-3-4 tree', u'2-3 tree', u'8 queens', u'AVL tree', u'Absolute performance guarantee', u'Abstract data type', u'Accepting state', u""Ackermann's function"", u'Active data structure', u'Acyclic directed graph', u'Adaptive Huffman coding', u'Adaptive heap sort', u'Adaptive k-d tree', u'Adaptive sort', u'Address-calculation sort', u'Adjacency-list representation', u'Adjacency-matrix representation', u'Adversary (online algorithm)', u'Algorithm', u'Algorithm BSTW', u'Algorithm FGK', u'Algorithm V', u'Algorithmic efficiency', u'Algorithmically solvable', u'All pairs shortest path', u'Alpha Skip Search algorithm', u'Alphabet (computer science)', u'Alternating Turing machine', u'Alternating path', u'Alternation (complexity)', u'American National Standards Institute', u'American flag sort', u'Amortized cost', u'Ancestor', u'Antichain', u'Antisymmetric relation', u'Apostolico\u2013Crochemore', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Approximation algorithm', u'Arborescence (graph theory)', u'Arithmetic coding', u'Arithmetic progression', u'Array data structure', u'Array index', u'Array merging', u'Array search', u'Articulation point', u'Assignment problem', u'Association list', u'Associative', u'Associative array', u'Asymptotic bound', u'Asymptotic lower bound', u'Asymptotic space complexity', u'Asymptotic time complexity', u'Asymptotic upper bound', u'Asymptotically tight bound', u'Augmenting path', u'Automata theory', u'Average-case cost', u'Average case', u'Axiomatic semantics', u'B*-tree', u'B+ tree', u'B-tree', u'BANG file', u'BB alpha tree', u'BD-tree', u'BPP (complexity)', u'BSP-tree', u'Backtracking', u'Baillie-PSW primality test', u'Balanced binary search tree', u'Balanced binary tree', u'Balanced k-way merge sort', u'Balanced merge sort', u'Balanced multiway merge', u'Balanced multiway tree', u'Balanced quicksort', u'Balanced two-way merge sort', u'Batcher sort', u'Baum Welch algorithm', u'Bellman\u2013Ford algorithm', u""Benford's law"", u'Best-case cost', u'Best-first search', u'Best case', u'Biconnected component', u'Biconnected graph', u'Bidirectional bubble sort', u'Big-O notation', u'Bin packing problem', u'Bin sort', u'Binary GCD algorithm', u'Binary decision diagram', u'Binary function', u'Binary heap', u'Binary insertion sort', u'Binary knapsack problem', u'Binary priority queue', u'Binary relation', u'Binary search', u'Binary search tree', u'Binary tree', u'Binary tree representation of trees', u'Bingo sort', u'Binomial heap', u'Binomial tree', u'Bipartite graph', u'Bipartite matching', u'Bisection method', u'Bit vector', u'Bitonic sort', u'Bk tree', u'Block (programming)', u'Block addressing index', u'Block search', u'Blocking flow', u'Bloom filter', u'Blossom (graph theory)', u'Bogosort', u'Boogol', u'Boolean datatype', u'Boolean expression', u'Boolean function', u'Bottleneck traveling salesman', u'Bottom-up tree automaton', u'Boundary-based representation', u'Bounded error probability in polynomial time', u'Bounded queue', u'Bounded stack', u'Bounding volume hierarchy', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Bozo sort', u""Bradford's law"", u'Branch (computer science)', u'Branch and bound', u'Branching (software)', u'Breadth-first search', u""Bresenham's algorithm"", u'Brick sort', u'Bridge (graph theory)', u'British Museum algorithm', u'Brute force attack', u'Brute force search', u'Brute force string search', u'Brute force string search with mismatches', u'Bubble sort', u'Bucket (computing)', u'Bucket array', u'Bucket sort', u'Bucket trie', u'Bucketing method', u'Buddy memory allocation', u'Buddy tree', u'Build-heap', u'Burrows\u2013Wheeler transform', u'Busy beaver', u'Byzantine generals', u'CRCW', u'C curve', u'Cactus stack', u'Calculus of Communicating Systems', u'Calendar queue', u'Candidate consistency testing', u'Candidate verification', u'Canonical complexity class', u'Capacitated facility location', u'Capacity constraint', u'Cartesian tree', u'Cascade merge sort', u'Caverphone', u'Cayley\u2013Purser algorithm', u'Cell probe model', u'Cell tree', u'Cellular automaton', u'Centroid', u'Chain (order theory)', u'Chaining (algorithm)', u'Child node', u'Chinese postman problem', u'Chinese remainder theorem', u'Christofides algorithm', u'Christofides heuristic', u'Chromatic index', u'Chromatic number', u'Church\u2013Turing thesis', u'Circuit complexity', u'Circuit value problem', u'Circular list', u'Circular queue', u'Clique (graph theory)', u'Clique problem', u'Clustering free', u'Co-NP', u'Coalesced hashing', u'Coarsening', u'Cocktail shaker sort', u'Codeword', u'Coding tree', u'Collective recursion', u'Collision resolution scheme', u'Colussi', u'Comb sort', u'Combination', u'Communicating Sequential Processes', u'Communicating sequential processes', u'Commutative', u'Compact DAWG', u'Compact trie', u'Comparison sort', u'Competitive analysis (online algorithm)', u'Competitive ratio', u'Complement (disambiguation)', u'Complete binary tree', u'Complete graph', u'Complete tree', u'Completely connected graph', u'Complexity', u'Complexity class', u'Computable', u'Computational tree logic', u'Computer configuration', u'Computer keyboard keys', u'Concave function', u'Concurrent flow', u'Concurrent read, concurrent write', u'Concurrent read, exclusive write', u'Confluently persistent data structure', u'Connected component (graph theory)', u'Connected graph', u'Constant function', u'Constraint satisfaction problem', u'Continuous knapsack problem', u""Cook's theorem"", u'Cook reduction', u'Counting sort', u'Cover (set theory)', u'Crew (algorithm)', u'Critical path problem', u'Cuckoo hashing', u'Cut (graph theory)', u'Cut (logic programming)', u'Cut vertex', u'Cutting plane', u'Cutting stock problem', u'Cutting theorem', u'Cycle sort', u'Cyclic redundancy check', u'D-adjacent', u'DAG shortest paths', u'DFS forest', u'DFTA', u'Damerau\u2013Levenshtein distance', u'Data domain', u'Data structure', u'Decidability (logic)', u'Decidable language', u'Decimation (signal processing)', u'Decision problem', u'Decision tree', u'Decomposable searching problem', u'Degree (disambiguation)', u'Dense graph', u'Depoissonization', u'Depth-first search', u'Depth-limited search', u'Deque', u'Derangement', u'Deterministic algorithm', u'Deterministic finite automata string search', u'Deterministic finite automaton', u'Deterministic finite state machine', u'Deterministic finite tree automaton', u'Deterministic pushdown automaton', u'Deterministic tree automaton', u'Deutsch\u2013Jozsa algorithm', u'Diagonalization argument', u'Diameter', u'Dichotomic search', u'Dictionary', u'Difference (set theory)', u'Digital circuit', u'Digital search tree', u'Digital tree', u""Dijkstra's algorithm"", u'Diminishing increment sort', u'Dining philosophers', u'Direct chaining hashing', u'Directed acyclic graph', u'Directed acyclic word graph', u'Directed graph', u'Discrete Fourier transform', u'Discrete interval encoding tree', u'Discrete p-center', u'Disjoint set', u'Disjunction', u'Distributed algorithm', u'Distribution sort', u'Distributional complexity', u'Divide and conquer algorithm', u'Divide and marriage before conquest', u'Division method', u""Don't care"", u'Doomsday rule', u'Double-direction bubble sort', u'Double-ended priority queue', u'Double Metaphone', u'Double hashing', u'Double left rotation', u'Double right rotation', u'Doubly chained tree', u'Doubly ended queue', u'Doubly linked list', u'Dragon curve', u'Dual graph', u'Dual linear program', u'Dutch national flag', u'Dyadic tree', u'Dynamic array', u'Dynamic data structure', u'Dynamic hashing', u'Dynamic programming', u'Dynamization transformation', u'Edge-weighted graph', u'Edge coloring', u'Edge connectivity', u'Edge crossing', u'Edit distance', u'Edit operation', u'Edit script', u'Elastic-bucket trie', u'Element uniqueness', u'End-of-string', u'Enfilade', u'Epidemic algorithm', u""Euclid's algorithm"", u'Euclidean Steiner tree', u'Euclidean algorithm', u'Euclidean distance', u'Euclidean traveling salesman problem', u'Euler cycle', u'Eulerian graph', u'Eulerian path', u'Exact string matching', u'Exchange sort', u'Exclusive or', u'Exclusive read, concurrent write', u'Exclusive read, exclusive write', u'Exhaustive search', u'Existential state', u'Expandable hashing', u'Expander graph', u'Exponential (disambiguation)', u'Extended Euclidean algorithm', u'Extended binary tree', u'Extended k-d tree', u'Extendible cell', u'Extendible hashing', u'External index', u'External memory algorithm', u'External memory data structure', u'External merge', u'External merge sort', u'External node', u'External quicksort', u'External radix sort', u'External sort', u'Extrapolation search', u'Extremal', u'Extreme point', u'FIFO (computing and electronics)', u'Facility location', u'Factorial', u'Fast fourier transform', u'Fathoming', u'Feasible region', u'Feasible solution', u'Feedback edge set', u'Feedback vertex set', u'Ferguson\u2013Forcade algorithm', u'Fibonacci heap', u'Fibonacci number', u'Fibonacci search', u'Fibonacci tree', u'Filial-heir chain', u'Find', u'Find kth least element', u'Finitary tree', u'Finite-state machine', u'Finite state automaton', u'Finite state machine', u'Finite state machine minimization', u'Finite state transducer', u'First child-next sibling binary tree', u'First come, first served', u'Fixed-grid method', u'Flash sort', u'Flow (mathematics)', u'Flow conservation', u'Flow function', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Forest (graph theory)', u'Forest editing problem', u'Formal language', u'Formal methods', u'Formal verification', u'Forward index', u'Fractal', u'Fractional knapsack problem', u'Fractional solution', u'Free edge', u'Free list', u'Free tree', u'Free vertex', u'Frequency count heuristic', u'Full array', u'Full binary tree', u'Full inverted index', u'Fully dynamic graph problem', u'Fully persistent data structure', u'Fully polynomial approximation scheme', u'Function (mathematics)', u'Function (programming)', u'Functional data structure', u'GBD-tree', u'Galil\u2013Giancarlo', u'Galil\u2013Seiferas', u'Gamma function', u'Geometric optimization problem', u'Global optimum', u'Glossary of graph theory', u'Gnome sort', u'Goobi', u'Graph (data structure)', u'Graph coloring', u'Graph concentration', u'Graph drawing', u'Graph isomorphism', u'Graph partition', u'Gray code', u'Greatest common divisor', u'Greedy algorithm', u'Greedy heuristic', u'Grid drawing', u'Grid file', u""Grover's algorithm"", u'HB-tree', u'Halting problem', u'Hamiltonian cycle', u'Hamiltonian path', u'Hamming distance', u'Hash collision', u'Hash function', u'Hash heap', u'Hash table', u'Hash table delete', u'Hausdorff distance', u'Heap (data structure)', u'Heap property', u'Heapify', u'Heapsort', u'Heaviest common subsequence', u'Height', u'Height-balanced binary search tree', u'Height-balanced tree', u'Heuristic', u'Hidden Markov model', u'Highest common factor', u'Hilbert curve', u'Histogram sort', u'Homeomorphic', u'Horizontal visibility map', u""Horner's rule"", u'Huffman encoding', u'Hungarian algorithm', u'Hybrid algorithm', u'Hyperedge', u'Hypergraph', u'Ideal merge', u'Identity function', u'Implies operator', u'In-branching', u'In-degree', u'In-order traversal', u'In-place sort', u'Inclusion-exclusion principle', u'Inclusive or', u'Incompressible string', u'Incremental algorithm', u'Independent set (graph theory)', u'Indeterminacy in computation (disambiguation)', u'Index file', u'Information theoretic bound', u'Insertion sort', u'Instantaneous description', u'Integer linear program', u'Integer multi-commodity flow', u'Integer polyhedron', u'Interactive proof system', u'Interior-based representation', u'Internal node', u'Internal sort', u'Internet Security Association and Key Management Protocol', u'Interpolation-sequential search', u'Interpolation search', u'Interpolation sort', u'Intersection (set theory)', u'Interval tree', u'Intractability (complexity)', u'Introsort', u'Introspective sort', u'Inverse Ackermann function', u'Inverted file index', u'Inverted index', u'Irreflexive', u'Isomorphic', u'Iteration', u'JSort', u'J sort', u'Jaro\u2013Winkler distance', u""Johnson's algorithm"", u'Jump list', u'Jump search', u'K-ary Huffman encoding', u'K-ary heap', u'K-ary tree', u'K-clustering', u'K-coloring', u'K-connected graph', u'K-d-B-tree', u'K-d tree', u'K-dimensional', u'K-dominant match', u'K-way merge', u'K-way merge sort', u'K-way tree', u'KV diagram', u""Karmarkar's algorithm"", u'Karnaugh map', u'Karp reduction', u'KmpSkip Search', u'Knapsack problem', u""Knight's tour"", u'Knuth\u2013Morris\u2013Pratt algorithm', u'Kolmogorov complexity', u""Kraft's inequality"", u'Kripke structure', u""Kruskal's algorithm"", u'Kth order Fibonacci numbers', u'Kth shortest path', u'Kth smallest element', u'K\xf6nigsberg bridges problem', u'L-reduction', u'LIFO (computing)', u'Labeled graph', u'Laboratory for Computer Science', u'Language', u'Las Vegas algorithm', u'Lattice (group)', u'Layered graph', u'Leaf', u'Leaf node', u'Least common multiple', u'Left rotation', u'Leftist tree', u'Lempel\u2013Ziv\u2013Welch', u'Level-order traversal', u'Levenshtein distance', u'Lexicographical order', u'Linear', u'Linear congruential generator', u'Linear hash', u'Linear insertion sort', u'Linear order', u'Linear probing', u'Linear probing sort', u'Linear product', u'Linear program', u'Linear quadtree', u'Linear search', u'Linked list', u'List (computing)', u'List contraction', u'List of algorithms', u'List of data structures', u'Little-o notation', u'Lm distance', u'Load factor (computer science)', u'Local alignment', u'Local optimum', u'Logarithm', u'Logarithmic scale', u'Logical conjunction', u'Logical disjunction', u'Logical nand', u'Logical nor', u'Longest common subsequence', u'Longest common substring', u""Lotka's law"", u'Lower bound', u'Lower triangular matrix', u'Lowest common ancestor', u'MAX-SNP', u'MODIFIND', u'Malhotra\u2013Kumar\u2013Maheshwari blocking flow', u'Manhattan distance', u'Many-one reduction', u'Markov chain', u'Master theorem', u'Matched edge', u'Matched vertex', u'Matching (graph theory)', u'Material conditional', u'Matrix-chain multiplication problem', u'Matrix (math)', u'Max-heap property', u'Maximal Shift', u'Maximal independent set', u'Maximally connected component', u'Maximum-flow problem', u'Maximum bipartite matching', u'Mealy machine', u'Mean', u'Median', u'Meld (data structures)', u'Membership function (mathematics)', u'Memoization', u'Memory segment', u'Merge algorithm', u'Merge sort', u'Meromorphic function', u'Metaheuristic', u'Metaphone', u'Midrange', u'Miller\u2013Rabin primality test', u'Min-heap property', u'Minimal perfect hashing', u'Minimum bounding box', u'Minimum cut', u'Minimum path cover', u'Minimum spanning tree', u'Minimum vertex cut', u'Mixed integer linear program', u'Mode (statistics)', u'Model checking', u'Model of computation', u'Moderately exponential', u'Monotone priority queue', u'Monotonically decreasing', u'Monotonically increasing', u'Monte Carlo algorithm', u'Moore machine', u'Morris-Pratt', u'Move-to-front heuristic', u'Move-to-root heuristic', u'Multi-commodity flow', u'Multi suffix tree', u'Multigraph', u'Multilayer grid file', u'Multiplication method', u'Multiprefix', u'Multiprocessor model', u'Multiset', u'Multiway decision', u'Multiway merge', u'Multiway search tree', u'Multiway tree', u""Munkres' assignment algorithm"", u'N-ary function', u'NC (complexity)', u'NC many-one reducibility', u'NIST', u'NIST Dictionary of Algorithms and Data Structures', u'NP-complete', u'NP-complete language', u'NP-hard', u'NP (complexity)', u'N queens', u'Naive string search', u'National Institute of Standards and Technology', u'Nearest neighbor search', u'Negation', u'Network flow problem', u'New York State Identification and Intelligence System', u'Next state', u'Node (computer science)', u'Nonbalanced merge', u'Nonbalanced merge sort', u'Nondeterministic Turing machine', u'Nondeterministic algorithm', u'Nondeterministic finite automaton', u'Nondeterministic finite state machine', u'Nondeterministic finite tree automaton', u'Nondeterministic polynomial time', u'Nondeterministic tree automaton', u'Nonterminal node', u'Not So Naive', u'Null tree', u'Nullary function', u'Numerical stability', u'Objective function', u'Occurrence', u'Octree', u'Offline algorithm', u'Offset (computer science)', u'Omega', u'Omicron', u'One-based indexing', u'One-dimensional', u'Online algorithm', u'Open addressing', u'Optimal cost', u'Optimal hashing', u'Optimal merge', u'Optimal mismatch', u'Optimal polygon triangulation problem', u'Optimal polyphase merge', u'Optimal polyphase merge sort', u'Optimal solution', u'Optimal triangulation problem', u'Optimal value', u'Optimization (mathematics)', u'Oracle Turing machine', u'Oracle set', u'Oracle tape', u'Order preserving hash', u'Order preserving minimal perfect hashing', u'Ordered array', u'Ordered binary decision diagram', u'Ordered linked list', u'Ordered tree', u'Orders of approximation', u'Oriented acyclic graph', u'Oriented graph', u'Oriented tree', u'Orthogonal drawing', u'Orthogonal lists', u'Orthogonally convex rectilinear polygon', u'Oscillating merge sort', u'Out-branching', u'Out-degree', u'Overlapping subproblems', u'P-complete', u'P-tree', u'P-way merge sort', u'PLOP-hashing', u'PRNG', u'Padding argument', u'Pagoda (data structure)', u'Pairing heap', u'Parallel Random Access Machine', u'Parallel computation thesis', u'Parallel prefix computation', u'Parametric searching', u'Parent', u'Partial function', u'Partial order', u'Partial recursive function', u'Partially decidable problem', u'Partially dynamic graph problem', u'Partially ordered set', u'Partially persistent data structure', u'Partition (set theory)', u'Passive data structure', u'Path (graph theory)', u'Path cover', u'Path system problem', u'Patience sorting', u'Patricia tree', u'Pattern', u'Pattern element', u'Peano curve', u""Pearson's hash"", u'Perfect binary tree', u'Perfect hashing', u'Perfect k-ary tree', u'Perfect matching', u'Performance guarantee', u'Performance ratio', u'Permutation', u'Persistent data structure', u'Phencyclidine', u'Phonetic coding', u'Pile (data structure)', u'Pipelined divide and conquer', u'Planar graph', u'Planar straight-line graph', u'Planarization', u'Point access method', u'Pointer jumping', u'Pointer machine', u'Poissonization', u'Polychotomy', u'Polyhedron', u'Polylogarithmic', u'Polynomial', u'Polynomial-time approximation scheme', u'Polynomial-time reduction', u'Polynomial hierarchy', u'Polynomial time', u'Polyphase merge', u'Polyphase merge sort', u'Polytope', u'Poset', u""Post's correspondence problem"", u'Postfix traversal', u""Postman's sort"", u'Postorder traversal', u'Post\u2013Turing machine', u'Potential method', u'Predicate (computer programming)', u'Prefix (computer science)', u'Prefix code', u'Prefix computation', u'Prefix sum', u'Prefix traversal', u'Preorder traversal', u""Prim's algorithm"", u'Primary clustering', u'Primitive recursive', u'Principle of optimality', u'Priority queue', u""Prisoner's dilemma"", u'Probabilistic Turing machine', u'Probabilistic algorithm', u'Probabilistically checkable proof', u'Probe sequence', u'Procedure (computer science)', u'Process algebra', u'Proper binary tree', u'Proper coloring', u'Proper subset', u'Property list', u'Prune and search', u'Pseudo-random number generator', u'Pth order Fibonacci numbers', u'Public key certificate', u'Purely functional language', u'Pushdown automaton', u'Pushdown transducer', u'Q sort', u'Qm sort', u'Quad trie', u'Quadratic probing', u'Quadtree', u'Quadtree complexity theorem', u'Quantum computation', u'Queue (data structure)', u'Quick search', u'Quicksort', u'R* tree', u'R+ tree', u'R-file', u'R-tree', u'RP (complexity)', u'Rabin\u2013Karp string search algorithm', u'Radix quicksort', u'Radix sort', u'Ragged matrix', u'Raita algorithm', u'Random access machine', u'Random number generation', u'Random number generator', u'Random sampling', u'Randomization', u'Randomized-Select', u'Randomized algorithm', u'Randomized binary search tree', u'Randomized complexity', u'Randomized polynomial time', u'Randomized rounding', u'Randomized search tree', u'Range (function)', u'Range sort', u'Rank (graph theory)', u'Ratcliff/Obershelp pattern recognition', u'Reachable', u'Rebalance', u'Recognizer', u'Rectangular matrix', u'Rectilinear Steiner tree', u'Recurrence equations', u'Recurrence relation', u'Recursion', u'Recursion termination', u'Recursion tree', u'Recursive (computer science)', u'Recursive data structure', u'Recursive doubling', u'Recursive language', u'Recursively enumerable language', u'Recursively solvable', u'Red-black tree', u'Reduced basis', u'Reduced digraph', u'Reduced ordered binary decision diagram', u'Reduction (complexity)', u'Reference (computer science)', u'Reflexive relation', u'Regular decomposition', u'Rehashing', u'Relation (mathematics)', u'Relational structure', u'Relative performance guarantee', u'Relaxation technique (mathematics)', u'Relaxed balance', u'Rescalable', u'Restricted universe sort', u'Result cache', u'Reverse Colussi', u'Reverse Factor', u""Rice's method"", u'Right-threaded tree', u'Right rotation', u'Root', u'Root balance', u'Rooted tree', u'Rotate left', u'Rotate right', u'Rotation', u'Rough graph', u'Run time (program lifecycle phase)', u'S-t cut', u'SBB tree', u'SPMD', u'Saguaro stack', u'Saturated edge', u'Scapegoat tree', u'Search algorithm', u'Search tree', u'Search tree property', u'Secant search', u'Secondary clustering', u'Select algorithm', u'Select and partition', u'Select kth element', u'Select mode', u'Selection problem', u'Selection sort', u'Self-balancing binary search tree', u'Self-loop', u'Self-organizing heuristic', u'Self-organizing list', u'Self-organizing sequential search', u'Semidefinite programming', u'Separate chaining hashing', u'Sepraation theorem (disambiguation)', u'Sequential search', u'Set (computer science)', u'Set cover', u'Set packing', u'Shadow heap', u'Shadow merge', u'Shadow merge insert', u'Shaker sort', u'Shannon\u2013Fano coding', u'Shared memory (interprocess communication)', u'Shell sort', u'Shift-Or', u""Shor's algorithm"", u'Shortcutting', u'Shortest common supersequence', u'Shortest common superstring', u'Shortest path', u'Shortest spanning tree', u'Shuffle', u'Shuffle sort', u'Shuffling', u'Sibling', u'Sierpinski triangle', u'Sierpi\u0144ski curve', u'Sieve of Eratosthenes', u'Sift up', u'Signature', u""Simon's algorithm"", u'Simple merge', u'Simple uniform hashing', u'Simplex communication', u'Simulated annealing', u'Simulation theorem', u'Single-destination shortest-path problem', u'Single-pair shortest-path problem', u'Single-source shortest-path problem', u'Single program multiple data', u'Singly linked list', u'Singularity analysis', u'Sink', u'Sinking sort', u'Skd-tree', u'Skew symmetry', u'Skip list', u'Skip search', u'Slope selection', u'Smith algorithm', u'Smith\u2013Waterman algorithm', u'Smoothsort', u'Solvable problem', u'Sort algorithm', u'Sort in place', u'Sort merge', u'Sorted array', u'Sorted list', u'Soundex', u'Space-constructible function', u'Spanning tree (mathematics)', u'Sparse graph', u'Sparse matrix', u'Sparsification', u'Sparsity', u'Spatial index', u'Spectral test', u'Splay tree', u'Square matrix', u'Square root', u'St-digraph', u'Stack (data structure)', u'Stack tree', u'Star-shaped polygon', u'Start state', u'State (computer science)', u'State machine', u'State transition', u'Static Huffman encoding', u'Static data structure', u'Steiner minimum tree', u'Steiner ratio', u'Steiner tree', u'Steiner tree problem', u'Steiner vertex', u'Steinhaus\u2013Johnson\u2013Trotter algorithm', u""Stirling's approximation"", u""Stirling's formula"", u'Stooge sort', u'Straight-line drawing', u'Strand sort', u'Strictly decreasing', u'Strictly increasing', u'Strictly lower triangular matrix', u'Strictly upper triangular matrix', u'String (computer science)', u'String editing problem', u'String matching', u'String matching on ordered alphabets', u'String matching with errors', u'String matching with mismatches', u'String searching', u'Strip packing', u'Strongly NP-hard', u'Strongly connected component', u'Strongly connected graph', u'Subadditive ergodic theorem', u'Subgraph isomorphism', u'Sublinear time algorithm', u'Subsequence', u'Subset', u'Substring', u'Subtree', u'Suffix (computer science)', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Superimposed code', u'Superset', u'Supersink', u'Supersource', u'Symmetric binary B-tree', u'Symmetric min max heap', u'Symmetric relation', u'Symmetric set difference', u'Symmetrically linked list', u'Symmetry breaking', u'Tail', u'Tail recursion', u'Target (CS)', u'Temporal logic', u'Ternary search', u'Ternary search tree', u'Text searching', u'Theta', u'Threaded binary tree', u'Threaded tree', u'Three-dimensional space', u'Three-way merge sort', u'Three-way radix quicksort', u'Time-constructible function', u'Time/space complexity', u'Top-down radix sort', u'Top-down tree automaton', u'Top-nodes algorithm', u'Topological order', u'Topological sort', u'Topology tree', u'Total function', u'Total order', u'Totally decidable language', u'Totally decidable problem', u'Totally undecidable problem', u'Tournament (graph theory)', u'Towers of Hanoi', u'Tractable problem', u'Transducer', u'Transitive closure', u'Transitive reduction', u'Transitive relation', u'Transpose sequential search', u'Travelling salesman problem', u'Treap', u'Tree automaton', u'Tree contraction', u'Tree data structure', u'Tree editing problem', u'Tree sort', u'Tree structure', u'Tree transducer', u'Tree traversal', u'Triangle inequality', u'Triconnected graph', u'Trie', u'Trinary function', u'Tripartition', u'Turbo-BM', u'Turbo Reverse Factor', u'Turing machine', u'Turing reduction', u'Turing transducer', u'Twin grid file', u'Two-dimensional', u'Two-level grid file', u'Two-way linked list', u'Two-way merge sort', u'Two Way algorithm', u'Unary function', u'Unbounded knapsack problem', u'Uncomputable function', u'Uncomputable problem', u'Undecidable language', u'Undecidable problem', u'Undirected graph', u'Uniform circuit complexity', u'Uniform circuit family', u'Uniform hashing', u'Uniform matrix', u'Union (computer science)', u'Union of automata', u'Universal Turing machine', u'Universal hashing', u'Universal state (Turing)', u'Universe', u'Unsolvable problem', u'Unsorted list', u'Upper triangular matrix', u'VP-tree', u'Van Emde Boas tree', u'Vehicle routing problem', u'Veitch diagram', u'Venn diagram', u'Vertex (graph theory)', u'Vertex coloring', u'Vertex connectivity', u'Vertex cover', u'Vertical visibility map', u'Virtual hashing', u'Visibility map', u'Visible (geometry)', u'Viterbi algorithm', u'Weak-heap', u'Weak-heap sort', u'Weak cluster', u'Weight-balanced tree', u'Weighted, directed graph', u'Weighted graph', u'Work-depth model', u'Work-efficient', u'Work-preserving', u'Worst-case cost', u'Worst-case minimum access', u'Worst case', u'Xor', u'Yule\u2013Simon distribution', u'ZPP (complexity)', u""Zeller's congruence"", u'Zhu\u2013Takaoka string matching algorithm', u""Zipf's law"", u'Zipfian distribution', u'Zipper (data structure)']"
List scheduling,"The basic idea of list scheduling is to make an ordered list of processes by assigning them some priorities, and then repeatedly execute the following two steps until a valid schedule is obtained :
Select from the list, the process with the highest priority for scheduling.
Select a resource to accommodate this process.
If no resource can be found, we select the next process in the list.
The priorities are determined statically before scheduling process begins. The first step chooses the process with the highest priority, the second step selects the best possible resource. Some known list scheduling strategies are :
Highest level first algorithm or HLF
Longest path algorithm or LP
Longest processing time
Critical path method
Heterogeneous Earliest Finish Time or HEFT. For the case heterogeneous workers.","[u'Pages using citations with accessdate and no URL', u'Scheduling algorithms']","[u'Critical path method', u'Heterogeneous Earliest Finish Time', u'Highest level first', u'International Standard Book Number', u'Longest path', u'Longest processing time']"
Lloyd's algorithm,"In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces, and partitions of these subsets into well-shaped and uniformly sized convex cells. Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition, and then re-partitions the input according to which of these centroids is closest. However, Lloyd's algorithm differs from k-means clustering in that its input is a continuous geometric region rather than a discrete set of points. Thus, when re-partitioning the input, Lloyd's algorithm uses Voronoi diagrams rather than simply determining the nearest center to each of a finite set of points as the k-means algorithm does.
Although the algorithm may be applied most directly to the Euclidean plane, similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input, which can be used for quantization, dithering, and stippling. Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method.","[u'Geometric algorithms', u'Mathematical optimization']","[u'ACM SIGGRAPH', u'ArXiv', u'Blue noise', u'Centroid', u'Centroidal Voronoi tessellation', u'Colors of noise', u'Computer science', u'Data compression', u'David Eppstein', u'Digital object identifier', u'Dithering', u'Electrical engineering', u'Euclidean plane', u'Euclidean space', u'Eurographics', u'Farthest-first traversal', u'Finite element method', u'IEEE Transactions on Information Theory', u'Information theory', u'K-means clustering', u'Laplacian smoothing', u'Linde\u2013Buzo\u2013Gray algorithm', u'Manhattan metric', u'Matthew T. Dickerson', u'Mean shift', u'Monte Carlo methods', u'Mosaic', u'Non-Euclidean geometry', u'Quantization (signal processing)', u'Stippling', u'Triangle mesh', u'Voronoi diagram']"
Local search (optimization),"In computer science, local search is a metaheuristic method for solving computationally hard optimization problems. Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of candidate solutions. Local search algorithms move from solution to solution in the space of candidate solutions (the search space) by applying local changes, until a solution deemed optimal is found or a time bound is elapsed.
Local search algorithms are widely applied to numerous hard computational problems, including problems from computer science (particularly artificial intelligence), mathematics, operations research, engineering, and bioinformatics. Examples of local search algorithms are WalkSAT and the 2-opt algorithm for the Traveling Salesman Problem.","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from May 2015', u'Optimization algorithms and methods']","[u'2-opt', u'Anytime algorithm', u'Approximation algorithm', u'Artificial intelligence', u'Bioinformatics', u'Boolean satisfiability problem', u'Candidate solution', u'Clause (logic)', u'Combinatorial optimization', u'Computer science', u'Constraint satisfaction', u'Convex programming', u'Cycle (graph theory)', u'Engineering', u'Facility location', u'Graph (mathematics)', u'Hill climbing', u'Holger H. Hoos', u'Hypersphere', u'Incomplete algorithm', u'Infinite-dimensional optimization', u'Integer programming', u'International Standard Book Number', u'Iterated local search', u'Iterative method', u'Juraj Hromkovi\u010d', u'K-medoid', u'Local optimum', u'Luus\u2013Jaakola', u'Machine learning', u'Mathematical optimization', u'Mathematics', u'Metaheuristic', u'Multiobjective optimization', u'Neighborhood relation', u'Neighbourhood (mathematics)', u'Nonlinear programming', u'Normal distribution', u'Nurse scheduling problem', u'Operations research', u'Optimization (mathematics)', u'Pattern search (optimization)', u'Quadratic programming', u'Random optimization', u'Random search', u'Reactive search optimization', u'Real number', u'Robust optimization', u'Shift work', u'Simulated annealing', u'Springer Verlag', u'Stochastic optimization', u'Stochastic programming', u'Tabu search', u'Travelling salesman problem', u'Uniform distribution (continuous)', u'Vertex cover', u'Vertex cover problem', u'Very large-scale neighborhood search', u'WalkSAT']"
LogitBoost,"In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The original paper casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm.

","[u'All stub articles', u'Artificial intelligence stubs', u'Classification algorithms', u'Ensemble learning', u'Machine learning algorithms']","[u'AdaBoost', u'Artificial intelligence', u'Boosting (meta-algorithm)', u'Computational learning theory', u'Convex optimization', u'Generalized additive model', u'Gradient boosting', u'Jerome H. Friedman', u'Logistic model tree', u'Logistic regression', u'Machine learning', u'Robert Tibshirani', u'Trevor Hastie']"
Look-ahead LR parser,"In computer science, an LALR parser or Look-Ahead LR parser is a simplified version of a canonical LR parser, to parse (separate and analyze) a text according to a set of production rules specified by a formal grammar for a computer language. (""LR"" means left-to-right, rightmost derivation.)
The LALR parser was invented by Frank DeRemer in his 1969 PhD dissertation, Practical Translators for LR(k) languages, in his treatment of the practical difficulties at that time of implementing LR(1) parsers. He showed that the LALR parser has more language recognition power than the LR(0) parser, while requiring the same number of states as the LR(0) parser for a language that can be recognized by both parsers. This makes the LALR parser a memory-efficient alternative to the LR(1) parser for languages that are not LR(0). It was also proved that there exist LR(1) languages that are not LALR. Despite this weakness, the power of the LALR parser is enough for many mainstream computer languages, including Java, though the reference grammars for many languages fail to be LALR due to being ambiguous.
The original dissertation gave no algorithm for constructing such a parser given some formal grammar. The first algorithms for LALR parser generation were published in 1973. In 1982, DeRemer and Tom Pennello published an algorithm that generated highly memory-efficient LALR parsers. LALR parsers can be automatically generated from some grammar by an LALR parser generator such as Yacc or GNU Bison. The automatically generated code may be augmented by hand-written code to augment the power of the resulting parser.","[u'All articles with unsourced statements', u'Articles with unsourced statements from December 2012', u'Parsing algorithms', u'Use dmy dates from July 2012']","[u'Ambiguous grammar', u'Backtracking', u'Bottom-up parsing', u'C++ language', u'C language', u'Canonical LR parser', u'Comparison of parser generators', u'Computer language', u'Computer memory', u'Computer science', u'Context-free grammar', u'Deterministic context-free language', u'Digital object identifier', u'Donald Knuth', u'Formal grammar', u'Frank DeRemer', u'GNU Bison', u'Gnu Compiler Collection', u'Initialism', u'Java technology', u'LALR parser generator', u'LL grammar', u'LL parser', u'LR parser', u'Parser generator', u'Parsing', u'Production (computer science)', u'Recursive descent parser', u'Rightmost derivation', u'SLR parser', u'Token (parser)', u'Token scanner', u'Tom Pennello', u'Yacc']"
Lossless data compression,"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).
Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).
Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.

","[u'All articles with unsourced statements', u'Articles with unsourced statements from August 2011', u'Articles with unsourced statements from December 2007', u'Articles with unsourced statements from November 2012', u'Data compression', u'Lossless compression algorithms']","[u'7-Zip', u'7zip', u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive Transform Acoustic Coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Algorithm', u'Algorithmic complexity theory', u'Amiga', u'Apple Lossless', u'Apt-X', u'Arithmetic coding', u'Audio Lossless Coding', u'Audio codec', u'Audio compression (data)', u'Autoregressive', u'Average bitrate', u'Benchmark (computing)', u'Bijection', u'Bit rate', u'Brotli', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Bzip2', u'C10n.info', u'CCM (software)', u'Calgary Corpus', u'Canonical Huffman code', u'Ccmx', u'Chain code', u'Chroma subsampling', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Comp.compression', u'Companding', u'Comparison of file archivers', u'Compress', u'Compression algorithm', u'Compression artifact', u'Constant bitrate', u'Context-mixing', u'Context mixing', u'Context tree weighting', u'Convolution', u'Counting argument', u'Cryptanalysis', u'Cryptosystem', u'DEFLATE', u'DEFLATE (algorithm)', u'DTS-HD Master Audio', u'Data-compression.com', u'Data compression', u'Data compression ratio', u'David A. Huffman', u'Deblocking filter', u'Delta (letter)', u'Delta encoding', u'Demo (computer programming)', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Discrete wavelet transform', u'Display resolution', u'Dolby TrueHD', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Executable compression', u'Exponential-Golomb coding', u'FAQ', u'Fibonacci coding', u'Film frame', u'Flashzip', u'Fourier transform', u'Fractal compression', u'Frame rate', u'FreeArc', u'Free Lossless Audio Codec', u'Function (mathematics)', u'GIF', u'GNU', u'GNU General Public License', u'Genetic algorithm', u'Genetics', u'Golomb coding', u'Grammar induction', u'Graphics Interchange Format', u'Gzip', u'HTTP', u'HapMap', u'Heuristics', u'Huffman coding', u'Hutter Prize', u'ILBM', u'Image compression', u'Image resolution', u'Indexed color', u'Indexed image', u'Information entropy', u'Information theory', u'Interchange File Format', u'Interlaced video', u'Intuition (knowledge)', u'JBIG2', u'JPEG2000', u'JPEG 2000', u'JPEG XR', u'JavaScript', u'Joint (audio engineering)', u'Karhunen\u2013Lo\xe8ve theorem', u'Kilobyte', u'Kolmogorov complexity', u'LAME', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZ77 and LZ78 (algorithms)', u'LZ78', u'LZJB', u'LZMA', u'LZRW', u'LZW', u'LZWL', u'LZX (algorithm)', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'List of codecs', u'Log area ratio', u'Lossless', u'Lossless JPEG', u'Lossless Transform Audio Compression', u'Lossy compression', u'MP3', u'MPEG-4 SLS', u'Macroblock', u'Matt Mahoney (computer scientist)', u'Meridian Lossless Packing', u'Modified Huffman coding', u'Modified discrete cosine transform', u""Monkey's Audio"", u'Motion compensation', u'Move-to-front transform', u'Multiple-image Network Graphics', u'NanoZip', u'Newsgroup', u'Normal number', u'Nyquist\u2013Shannon sampling theorem', u'OpenCTM', u'OptimFROG', u'Original Sound Quality', u'PAQ', u'PKWARE, Inc.', u'PPMd', u'Peak signal-to-noise ratio', u'Pi', u'Pigeonhole principle', u'Pixel', u'Point-to-Point Protocol', u'Portable Network Graphics', u'Precompressor', u'Prediction by partial matching', u'Progressive Graphics File', u'Psychoacoustics', u'PubMed Central', u'PubMed Identifier', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Random', u'Range encoding', u'Rate\u2013distortion theory', u'RealPlayer', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Secure Shell', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Shorten (file format)', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel Ziv', u'Statistical Lempel\u2013Ziv', u'Sub-band coding', u'Super Audio CD', u'TIFF', u'TTA (codec)', u'Tagged Image File Format', u'Timeline of information theory', u'Tunstall coding', u'UTF-8', u'Unary coding', u'Unicity distance', u'United States', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'WavPack', u'Wavelet compression', u'WebP', u'Wikipedia', u'WinRK', u'Windows Media Audio 9 Lossless', u'XML', u'Xz', u'ZIP (file format)', u'\u039c-law algorithm']"
Luhn algorithm,"The Luhn algorithm or Luhn formula, also known as the ""modulus 10"" or ""mod 10"" algorithm, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the US, and Canadian Social Insurance Numbers. It was created by IBM scientist Hans Peter Luhn and described in U.S. Patent No. 2,950,048, filed on January 6, 1954, and granted on August 23, 1960.
The algorithm is in the public domain and is in wide use today. It is specified in ISO/IEC 7812-1. It is not intended to be a cryptographically secure hash function; it was designed to protect against accidental errors, not malicious attacks. Most credit cards and many government identification numbers use the algorithm as a simple method of distinguishing valid numbers from mistyped or otherwise incorrect numbers.","[u'Articles with example Python code', u'Checksum algorithms', u'Error detection and correction', u'Modular arithmetic']","[u'Algorithm', u'Bank card number', u'Canada', u'Check digit', u'Checksum', u'Credit card number', u'Cryptographic hash function', u'Damm algorithm', u'Hans Peter Luhn', u'IBM', u'IMEI', u'ISO/IEC 7812', u'Luhn mod N algorithm', u'Modular arithmetic', u'National Provider Identifier', u'Public domain', u'Python (programming language)', u'Social Insurance Number', u'Verhoeff algorithm']"
Luhn mod N algorithm,"The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) that allows it to work with sequences of non-numeric characters. This can be useful when a check digit is required to validate an identification string composed of letters, a combination of letters and digits or even any arbitrary set of characters.

","[u'All articles lacking sources', u'Articles lacking sources from May 2010', u'Articles with example code', u'Checksum algorithms', u'Modular arithmetic']","[u'Associative array', u'International Securities Identification Number', u'Luhn algorithm']"
Luleå algorithm,"The Luleå algorithm of computer science, designed by Degermark et al. (1997), is a patented technique for storing and searching internet routing tables efficiently. It is named after the Luleå University of Technology, the home institute of the technique's authors. The name of the algorithm does not appear in the original paper describing it, but was used in a message from Craig Partridge to the Internet Engineering Task Force describing that paper prior to its publication.
The key task to be performed in internet routing is to match a given IPv4 address (viewed as a sequence of 32 bits) to the longest prefix of the address for which routing information is available. This prefix matching problem may be solved by a trie, but trie structures use a significant amount of space (a node for each bit of each address) and searching them requires traversing a sequence of nodes with length proportional to the number of bits in the address. The Luleå algorithm shortcuts this process by storing only the nodes at three levels of the trie structure, rather than storing the entire trie.
The main advantage of the Luleå algorithm for the routing task is that it uses very little memory, averaging 4–5 bytes per entry for large routing tables. This small memory footprint often allows the entire data structure to fit into the routing processor's cache, speeding operations. However, it has the disadvantage that it cannot be modified easily: small changes to the routing table may require most or all of the data structure to be reconstructed.","[u'Internet architecture', u'Networking algorithms', u'Routing algorithms', u'Routing software']","[u'Binary search', u'Bit vector', u'Computer science', u'Craig Partridge', u'Data', u'Digital object identifier', u'IP address', u'IPv4', u'International Standard Book Number', u'Internet', u'Internet Engineering Task Force', u'Lule\xe5 University of Technology', u'Node (computer science)', u'Patent', u'Prefix', u'Routing table', u'Sequential search', u'Trie', u'Word (data type)']"
MD5,"The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety of cryptographic applications, and is also commonly used to verify data integrity.
MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function, MD4. The source code in RFC 1321 contains a ""by attribution"" RSA license.
In 1996 a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1—which has since been found to be vulnerable as well. In 2004 it was shown that MD5 is not collision resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity. As of 2010, the CMU Software Engineering Institute considers MD5 ""cryptographically broken and unsuitable for further use"", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.","[u'All articles with unsourced statements', u'Articles with example pseudocode', u'Articles with unsourced statements from August 2014', u'Broken hash functions', u'Checksum algorithms', u'Cryptographic hash functions', u'Use dmy dates from April 2014']","[u'ASCII', u'Alexander Sotirov', u'Arjen Lenstra', u'Authenticated encryption', u'Avalanche effect', u'BLAKE (hash function)', u'Bates numbering', u'Bcrypt', u'Benne de Weger', u'Birthday attack', u'Bit', u'Block cipher', u'Brute force attack', u'Byte', u'CA certificate', u'CBC-MAC', u'CCM mode', u'CMAC', u'CMU Software Engineering Institute', u'CNET.com', u'CRYPTREC', u'CWC mode', u'Chaos Communication Congress', u'Checksum', u'Chosen-prefix collision attack', u'Collision (computer science)', u'Collision attack', u'Collision resistance', u'Collision resistant', u'Comparison of cryptographic hash functions', u'Crypt (C)', u'Cryptanalysis', u'Cryptographic hash function', u'Cryptographically secure pseudorandom number generator', u'Cryptography', u'Data Authentication Algorithm', u'Data integrity', u'Digital certificate', u'Digital object identifier', u'Digital signature', u'Distributed computing', u'EAX mode', u'EPFL', u'Electronic discovery', u'Elliptic curve only hash', u'Endianness', u'Eurocrypt', u'Fast Syndrome Based Hash', u'Flame (malware)', u'GOST (hash function)', u'Galois/Counter Mode', u'Google', u'Graphics processing unit', u'Gr\xf8stl', u'HAS-160', u'HAVAL', u'HMAC', u'Hans Dobbertin', u'HashClash', u'Hash collision', u'Hash function security summary', u'Hash value', u'Hexadecimal', u'History of cryptography', u'IAPM (mode)', u'IBM p690', u'Initialization vector', u'Institute of Electrical and Electronics Engineers', u'International Standard Book Number', u'JH (hash function)', u'Jacob Appelbaum', u'Key derivation function', u'Key stretching', u'Kupyna', u'LM hash', u'Length extension attack', u'Logical conjunction', u'Logical disjunction', u'MD2 (cryptography)', u'MD4', u'MD5CRK', u'MD6', u'MDC-2', u'Massachusetts Institute of Technology', u'Md5deep', u'Md5sum', u'Merkle\u2013Damg\xe5rd construction', u'Message Passing Interface', u'Message authentication', u'Message authentication code', u'Message digest', u'Microsoft', u'Modular addition', u'N-Hash', u'NESSIE', u'NIST hash function competition', u'Negation', u'Nibble', u'OCB mode', u'Octet (computing)', u'One-key MAC', u'One-way compression function', u'Outline of cryptography', u'PBKDF2', u'PMAC (cryptography)', u'Padding (cryptography)', u'Password', u'PlayStation 3', u'Poly1305', u'PostScript', u'Preimage attack', u'Public-key cryptography', u'Public key certificate', u'RIPEMD', u'RIPEMD-160', u'RSA Laboratories', u'RadioGat\xfan', u'Rainbow table', u'RapidSSL', u'Request for Comments', u'Ronald Rivest', u'SHA-1', u'SHA-2', u'SHA-3', u'SWIFFT', u'Salt (cryptography)', u'Scrypt', u'Search engine', u'Sic', u'Side-channel attack', u'SipHash', u'Skein (hash function)', u'Snefru', u'Software', u'Sony', u'Springer Berlin Heidelberg', u'Steganography', u'Stream cipher', u'Streebog', u'Symmetric-key algorithm', u'The quick brown fox jumps over the lazy dog', u'Tiger (cryptography)', u'Transport Layer Security', u'UMAC', u'Uniform Resource Locator', u'United States Cyber Command', u'VMAC', u'VeriSign', u'Very smooth hash', u'Vlastimil Klima', u'Whirlpool (cryptography)', u'Wired News', u'X.509', u'XOR', u'Xiaoyun Wang', u'Xuejia Lai']"
Maekawa's Algorithm,Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system. The basis of this algorithm is a quorum like approach where any one site needs only to seek permissions from a subset of other sites.,"[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Concurrency control algorithms']","[u'Distributed system', u""Lamport's Distributed Mutual Exclusion Algorithm"", u""Lamport's bakery algorithm"", u'Logical clock', u'Mutual exclusion', u""Raymond's algorithm"", u'Ricart-Agrawala algorithm']"
Marching cubes,"Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (sometimes called voxels). This paper is one of the most cited papers in the computer graphics field. The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images, and special effects or 3-D modelling with what is usually called metaballs or other metasurfaces. An analogous two-dimensional method is called the marching squares algorithm.","[u'3D computer graphics', u'All articles with unsourced statements', u'Articles with unsourced statements from August 2015', u'Commons category template with no category set', u'Commons category without a link on Wikidata', u'Computer graphics algorithms', u'Mesh generation']","[u'Algorithm', u'Asymptotic decider', u'Computed axial tomography', u'Computer graphics', u'Digital object identifier', u'General Electric', u'Gradient', u'Illumination model', u'Image-based meshing', u'International Standard Book Number', u'Isosurface', u'Linear interpolation', u'MRI', u'Magnetic resonance imaging', u'Marching squares', u'Marching tetrahedra', u'Marching tetrahedrons', u'Medical visualization', u'Metaballs', u'Metasurface', u'Polygonal mesh', u'SIGGRAPH', u'Scalar field', u'Voxel']"
Marching squares,"Marching squares is a computer graphics algorithm that generates contours for a two-dimensional scalar field (rectangular array of individual numerical values). A similar method can be used to contour 2D triangle meshes.
The contours can be of two kinds:
Isolines - lines following a single data level, or isovalue.
Isobands - filled areas between isolines.
Typical applications include the Contour lines on topographic maps or the generation of isobars for weather maps.
Marching squares takes a similar approach to the 3D marching cubes algorithm:
Process each cell in the grid independently.
Calculate a cell index using comparisons of the contour level(s) with the data values at the cell corners.
Use a pre-built lookup table, keyed on the cell index, to describe the output geometry for the cell.
Apply linear interpolation along the boundaries of the cell to calculate the exact contour position.",[u'Computer graphics algorithms'],"[u'Algorithm', u'Array data structure', u'Average', u'Binary numeral system', u'Bit', u'Bitwise OR', u'C language', u'Cache (computing)', u'Clockwise', u'Computer graphics', u'Concave (disambiguation)', u'Contour line', u'Contour lines', u'Delaunay triangulation', u'Digital object identifier', u'Embarrassingly parallel', u'Fortran', u'Heptagon', u'Least significant bit', u'Linear interpolation', u'Logical shift', u'Lookup table', u'Marching cubes', u'Most significant bit', u'Parallel algorithm', u'Plane (geometry)', u'Quadrilateral', u'Saddle points', u'Scalar field', u'Scientific visualization', u'Short integer', u'Simplex', u'Ternary numeral system', u'Three-dimensional space', u'Topology', u'Triangulated irregular network']"
Marching tetrahedrons,"Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces. It clarifies a minor ambiguity problem of the marching cubes algorithm with some cube configurations.
Since more than 20 years have passed from the patent filing date of the marching cubes (June 5, 1985), the original algorithm can be used freely again, adding only the minor modification to circumvent the aforementioned ambiguity in some configurations.
In marching tetrahedra, each cube is split into six irregular tetrahedra by cutting the cube in half three times, cutting diagonally through each of the three pairs of opposing faces. In this way, the tetrahedra all share one of the main diagonals of the cube. Instead of the twelve edges of the cube, we now have nineteen edges: the original twelve, six face diagonals, and the main diagonal. Just like in marching cubes, the intersections of these edges with the isosurface are approximated by linearly interpolating the values at the grid points.
Adjacent cubes share all edges in the connecting face, including the same diagonal. This is an important property to prevent cracks in the rendered surface, because interpolation of the two distinct diagonals of a face usually gives slightly different intersection points. An added benefit is that up to five computed intersection points can be reused when handling the neighbor cube. This includes the computed surface normals and other graphics attributes at the intersection points.
Each tetrahedron has sixteen possible configurations, falling into three classes: no intersection, intersection in one triangle and intersection in two (adjacent) triangles. It is straightforward to enumerate all sixteen configurations and map them to vertex index lists defining the appropriate triangle strips.","[u'All articles needing additional references', u'Articles needing additional references from September 2012', u'Computer graphics algorithms']","[u'Asymptotic decider', u'Computer graphics', u'Image-based meshing', u'Implicit surface', u'International Standard Book Number', u'Isosurface', u'Lookup table', u'Marching cubes', u'Surface normal', u'Tessellation', u'Tetrahedron', u'Triangle strip']"
Mark-compact algorithm,"In computer science, a mark-compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory. Mark-compact algorithms can be regarded as a combination of the mark-sweep algorithm and Cheney's copying algorithm. First, reachable objects are marked, then a compacting step relocates the reachable (marked) objects towards the beginning of the heap area. Compacting garbage collection is used by Microsoft's Common Language Runtime and by the Glasgow Haskell Compiler.","[u'Automatic memory management', u'Memory management algorithms', u'Use dmy dates from August 2012']","[u'Algorithm', u'Automatic variable', u'Big O notation', u'Boehm garbage collector', u'Buffer over-read', u'Buffer overflow', u'C dynamic memory allocation', u""Cheney's algorithm"", u'Common Language Runtime', u'Comparison sort', u'Computer science', u'Dangling pointer', u'Delete (C++)', u'Demand paging', u'Finalizer', u'Fragmentation (computer)', u'Fragmentation (computing)', u'Garbage (computer science)', u'Garbage collection (computer science)', u'Glasgow Haskell Compiler', u'International Symposium on Memory Management', u'LISP2', u'Manual memory management', u'Mark-sweep algorithm', u'Memory leak', u'Memory management', u'Memory management (operating systems)', u'Memory management unit', u'Memory safety', u'Memory segmentation', u'Microsoft', u'New (C++)', u'Page table', u'Paging', u'Protected mode', u'Real mode', u'Reference counting', u'Region-based memory management', u'Stack overflow', u'Static memory allocation', u'Strong reference', u'Translation lookaside buffer', u'Unreachable memory', u'Virtual 8086 mode', u'Virtual memory', u'Virtual memory compression', u'Weak reference', u'X86 memory segmentation']"
Marr–Hildreth algorithm,"In computer vision, the Marr–Hildreth algorithm is a method of detecting edges in digital images, that is, continuous curves where there are strong and rapid variations in image brightness. The Marr–Hildreth edge detection method is simple and operates by convolving the image with the Laplacian of the Gaussian function, or, as a fast approximation by Difference of Gaussians. Then, zero crossings are detected in the filtered result to obtain the edges. The Laplacian-of-Gaussian image operator is sometimes also referred to as the Mexican hat wavelet due to its visual shape when turned upside-down. David Marr and Ellen C. Hildreth are two of the inventors.
The Marr–Hildreth operator, however, suffers from two main limitations. It generates responses that do not correspond to edges, so-called ""false edges"", and the localization error may be severe at curved edges. Today, there are much better edge detection methods, such as the Canny edge detector based on the search for local directional maxima in the gradient magnitude, or the differential approach based on the search for zero crossings of the differential expression that corresponds to the second-order derivative in the gradient direction (Both of these operations preceded by a Gaussian smoothing step.) For more details, see the article on Edge detection.","[u'Algorithms and data structures stubs', u'All articles needing additional references', u'All stub articles', u'Articles needing additional references from September 2014', u'Computer science stubs', u'Feature detection (computer vision)']","[u'Algorithm', u'Blob detection', u'CVIPtools', u'Canny edge detector', u'Computer vision', u'Data structure', u'David Marr (neuroscientist)', u'David Marr (psychologist)', u'Difference of Gaussians', u'Digital image', u'Digital object identifier', u'Edge detection', u'Ellen Hildreth', u'Gaussian function', u'International Standard Book Number', u'Laplacian', u'Mexican hat wavelet', u'Zero crossing']"
Marzullo's algorithm,"Marzullo's algorithm, invented by Keith Marzullo for his Ph.D. dissertation in 1984, is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources. A refined version of it, renamed the ""intersection algorithm"", forms part of the modern Network Time Protocol. The Marzullo's algorithm is also used to compute the relaxed intersection of n boxes (or more generally n subsets of Rn), as required by several robust set estimation methods.",[u'Agreement algorithms'],"[u'Agreement algorithm', u'Asymptotic', u'Big O notation', u'Confidence interval', u'Consistent', u'Intersection algorithm', u'Keith Marzullo', u'Linear', u'Network Time Protocol', u'Noise', u'Probabilistic model', u'Relaxed intersection', u'Set estimation', u'Sorting algorithm', u'Tuple']"
Match Rating Approach,"The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.
The algorithm itself has a simple set of encoding rules but a more lengthy set of comparison rules. The main mechanism being the similarity comparison which calculates the number of unmatched characters by comparing the strings from left to right and then from right to left and removing identical characters. This value is subtracted from 6 and then compared to a minimum threshold. The minimum threshold is defined by table A and is dependent upon the length of the strings.
The encoded name is known (perhaps incorrectly) as a personal numeric identifier (PNI). The PNI codex can never contain more than 6 alpha only characters.
Match rating approach performs well with names containing the letter ""y"" unlike the original flavour of the NYSIIS algorithm. For example, the surnames ""Smith"" and ""Smyth"" are successfully matched.
MRA does not perform well with encoded names that differ in length by more than 2.

","[u'All Wikipedia articles needing context', u'All pages needing cleanup', u'Phonetic algorithms', u'Wikipedia articles needing context from October 2009', u'Wikipedia introduction cleanup from October 2009']","[u'Codex', u'Homophonous', u'NYSIIS', u'Phonetic algorithm', u'Western Airlines']"
Matrix multiplication algorithm,"Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient. Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition and in seemingly unrelated problems such counting the paths through a graph. Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).
Directly applying the mathematical definition of matrix multiplication gives an algorithm that takes time on the order of n3 to multiply two n × n matrices (Θ(n3) in big O notation). Better asymptotic bounds on the time required to multiply matrices have been known since the work of Strassen in the 1960s, but it is still unknown what the optimal time is (i.e., what the complexity of the problem is).","[u'All articles containing potentially dated statements', u'Articles containing potentially dated statements from 2010', u'Matrix multiplication algorithms', u'Unsolved problems in computer science']","[u'Abelian group', u'Analysis of algorithms', u'ArXiv', u'Asymptotic notation', u'Bal\xe1zs Szegedy', u'Basic Linear Algebra Subprograms', u'Big O notation', u'Block matrix', u'CPU cache', u'CYK algorithm', u'Cache-oblivious algorithm', u'Cache-oblivious matrix multiplication', u'Cambridge University Press', u""Cannon's algorithm"", u'Charles E. Leiserson', u'Chris Umans', u'CiteSeer', u'Clifford Stein', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Computational complexity of mathematical operations', u'Computational complexity theory', u'Coppersmith\u2013Winograd algorithm', u'Critical path length', u'Digital object identifier', u'Distributed computing', u'Divide and conquer', u'Don Coppersmith', u'Floating point', u'Fork\u2013join model', u""Freivalds' algorithm"", u'Graph (graph theory)', u'Group theory', u'Harald Prokop', u'International Standard Book Number', u'Introduction to Algorithms', u'Jack Dongarra', u'List of unsolved problems in computer science', u'Locality of reference', u'Loop tiling', u'Loop unrolling', u'MapReduce', u'Master theorem', u'Mathematical Reviews', u'Matrix chain multiplication', u'Matrix decomposition', u'Matrix multiplication', u'Mesh networking', u'Multiprocessing', u'Multiprogramming', u'Noga Alon', u'Numerical Recipes', u'Numerical algorithm', u'Numerical linear algebra', u'Numerical stability', u'Parallel algorithm', u'Parallel computing', u'Pattern recognition', u'Pseudocode', u'Ran Raz', u'Recursion', u'Robert Kleinberg', u'Ron Rivest', u'Row-major order', u'SIMD', u'Saul Teukolsky', u'Scalar multiplication', u'Scientific computing', u'Shared-memory multiprocessor', u'Shmuel Winograd', u'Sparse matrix', u'Sparse matrix-vector multiplication', u'Speedup', u'Steven Skiena', u'Strassen algorithm', u'Sunflower conjecture', u'System of linear equations', u'Thomas H. Cormen', u'Time complexity', u'Translation lookaside buffer', u'Triple product property', u'Volker Strassen', u'Wreath product']"
Medical algorithms,"A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.","[u'Algorithms', u'All articles that may contain original research', u'All articles to be expanded', u'Articles needing translation from Russian Wikipedia', u'Articles that may contain original research from October 2007', u'Articles to be expanded from September 2015', u'Health informatics', u'Knowledge representation']","[u'Algorithm', u'Arden syntax', u'Artificial neural network', u'Automatic control', u'Automation', u'Body mass index', u'Calculation', u'Calculators', u'Chest pain', u'Clinical decision support system', u'Clinical trial protocol', u'Clinician', u'Computation', u'Computerized health diagnostics', u'Consensus (medical)', u'Decision-making', u'Decision making', u'Decision tree', u'Diagnosis', u'Digital object identifier', u'Etiology', u'Evidence-based medicine', u'Flowcharts', u'Food energy', u'Formula', u'Guideline (medical)', u'Healthcare', u'ICU scoring systems', u'Journal club', u'Journal of the American Medical Informatics Association', u'Logic', u'Look-up table', u'Medical', u'Medical care', u'Medical equipment', u'Medical guideline', u'Medical informatics', u'Medical logic module', u'Nomogram', u'Obesity', u'Odds algorithm', u'Overweight', u'Physician', u'Prediction', u'Prognosis', u'PubMed Central', u'Stanford University', u'Statistical survey', u'Symptom', u'Texas Medication Algorithm Project', u'Treatment Guidelines from The Medical Letter']"
Memetic algorithm,"Memetic algorithms (MA) represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MA are also referred to in the literature as Baldwinian evolutionary algorithms (EA), Lamarckian EAs, cultural algorithms, or genetic local search.","[u'All Wikipedia articles needing context', u'All articles with unsourced statements', u'All pages needing cleanup', u'Articles with unsourced statements from September 2014', u'Evolutionary algorithms', u'Wikipedia articles needing context from February 2011', u'Wikipedia introduction cleanup from February 2011']","[u'Algorithm', u'Artificial neural network', u'Bin packing problem', u'Charged particle beam', u'Chromosome', u'Circuit design', u'Cluster analysis', u'Digital object identifier', u'Dual-phase evolution', u'Evolutionary algorithm', u'Evolutionary computation', u'Expert system', u'Expression profiling', u'Feature selection', u'Generalized Assignment Problem', u'Genetic algorithm', u'Genotype', u'Graph coloring', u'Graph partition', u'Heuristic', u'Hyper-heuristic', u'IEEE Computational Intelligence Society', u'IEEE Transactions on Evolutionary Computation', u'Independent set problem', u'International Standard Book Number', u'Knapsack problem', u'Meme', u'Motion planning', u'NHL', u'NP (complexity)', u'Nurse rostering problem', u'Pattern recognition', u'Processor allocation', u'PubMed Identifier', u'Quadratic assignment problem', u'Richard Dawkins', u'Schedule (workplace)', u'Set cover problem', u'Single machine scheduling', u'Springer Science+Business Media', u'Travelling salesman problem', u'Universal Darwinism', u'VLSI']"
Merge algorithm,"Merge algorithms are a family of algorithms that run sequentially over multiple sorted lists, typically producing more sorted lists as output. This is well-suited for machines with tape drives.
The general merge algorithm has a set of pointers p0..n that point to positions in a set of lists L0..n. Initially they point to the first item in each list. The algorithm is as follows:
While any of p0..n still point to data inside of L0..n instead of past the end:
do something with the data items p0..n point to in their respective lists
find out which of those pointers points to the item with the lowest key; advance one of those pointers to the next item in its list","[u'All articles needing additional references', u'All articles needing expert attention', u'Articles needing additional references from August 2008', u'Articles needing expert attention from August 2009', u'Articles needing expert attention with no reason or talk parameter', u'Articles with example pseudocode', u'Computer science articles needing expert attention', u'Sorting algorithms']","[u'Algorithm', u'All nearest smaller values', u'Array data structure', u'Big O notation', u'C++', u'Charles E. Leiserson', u'Clifford Stein', u'Collections', u'Computer language', u'Digital object identifier', u'Donald Knuth', u'Heap (data structure)', u'International Standard Book Number', u'Introduction to Algorithms', u'Iterator', u'Join (SQL)', u'Join (Unix)', u'Join (relational algebra)', u'Merge (revision control)', u'Merge sort', u'Multi-core processor', u'Parallel computing', u'Pointer (computer programming)', u'Priority queue', u'Python (programming language)', u'Ronald L. Rivest', u'Sorting algorithm', u'Standard Template Library', u'Tape drive', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Uzi Vishkin']"
Merge sort,"In computer science, merge sort (also commonly spelled mergesort) is an O(n log n) comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948.","[u'All articles with dead external links', u'All articles with unsourced statements', u'Articles with dead external links from June 2013', u'Articles with example pseudocode', u'Articles with inconsistent citation formats', u'Articles with unsourced statements from April 2014', u'Articles with unsourced statements from June 2008', u'Articles with unsourced statements from March 2014', u'Comparison sorts', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Android (operating system)', u'Array data structure', u'Art of Computer Programming', u'Average performance', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary logarithm', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cache (computing)', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'CiteSeer', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Digital object identifier', u'Disk storage', u'Divide and conquer algorithm', u'Donald Knuth', u'External sorting', u'Flashsort', u'Fork\u2013join model', u'GNU Octave', u'Gnome sort', u'Heapsort', u'Herman Goldstine', u'Hybrid algorithm', u'IBM 729', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'International Standard Serial Number', u'Introduction to Algorithms', u'Introsort', u'JSort', u'Java 7', u'Java platform', u'John von Neumann', u'Library sort', u'Linked list', u'Lisp programming language', u'List (computing)', u'Locality of reference', u'Magnetic tape', u'Master theorem', u'Memory hierarchy', u'Merge algorithm', u'Novosibirsk', u'Odd\u2013even sort', u'Oscillating merge sort', u'Page (computer memory)', u'Pairwise sorting network', u'Pancake sorting', u'Parallel Random Access Machine', u'Patience sorting', u'Perl', u'Pigeonhole sort', u'Polyphase merge sort', u'Primary storage', u'Proxmap sort', u'Pseudocode', u'Python (programming language)', u'Quasilinear time', u'Quicksort', u'Radix sort', u'Ron Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Software optimization', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Tape drive', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Tim Peters (software engineer)', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Worst-case performance']"
Metaphone,"Lawrence Philips redirects here. For the football player, see Lawrence Phillips.
Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.
The original author later produced a new version of the algorithm, which he named Double Metaphone. Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. In 2009 Lawrence Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings.",[u'Phonetic algorithms'],"[u'ASCII', u'C/C++ Users Journal', u'Caverphone', u'Celtic languages', u'Ch (digraph)', u'Chinese language', u'Consonant', u'English language', u'French language', u'Germanic languages', u'Greek language', u'Italian language', u'Lawrence Phillips', u'Match Rating Approach', u'New York State Identification and Intelligence System', u'Phonetic algorithm', u'Sh (digraph)', u'Slavic languages', u'Soundex', u'Spanish language', u'Th (digraph)', u'Theta', u'Vowels']"
Methods of computing square roots,"In numerical analysis, a branch of mathematics, there are several square root algorithms or methods of computing the principal square root of a nonnegative real number. For the square roots of a negative or complex number, see below.
Finding  is the same as solving the equation . Therefore, any general numerical root-finding algorithm can be used. Newton's method, for example, reduces in this case to the so-called Babylonian method:

Generally, these methods yield approximate results. To get a higher precision for the root, a higher precision for the square is required and a larger number of steps must be calculated.","[u'All articles needing expert attention', u'All articles that are too technical', u'All articles that may contain original research', u'Articles needing expert attention from September 2012', u'Articles that may contain original research from January 2012', u'Computer arithmetic algorithms', u'Pages using web citations with no URL', u'Root-finding algorithms', u'Wikipedia articles that are too technical from September 2012']","[u'Absolute value', u'Algorithm', u'Alpha max plus beta min algorithm', u'Arithmetic mean', u'Babylonian mathematics', u'Bakhshali manuscript', u'Binary numeral system', u'Bitwise operation', u'Brahmagupta', u'CORDIC', u'Calculator', u'CiteSeer', u'Complex number', u'Continued fraction', u'David Wheeler (computer scientist)', u'Division algorithm', u'Electronic Delay Storage Automatic Calculator', u'Eric W. Weisstein', u'Exponent bias', u'Exponential function', u'Fast inverse square root', u'Floating point', u'Fortran', u'Fused multiply\u2013add', u'Generalized continued fraction', u'Geometric mean', u""Halley's method"", u'Hero of Alexandria', u""Heron's formula"", u""Heron's method"", u'Hexadecimal', u""Householder's method"", u'IEEE floating-point standard', u'Inequality of arithmetic and geometric means', u'Integer square root', u'International Standard Book Number', u'Logarithm table', u'Long division', u'MathWorld', u'Maurice Wilkes', u'Mental calculation', u'Methods of computing square roots', u'Multiplicative inverse', u""Napier's bones"", u'Natural logarithm', u'Natural number', u""Newton's method"", u'Nonnegative', u'Normalized number', u'Nth root algorithm', u'Numeral system', u'Numerical analysis', u'On-Line Encyclopedia of Integer Sequences', u'Order of convergence', u'P-adic number', u""Pell's equation"", u'Periodic continued fraction', u'Quadratic convergence', u'Quadratic integer', u'Quadratic irrational', u'Rate of convergence', u'Real number', u'Recurrence relation', u'Relative error', u'Root-finding algorithm', u'SGI Indigo', u'Scientific notation', u'Seed value', u'Shifting nth-root algorithm', u'Shifting nth root algorithm', u'Slide rule', u'Space-time tradeoff', u'Square number', u'Square root', u'Square root of 2', u'Stanley Gill', u'Taylor series', u'Vedic Mathematics (book)']"
Metropolis light transport,"The Metropolis light transport (MLT) is an application of a variant of the Monte Carlo method called the Metropolis-Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes.
The procedure constructs paths from the eye to a light source using bidirectional path tracing, then constructs slight modifications to the path. Some careful statistical calculation (the Metropolis algorithm) is used to compute the appropriate distribution of brightness over the image. This procedure has the advantage, relative to bidirectional path tracing, that once a path has been found from light to eye, the algorithm can then explore nearby paths; thus difficult-to-find light paths can be explored more thoroughly with the same number of simulated photons. In short, the algorithm generates a path and stores the path's 'nodes' in a list. It can then modify the path by adding extra nodes and creating a new light path. While creating this new path, the algorithm decides how many new 'nodes' to add and whether or not these new nodes will actually create a new path.
Metropolis Light Transport is an unbiased method that, in some cases (but not always), converges to a solution of the rendering equation faster than other unbiased algorithms such as path tracing or bidirectional path tracing.","[u'All articles lacking in-text citations', u'All articles with unsourced statements', u'All stub articles', u'Articles lacking in-text citations from February 2014', u'Articles with unsourced statements from July 2010', u'Computing stubs', u'Global illumination algorithms', u'Monte Carlo methods']","[u'3D computer graphics', u'Arion Render', u'Digital object identifier', u'Eric Veach', u'Indigo Renderer', u'International Standard Book Number', u'Iray', u'Kerkythea', u'Leonidas J. Guibas', u'LuxRender', u'Metropolis-Hastings algorithm', u'Mitsuba Renderer', u'Monte Carlo method', u'Nicholas Metropolis', u'Path tracing', u'Rendering equation', u'Stanford University']"
Metropolis–Hastings algorithm,"In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.","[u'Markov chain Monte Carlo', u'Monte Carlo methods', u'Statistical algorithms']","[u'Adaptive rejection sampling', u'American Statistician', u'Ann. Appl. Probab.', u'Arianna W. Rosenbluth', u'Augusta H. Teller', u'Autocorrelation', u'Bayesian statistics', u'Bernd A. Berg', u'Bibcode', u'Biometrika', u'Canonical ensemble', u'Detailed balance', u'Digital object identifier', u'Edward Teller', u'Emilio Segr\xe8', u'Enrico Fermi', u'Equation of State Calculations by Fast Computing Machines', u'Equations of State Calculations by Fast Computing Machines', u'Expected value', u'Gaussian distribution', u'Genetic algorithm', u'Gibbs sampling', u'Hierarchical Bayesian model', u'Histogram', u'International Standard Book Number', u'JSTOR', u'John Wiley & Sons', u'Journal of Chemical Physics', u'Markov Chain', u'Markov chain', u'Markov chain Monte Carlo', u'Markov process', u'Marshall N. Rosenbluth', u'Mean field particle methods', u'Metropolis light transport', u'Monte Carlo integration', u'Multiple-try Metropolis', u'Multivariate distribution', u'Nicholas Metropolis', u'Parallel tempering', u'Particle filter', u'Perseus Publishing', u'Physics of Plasmas', u'Posterior probability', u'Probability distribution', u'Pseudo-random number sampling', u'Random variable', u'Random walk', u'Rejection sampling', u'Rosenbrock function', u'Roy Glauber', u'Sample (statistics)', u'Simulated annealing', u'Slice sampling', u'Stan Ulam', u'Statistical physics', u'Statistics', u'W. K. Hastings', u'World Scientific', u'Zentralblatt MATH']"
Midpoint circle algorithm,"In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for drawing a circle. Bresenham's circle algorithm is derived from the midpoint circle algorithm. The algorithm can be generalized to conic sections.

The algorithm is related to work by Pitteway and Van Aken.

","[u'All Wikipedia articles needing clarification', u'Articles with example C code', u'Articles with example JavaScript code', u'Digital geometry', u'Geometric algorithms', u'Wikipedia articles needing clarification from February 2009']","[u'Arc (geometry)', u'Basis (linear algebra)', u""Bresenham's line algorithm"", u'Circle', u'Computer graphics', u'Conic section', u'Ellipse', u'International Standard Book Number', u'Methods of computing square roots', u'Minecraft', u'Performance tuning', u'Slope', u'Square (algebra)', u'Square root', u'Trigonometry']"
Montgomery reduction,"In modular arithmetic computation, Montgomery modular multiplication, more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication, introduced in 1985 by the American mathematician Peter L. Montgomery.  
Given two integers a and b, the classical modular multiplication algorithm computes ab mod N. Montgomery multiplication works by transforming a and b into a special representation known as Montgomery form. For a modulus N, the Montgomery form of a is defined to be aR mod N for some constant R depending only on N and the underlying computer architecture. If aR mod N and bR mod N are the Montgomery forms of a and b, then their Montgomery product is abR mod N. Montgomery multiplication is a fast algorithm to compute the Montgomery product. Transforming the result out of Montgomery form yields the classical modular product ab mod N.
Because of the overhead involved in converting a and b into Montgomery form, computing a single product by Montgomery multiplication is slower than computing the product in the integers and performing a modular reduction by division or Barrett reduction. However, when many products are required, as in modular exponentiation, the conversion to Montgomery form becomes a negligible fraction of the time of the computation, and performing the computation by Montgomery multiplication is faster than the available alternatives. Many important cryptosystems such as RSA and Diffie–Hellman key exchange are based on arithmetic operations modulo a large number, and for these cryptosystems, the increased speed afforded by Montgomery multiplication can be important in practice.","[u'Computer arithmetic', u'Cryptographic algorithms', u'Modular arithmetic']","[u'Alfred J. Menezes', u'Barrett reduction', u""B\xe9zout's identity"", u'Carry-save adder', u'CiteSeer', u'Diffie\u2013Hellman key exchange', u'Euclidean division', u'Exponentiation by squaring', u'Extended Euclidean algorithm', u'Isomorphism', u'Jacobi symbol', u'Little endian', u'Mathematics of Computation', u'Modular exponentiation', u'Modular inverse', u'Peter Montgomery (mathematician)', u'Quotient ring', u'RSA (cryptosystem)', u'Scott A. Vanstone', u'Side channel attack']"
Muller's method,"Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0. It was first presented by David E. Muller in 1956.
Muller's method is based on the secant method, which constructs at every iteration a line through two points on the graph of f. Instead, Muller's method uses three points, constructs the parabola through these three points, and takes the intersection of the x-axis with the parabola to be the next approximation.",[u'Root-finding algorithms'],"[u'David E. Muller', u'Degree of a polynomial', u'Divided differences', u'International Standard Book Number', u'JSTOR', u'Loss of significance', u""Newton's method"", u'Newton polynomial', u'Numerical analysis', u'Parabola', u'Polynomial', u'Quadratic equation', u'Rate of convergence', u'Recurrence relation', u'Root-finding algorithm', u'Secant method', u""Sidi's generalized secant method"", u'X-axis', u'Zero of a function']"
Multi level feedback queue,"In computer science, a multilevel feedback queue is a scheduling algorithm. Solaris 2.6 Time-Sharing (TS) scheduler implements this algorithm. The Mac OS X and Microsoft Windows schedulers can both be regarded as examples of the broader class of multilevel feedback queue schedulers. This scheduling algorithm is intended to meet the following design requirements for multimode systems:
Give preference to short jobs.
Give preference to I/O bound processes.
Separate processes into categories based on their need for the processor.
The Multi-level Feedback Queue scheduler was first developed by Fernando J. Corbató et al. in 1962, and this work, along with other work on Multics, led the ACM to award Corbató the Turing Award.",[u'Processor scheduling algorithms'],"[u'Aging (scheduling)', u'Central processing unit', u'Computer science', u'Digital object identifier', u'FIFO (computing and electronics)', u'Fair-share scheduling', u'Fernando J. Corbat\xf3', u'First-come, first-served', u'I/O bound', u'International Standard Book Number', u'Lottery scheduling', u'Multilevel queue', u'Multimode systems', u'Preemption (computing)', u'Resource starvation', u'Round-robin scheduling', u'Scheduling (computing)', u'Turing Award']"
Multiplication algorithm,"A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.","[u'All articles needing additional references', u'All articles to be expanded', u'Articles needing additional references from January 2013', u'Articles needing additional references from May 2013', u'Articles needing additional references from September 2012', u'Articles to be expanded from October 2008', u'Computer arithmetic algorithms', u'Multiplication', u'Pages with citations having bare URLs', u'Pages with citations lacking titles', u'Unsolved problems in computer science']","[u'ACC0', u'AKS primality test', u'Abacus', u'Addison-Wesley', u'Addition', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Analog computer', u'Analog signal', u'Ancient Egyptian multiplication', u'Arbitrary-precision arithmetic', u'Arnold Sch\xf6nhage', u'BBC News', u'Baby-step giant-step', u'Babylonian mathematics', u'Baillie\u2013PSW primality test', u'Big O notation', u'Bignum', u'Binary GCD algorithm', u'Binary multiplier', u'Booth encoding', u'Branching program', u'Chakravala method', u""Cipolla's algorithm"", u'Computer algebra system', u'Computer hardware', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Digital data', u'Digital object identifier', u'Discrete Fourier transform', u'Discrete logarithm', u'Distributivity', u'Divide and conquer algorithm', u'Division algorithm', u""Dixon's factorization method"", u'Donald Knuth', u'Elementary school', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Enderun', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u'FL (complexity)', u'Fast Fourier transform', u""Fermat's factorization method"", u'Fermat primality test', u'Fibonacci', u'Floating-point unit', u'Floating point', u'Floor and ceiling functions', u'Function field sieve', u""F\xfcrer's algorithm"", u'Gauss', u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Grid method multiplication', u'Hardware multiplier', u'Horner scheme', u'Index calculus algorithm', u'Integer factorization', u'Integer overflow', u'Integer square root', u'Integrated circuit', u'International Standard Book Number', u'International Standard Serial Number', u'JSTOR', u'Joachim von zur Gathen', u'Karatsuba algorithm', u'Kronecker substitution', u'Lattice multiplication', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Liber Abaci', u'Linear map', u'List of unsolved problems in computer science', u'Logarithm', u'Logical AND', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MOS Technology 6502', u'Matrak\xe7\u0131 Nasuh', u'Mental calculation', u'Mersenne Prime', u'Microcode', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Muhammad ibn Musa al-Khwarizmi', u'Multiplication', u'Multiplication table', u""Napier's bones"", u""Napier's rods"", u'Number-theoretic transform', u'Number theory', u'Numeral system', u'Operational amplifier', u'Partial products algorithm', u'Peasant multiplication', u'Piecewise linear function', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u'Poker chip', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial', u'Primality test', u'Primary school', u'Prosthaphaeresis', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Radix', u'Rational sieve', u'Recursion', u'Ring (mathematics)', u'Rob Eastaway', u'Rounding error', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Slide rule', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'The Art of Computer Programming', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trachtenberg system', u'Trial division', u'Twiddle factor', u'Volker Strassen', u'Voltage', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Nagle's algorithm,"Nagle's algorithm, named after John Nagle, is a means of improving the efficiency of TCP/IP networks by reducing the number of packets that need to be sent over the network.
Nagle's document, Congestion Control in IP/TCP Internetworks (RFC 896) describes what he called the ""small packet problem"", where an application repeatedly emits data in small chunks, frequently only 1 byte in size. Since TCP packets have a 40 byte header (20 bytes for TCP, 20 bytes for IPv4), this results in a 41 byte packet for 1 byte of useful information, a huge overhead. This situation often occurs in Telnet sessions, where most keypresses generate a single byte of data that is transmitted immediately. Worse, over slow links, many such packets can be in transit at the same time, potentially leading to congestion collapse.
Nagle's algorithm works by combining a number of small outgoing messages, and sending them all at once. Specifically, as long as there is a sent packet for which the sender has received no acknowledgment, the sender should keep buffering its output until it has a full packet's worth of output, so that output can be sent all at once.","[u'All articles needing additional references', u'Articles needing additional references from June 2014', u'Networking algorithms', u'Transmission Control Protocol']","[u'ACK (TCP)', u'Bandwidth (computing)', u'Byte', u'Congestion collapse', u'IPv4', u'International Standard Book Number', u'Larry L. Peterson', u'Latency (engineering)', u'Maximum segment size', u'TCP/IP', u'TCP delayed acknowledgment', u'Telnet', u'Transmission Control Protocol', u'User Datagram Protocol']"
Nearest neighbour algorithm,"The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.
Below is the application of nearest neighbour algorithm on TSP
These are the steps of the algorithm:
start on an arbitrary vertex as current vertex.
find out the shortest edge connecting current vertex and an unvisited vertex V.
set current vertex to V.
mark V as visited.
if all the vertices in domain are visited, then terminate.
Go to step 2.
The sequence of the visited vertices is the output of the algorithm.
The nearest neighbour algorithm is easy to implement and executes quickly, but it can sometimes miss shorter routes which are easily noticed with human insight, due to its ""greedy"" nature. As a general guide, if the last few stages of the tour are comparable in length to the first stages, then the tour is reasonable; if they are much greater, then it is likely that there are much better tours. Another check is to use an algorithm such as the lower bound algorithm to estimate if this tour is good enough.
In the worst case, the algorithm results in a tour that is much longer than the optimal tour. To be precise, for every constant r there is an instance of the traveling salesman problem such that the length of the tour computed by the nearest neighbour algorithm is greater than r times the length of the optimal tour. Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour.
The nearest neighbour algorithm may not find a feasible tour at all, even when one exists.","[u'Approximation algorithms', u'Graph algorithms', u'Heuristic algorithms', u'Travelling salesman problem']","[u'Algorithm', u'K-nearest neighbor algorithm', u'Lower bound algorithm', u'Nearest neighbor (disambiguation)', u'Travelling salesman problem']"
Needleman–Wunsch algorithm,"The Needleman–Wunsch algorithm is an algorithm used in bioinformatics to align protein or nucleotide sequences. It was one of the first applications of dynamic programming to compare biological sequences. The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970. The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems and uses the solutions to the smaller problems to reconstruct a solution to the larger problem. It is also sometimes referred to as the optimal matching algorithm and the global alignment technique. The Needleman–Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance.","[u'All articles needing expert attention', u'All articles that are too technical', u'Articles needing expert attention from September 2013', u'Articles with example pseudocode', u'Bioinformatics algorithms', u'Computational phylogenetics', u'Dynamic programming', u'Pages using citations with accessdate and no URL', u'Pages using web citations with no URL', u'Sequence alignment algorithms', u'Wikipedia articles that are too technical from September 2013']","[u'Aho\u2013Corasick algorithm', u'Algorithm', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Array data structure', u'BLOSUM', u'Bellman equation', u'Bioinformatics', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Camera resectioning', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computer stereo vision', u'DNA sequences', u'Damerau\u2013Levenshtein distance', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Distortions', u'Dynamic programming', u'Dynamic time warping', u'Edit distance', u'Embedded systems', u'Gap penalty', u'Generalized suffix tree', u'Hamming distance', u""Hirschberg's algorithm"", u'Indel', u'Jaro\u2013Winkler distance', u'Journal of the ACM', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Matrix (mathematics)', u'Michael J. Fischer', u'Nondeterministic finite automaton', u'Nucleotide', u'Optimal matching', u'Parsing', u'Pattern matching', u'Pixels', u'Point accepted mutation', u'Protein', u'PubMed Central', u'PubMed Identifier', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Real-time computing', u'Recursion', u'Regular expression', u'Regular tree grammar', u'Reverse Engineering', u'Rope (data structure)', u'Scan lines', u'Sequence alignment', u'Sequence mining', u'Sequential pattern mining', u'Similarity matrix', u'Smith-Waterman algorithm', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Vladimir Levenshtein', u'Wagner\u2013Fischer algorithm']"
Nelder–Mead method,"See simplex algorithm for Dantzig's algorithm for the problem of linear optimization.
The Nelder–Mead method or downhill simplex method or amoeba method is a commonly applied numerical method used to find the minimum or maximum of an objective function in a many-dimensional space. It is applied to nonlinear optimization problems for which derivatives may not be known. However, the Nelder–Mead technique is a heuristic search method that can converge to non-stationary points on problems that can be solved by alternative methods.
The Nelder–Mead technique was proposed by John Nelder & Roger Mead (1965).","[u'Operations research', u'Optimization algorithms and methods']","[u'Approximation algorithm', u'Augmented Lagrangian method', u'BFGS method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'CMA-ES', u'COBYLA', u'Centroid', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Derivative-free optimization', u'Differential evolution', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'George B. Dantzig', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic', u'Heuristic algorithm', u'Hill climbing', u""Himmelblau's function"", u'Integer programming', u'International Standard Book Number', u'Iterative method', u'John Nelder', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u'LINCOA', u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Michael J. D. Powell', u'Minimum spanning tree', u'NEWUOA', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Numerical method', u'Objective function', u'Optimization (mathematics)', u'Optimization algorithm', u'Pattern search (optimization)', u'Penalty method', u'Polytope', u""Powell's method"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Revised simplex algorithm', u'Rosenbrock function', u'Sequential quadratic programming', u'Simplex', u'Simplex algorithm', u'Simulated annealing', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Tetrahedron', u'Truncated Newton method', u'Trust region', u'Unimodal', u'Wolfe conditions']"
Nested loop join,"A nested loop join is a naive algorithm that joins two sets by using two nested loops. Join operations are important to database management.

","[u'All articles lacking sources', u'All articles needing expert attention', u'All stub articles', u'Articles lacking sources from January 2010', u'Articles needing expert attention from March 2011', u'Articles needing expert attention with no reason or talk parameter', u'Computer science stubs', u'Join algorithms', u'Mathematics articles needing expert attention']","[u'Algorithm', u'Block nested loop', u'Computer science', u'Database', u'Loop (computing)', u'Memory (computing)']"
Nested sampling algorithm,"The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling.","[u'All Wikipedia articles needing context', u'All pages needing cleanup', u'Bayesian statistics', u'Model selection', u'Statistical algorithms', u'Wikipedia articles needing context from October 2009', u'Wikipedia introduction cleanup from October 2009']","[u'ArXiv', u'Astronomy', u""Bayes' theorem"", u'Bayes factor', u'Bayesian model comparison', u'Bayesian statistics', u'Bibcode', u'C (programming language)', u'Computation', u'Condensed matter physics', u'Cosmology', u'Digital object identifier', u'Finite element', u'Finite element updating', u'International Standard Book Number', u'Lebesgue integration', u'Model selection', u'Physicist', u'Python (programming language)', u'R (programming language)', u'Statistical physics', u'Structural dynamics']"
Network scheduler,"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer. There are several network schedulers available for the different operating system kernels, that implement many of the existing network scheduling algorithms.
The network scheduler logic decides, in a way similar to statistical multiplexers, which network packet to forward next from the buffer. The buffer works as a queuing system, storing the network packets temporarily until they are transmitted. The buffer space may be divided into different queues, with each of them holding the packets of one flow according to configured packet classification rules; for example, packets can be divided into flows by their source and destination IP addresses. Network scheduling algorithms and their associated settings determine how the network scheduler manages the buffer.
Also, network schedulers are enabling accomplishment of the active queue management and traffic shaping.","[u'All articles with unsourced statements', u'All pages needing cleanup', u'Articles needing cleanup from September 2014', u'Articles with sections that need to be turned into prose from September 2014', u'Articles with unsourced statements from September 2014', u'Linux kernel features', u'Network performance', u'Network scheduling algorithms', u'Network theory', u'Wikipedia articles needing clarification from June 2014']","[u'ALTQ', u'Acknowledgement (data networks)', u'Active queue management', u'Adaptive virtual queue', u'Algorithm', u'Arbiter (electronics)', u'Bandwidth management', u'Berkeley Software Distribution', u'Blue (queue management algorithm)', u'Buffer (telecommunication)', u'Bufferbloat', u'Circular buffer', u'Class-based queueing', u'CoDel', u'Credit-based fair queuing', u'Data buffer', u'Data link layer', u'Deficit round robin', u'Differentiated services', u'Ethernet frame', u'FIFO (computing and electronics)', u'Fair queuing', u'Free and open-source software', u'GNU General Public License', u'Generalized random early detection', u'Generic cell rate algorithm', u'Heavy-hitter filter', u'Hierarchical fair-service curve', u'Ifconfig', u'Integrated services', u'Iproute2', u'Kernel.org', u'Kernel (computing)', u'Latency (engineering)', u'Linux kernel', u'Linux kernel module', u'Netfilter', u'Network congestion', u'Network interface controller', u'Network latency', u'Network packet', u'Nftables', u'OSI model', u'OpenWrt', u'Operating system', u'Packet-switched network', u'Packet delay variation', u'Packet switching', u'Process scheduler', u'Proportional integral controller enhanced', u'Quality of service', u'Queue (abstract data type)', u'Queueing theory', u'Quick fair queueing', u'Random early detection', u'Ring buffer', u'Robust random early detection', u'Round-robin scheduling', u'Router (computing)', u'Statistical multiplexer', u'Statistical time division multiplexing', u'Stochastic fairness queuing', u'Systemd', u'Tail drop', u'Throughput', u'Token Bucket', u'Token bucket filter', u'Traffic classification', u'Traffic shaping', u'Transmission Control Protocol', u'Trivial link equalizer', u'Type of service', u'Voice over IP', u'Weighted fair queuing', u'Weighted random early detection', u'Weighted round robin']"
New York State Identification and Intelligence System,"The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System (now a part of the New York State Division of Criminal Justice Services). It features an accuracy increase of 2.7% over the traditional Soundex algorithm.",[u'Phonetic algorithms'],"[u'New York State', u'Phonetic algorithm', u'Scala (programming language)', u'Soundex']"
Newell's algorithm,"Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal. It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.
In the depth sorting phase of hidden surface removal, if two polygons have no overlapping extents or extreme minimum and maximum values in the x, y, and z directions, then they can be easily sorted. If two polygons, Q and P, do have overlapping extents in the Z direction, then it is possible that cutting is necessary.

In that case Newell's algorithm tests the following:
Test for Z overlap; implied in the selection of the face Q from the sort list
The extreme coordinate values in X of the two faces do not overlap (minimax test in X)
The extreme coordinate values in Y of the two faces do not overlap (minimax test in Y)
All vertices of P lie deeper than the plane of Q
All vertices of Q lie closer to the viewpoint than the plane of P
The rasterisation of P and Q do not overlap
Note that the tests are given in order of increasing computational difficulty.
Note also that the polygons must be planar.
If the tests are all false, then the polygons must be split. Splitting is accomplished by selecting one polygon and cutting it along the line of intersection with the other polygon. The above tests are again performed, and the algorithm continues until all polygons pass the above tests.","[u'3D computer graphics', u'All stub articles', u'Computer graphics algorithms', u'Computer graphics stubs']","[u'3D computer graphics', u'Bob Sproull', u'Boolean operations on polygons', u'CADCentre', u'Computer graphics', u'Dick Newell', u'Digital object identifier', u'Hidden surface determination', u'Ivan Sutherland', u'Martin Newell (computer scientist)', u'Minimax', u""Painter's algorithm"", u'Plane (geometry)', u'Polygon', u'Rasterisation']"
Newton's method,"In numerical analysis, Newton's method (also known as the Newton–Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.

The Newton–Raphson method in one variable is implemented as follows:
Given a function ƒ defined over the reals x, and its derivative ƒ', we begin with a first guess x0 for a root of the function f. Provided the function satisfies all the assumptions made in the derivation of the formula, a better approximation x1 is

Geometrically, (x1, 0) is the intersection with the x-axis of the tangent to the graph of f at (x0, f (x0)).
The process is repeated as

until a sufficiently accurate value is reached.
This algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations.","[u'All articles lacking in-text citations', u'All articles needing additional references', u'Articles lacking in-text citations from February 2014', u'Articles needing additional references from February 2014', u'Articles needing additional references from November 2013', u'Articles with inconsistent citation formats', u'Commons category with local link same as on Wikidata', u'Optimization algorithms and methods', u'Root-finding algorithms', u'Use dmy dates from January 2012']","[u'Abraham de Moivre', u'Absolute time and space', u""Aitken's delta-squared process"", u'Almost all', u'An Historical Account of Two Notable Corruptions of Scripture', u'Approximation algorithm', u'Arithmetica Universalis', u'Arthur Cayley', u'Augmented Lagrangian method', u'Babylonian method', u'Banach space', u'Barrier function', u'Basin of attraction', u'Bellman\u2013Ford algorithm', u'Benjamin Pulleyn', u'Bisection method', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Bucket argument', u'Calculus', u'Catherine Barton', u'Chaos theory', u'Classical mechanics', u'Claude Lemar\xe9chal', u'Combinatorial optimization', u'Comparison of optimization software', u'Complex analysis', u'Convex minimization', u'Convex optimization', u'Copernican Revolution', u'Corpuscular theory of light', u'Cranbury Park', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'De analysi per aequationes numero terminorum infinitas', u'De motu corporum in gyrum', u'Derivative', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Division algorithm', u'Division by zero', u'Dynamic programming', u'Dynamics (mechanics)', u'Early life of Isaac Newton', u'Edmonds\u2013Karp algorithm', u'Elements of the Philosophy of Newton', u'Ellipsoid method', u'Encyclopedia of Mathematics', u'Endre S\xfcli', u'Eric W. Weisstein', u'Euler method', u'Evolutionary algorithm', u'Exchange algorithm', u'Fast inverse square root', u'Finite difference', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Fractal', u'Franciscus Vieta', u'Frank\u2013Wolfe algorithm', u'Fr\xe9chet derivative', u'Function (mathematics)', u'Functional (mathematics)', u'Gauss\u2013Newton algorithm', u'General Scholium', u'Generalized Gauss\u2013Newton method', u'Generalized inverse', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Graph of a function', u'Gravitational constant', u'Greedy algorithm', u""Halley's method"", u""Hensel's lemma"", u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u""Householder's method"", u'Hypotheses non fingo', u'Impact depth', u'Inertia', u'Integer programming', u'Integer square root', u'Intermediate value theorem', u'International Standard Book Number', u'Interval (mathematics)', u'Isaac Barrow', u'Isaac Newton', u""Isaac Newton's occult studies"", u'Isaac Newton S/O Philipose', u'Isaac Newton in popular culture', u'Iterative method', u'Jacobian matrix', u'Jacobian matrix and determinant', u'Jamsh\u012bd al-K\u0101sh\u012b', u'John Colson', u'John Conduitt', u'John Keill', u'John Wallis', u""Johnson's algorithm"", u'Joseph Fourier', u'Joseph Raphson', u'Kantorovich theorem', u""Karmarkar's algorithm"", u""Kepler's laws of planetary motion"", u'Kissing number problem', u""Kruskal's algorithm"", u'Lagrange remainder', u""Laguerre's method"", u'Later life of Isaac Newton', u'Leibniz\u2013Newton calculus controversy', u""Lemke's algorithm"", u'Leonid Kantorovich', u'Levenberg\u2013Marquardt algorithm', u'Limit of a sequence', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'List of things named after Isaac Newton', u'Local convergence', u'Local search (optimization)', u'MathWorld', u'Mathematical Gazette', u'Mathematical Reviews', u'Mathematical optimization', u'Mathematics in medieval Islam', u'Matroid', u'Metaheuristic', u'Method of Fluxions', u'Methods of computing square roots', u'Minimum spanning tree', u'Multiplicative inverse', u'Multiplicity (mathematics)', u'Neighbourhood (mathematics)', u'Nelder\u2013Mead method', u""Newton's cannonball"", u""Newton's cradle"", u""Newton's identities"", u""Newton's inequalities"", u""Newton's law of cooling"", u""Newton's law of universal gravitation"", u""Newton's laws of motion"", u""Newton's metal"", u""Newton's method in optimization"", u""Newton's reflector"", u""Newton's rings"", u""Newton's theorem about ovals"", u""Newton's theorem of revolving orbits"", u'Newton (Blake)', u'Newton (unit)', u'Newton disc', u'Newton fractal', u'Newton polygon', u'Newton polynomial', u'Newton scale', u'Newtonian dynamics', u'Newtonian fluid', u'Newtonian potential', u'Newtonian telescope', u'Newtonianism', u'Newton\u2013Cartan theory', u'Newton\u2013Cotes formulas', u'Newton\u2013Euler equations', u'Newton\u2013Okounkov body', u'Newton\u2013Pepys problem', u'Non-linear least squares', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Notes on the Jewish Temple', u'Numerical analysis', u'Opticks', u'Optimization algorithm', u'Parameterized post-Newtonian formalism', u'Penalty method', u'Philosophi\xe6 Naturalis Principia Mathematica', u'Post-Newtonian expansion', u""Powell's method"", u'Power number', u'Power series', u'Problem of Apollonius', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quaestiones quaedam philosophicae', u'Quasi-Newton method', u'Rate of convergence', u'Real number', u'Religious views of Isaac Newton', u'Revised simplex algorithm', u'Richardson extrapolation', u'Root-finding algorithm', u'Root of a function', u'Rotating spheres', u'Schr\xf6dinger\u2013Newton equation', u'Scientific revolution', u'Scoring algorithm', u'Secant method', u'Second derivative', u'Seki K\u014dwa', u'Sequence', u'Sequential quadratic programming', u'Sextant', u'Sharaf al-Din al-Tusi', u'Simplex algorithm', u'Simulated annealing', u'Solar mass', u'Springer Science+Business Media', u'Standing on the shoulders of giants', u'Stationary point', u""Steffensen's method"", u'Structural coloration', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive over-relaxation', u'Successive parabolic interpolation', u'Supremum', u'Symmetric rank-one', u'System of linear equations', u'Table of Newtonian series', u'Tabu search', u'Tangent', u'Tangent line', u""Taylor's theorem"", u'The Chronology of Ancient Kingdoms Amended', u'The College Mathematics Journal', u'The Mysteryes of Nature and Art', u'The Queries', u'Thomas Simpson', u'Topological neighborhood', u'Transcendental equation', u'Transcendental function', u'Truncated Newton method', u'Trust region', u'Wikibooks', u'William Clarke (apothecary)', u'William Jones (mathematician)', u'William Stukeley', u'Wolfe conditions', u'Woolsthorpe Manor', u'Writing of Principia Mathematica']"
Newton–Raphson division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output.","[u'All articles to be expanded', u'All articles with unsourced statements', u'All pages needing factual verification', u'Articles to be expanded from September 2012', u'Articles with example pseudocode', u'Articles with unsourced statements from February 2012', u'Articles with unsourced statements from February 2014', u'Binary arithmetic', u'Computer arithmetic', u'Computer arithmetic algorithms', u'Division (mathematics)', u'Wikipedia articles needing clarification from July 2015', u'Wikipedia articles needing factual verification from June 2015']","[u'AMD', u'Algorithm', u'Analysis of algorithms', u'Approximation', u'Barrett reduction', u'Binomial theorem', u'Chunking (division)', u'Cryptography', u'Digital object identifier', u'Division (mathematics)', u'Double precision', u'Equioscillation theorem', u""Euclid's Elements"", u'Euclidean division', u'Extended precision', u'Fixed point arithmetic', u'Floating point', u'Fused multiply\u2013add', u'Greatest common divisor', u'Hexadecimal', u'Integer (computer science)', u'International Standard Book Number', u'Karatsuba algorithm', u'Long division', u'Lookup table', u'Microprocessor', u'Modular arithmetic', u'Multiplication algorithm', u'Multiplicative inverse', u""Newton's method"", u'OCLC', u'Original Intel Pentium (P5 microarchitecture)', u'Output-sensitive algorithm', u'Pentium FDIV bug', u'Power of two', u'Precision (computer science)', u'Quotient', u'Radix', u'Rate of convergence', u'Relative error', u'Remainder', u'Remez algorithm', u'Round-off error', u'Sch\xf6nhage\u2013Strassen algorithm', u'Short division', u'Single precision', u'Toom\u2013Cook multiplication']"
Non-restoring division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output.","[u'All articles to be expanded', u'All articles with unsourced statements', u'All pages needing factual verification', u'Articles to be expanded from September 2012', u'Articles with example pseudocode', u'Articles with unsourced statements from February 2012', u'Articles with unsourced statements from February 2014', u'Binary arithmetic', u'Computer arithmetic', u'Computer arithmetic algorithms', u'Division (mathematics)', u'Wikipedia articles needing clarification from July 2015', u'Wikipedia articles needing factual verification from June 2015']","[u'AMD', u'Algorithm', u'Analysis of algorithms', u'Approximation', u'Barrett reduction', u'Binomial theorem', u'Chunking (division)', u'Cryptography', u'Digital object identifier', u'Division (mathematics)', u'Double precision', u'Equioscillation theorem', u""Euclid's Elements"", u'Euclidean division', u'Extended precision', u'Fixed point arithmetic', u'Floating point', u'Fused multiply\u2013add', u'Greatest common divisor', u'Hexadecimal', u'Integer (computer science)', u'International Standard Book Number', u'Karatsuba algorithm', u'Long division', u'Lookup table', u'Microprocessor', u'Modular arithmetic', u'Multiplication algorithm', u'Multiplicative inverse', u""Newton's method"", u'OCLC', u'Original Intel Pentium (P5 microarchitecture)', u'Output-sensitive algorithm', u'Pentium FDIV bug', u'Power of two', u'Precision (computer science)', u'Quotient', u'Radix', u'Rate of convergence', u'Relative error', u'Remainder', u'Remez algorithm', u'Round-off error', u'Sch\xf6nhage\u2013Strassen algorithm', u'Short division', u'Single precision', u'Toom\u2013Cook multiplication']"
Nth root algorithm,"The principal nth root  of a positive real number A, is the positive real solution of the equation

(for integer n there are n distinct complex solutions to this equation if , but only one is positive and real).
There is a very fast-converging nth root algorithm for finding :
Make an initial guess 
Set . In practice we do .
Repeat step 2 until the desired precision is reached, i.e.  .
A special case is the familiar square-root algorithm. By setting n = 2, the iteration rule in step 2 becomes the square root iteration rule:

Several different derivations of this algorithm are possible. One derivation shows it is a special case of Newton's method (also called the Newton-Raphson method) for finding zeros of a function  beginning with an initial guess. Although Newton's method is iterative, meaning it approaches the solution through a series of increasingly accurate guesses, it converges very quickly. The rate of convergence is quadratic, meaning roughly that the number of bits of accuracy doubles on each iteration (so improving a guess from 1 bit to 64 bits of precision requires only 6 iterations). For this reason, this algorithm is often used in computers as a very fast method to calculate square roots.
For large n, the nth root algorithm is somewhat less efficient since it requires the computation of  at each step, but can be efficiently implemented with a good exponentiation algorithm.",[u'Root-finding algorithms'],"[u'Complex number', u'Exponentiation', u'International Standard Book Number', u'Limit of a sequence', u'Methods of computing square roots', u'Negative and positive numbers', u""Newton's method"", u'Nth root', u'Principal branch', u'Real number', u'Recurrence relation']"
OPTICS algorithm,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander. Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster. This is represented as a dendrogram.","[u'Articles with specifically marked weasel-worded phrases from October 2014', u'Data clustering algorithms']","[u'AAAI Press', u'ACM Press', u'Anomaly detection', u'Artificial neural network', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Bayesian network', u'Bias-variance dilemma', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Canonical correlation analysis', u'Cluster analysis', u'Computational learning theory', u'Conditional random field', u'Convolutional neural network', u'Correlation clustering', u'DBSCAN', u'Data mining', u'Decision tree learning', u'Deep learning', u'Dendrogram', u'Digital object identifier', u'Dimensionality reduction', u'ELKI', u'Empirical risk minimization', u'Ensemble learning', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Fixed-radius near neighbors', u'Grammar induction', u'Graphical model', u'Hans-Peter Kriegel', u'Heap (data structure)', u'Hidden Markov model', u'Hierarchical clustering', u'Independent component analysis', u'International Standard Book Number', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Learning to rank', u'Linear discriminant analysis', u'Linear regression', u'Local outlier factor', u'Logistic regression', u'Machine learning', u'Mean-shift', u'Multilayer perceptron', u'Naive Bayes classifier', u'Non-negative matrix factorization', u'Online machine learning', u'Perceptron', u'Principal component analysis', u'Priority queue', u'Probably approximately correct learning', u'Random forest', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Self-organizing map', u'Semi-supervised learning', u'Single-linkage clustering', u'Spanning tree', u'Spatial index', u'Springer-Verlag', u'Statistical classification', u'Statistical learning theory', u'Structured prediction', u'Subspace clustering', u'Supervised learning', u'Support vector machine', u'T-distributed stochastic neighbor embedding', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Weka (machine learning)']"
Odd-even sort,"In computing, an odd–even sort or odd–even transposition sort (also known as brick sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.","[u'Accuracy disputes from July 2014', u'All stub articles', u'Articles containing proofs', u'Articles with example pseudocode', u'Comparison sorts', u'Computer science stubs', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bubblesort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Zero-based']"
Odds algorithm,"The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below.
The odds-algorithm applies to a class of problems called last-success-problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a ""specific event""). This identification must be done at the time of observation. No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of ""stopping"" to take a well-defined action. Such problems are encountered in several situations.","[u'Mathematical optimization', u'Optimal decisions', u'Statistical algorithms']","[u'Annals of Probability', u'Clinical trial', u'Compassionate use', u'European Mathematical Society', u'Expanded access', u'F. Thomas Bruss', u'House selling problem', u'Journal of Applied Probability', u'Odds', u'Optimal stopping', u'Parking problem', u'Poisson process', u'Portfolio (finance)', u""Sciences et Technologies de l'automation"", u'Secretary problem', u'Secretary problems', u'Sequential estimate']"
Odlyzko–Schönhage algorithm,"In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Schönhage 1988). The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) equally spaced values from O(N2) to O(N1+ε) steps (at the cost of storing O(N1+ε) intermediate values). The Riemann–Siegel formula used for calculating the Riemann zeta function with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2. This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+ε steps to about T1+ε steps.
The algorithm can be used not just for the Riemann zeta function, but also for many other functions given by Dirichlet series.
The algorithm was used by Gourdon (2004) to verify the Riemann hypothesis for the first 1013 zeros of the zeta function.","[u'Algorithms and data structures stubs', u'All stub articles', u'Analytic number theory', u'Computational number theory', u'Computer science stubs', u'Zeta and L-functions']","[u'Algorithm', u'Andrew Odlyzko', u'Arnold Sch\xf6nhage', u'Data structure', u'Digital object identifier', u'Dirichlet series', u'Fast Fourier transform', u'JSTOR', u'Mathematical Reviews', u'Odlyzko', u'Riemann hypothesis', u'Riemann zeta function', u'Riemann\u2013Siegel formula', u'Sch\xf6nhage']"
Online algorithm,"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.
In contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand.
As an example, consider the sorting algorithms selection sort and insertion sort: Selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.
Note that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.
Not every online algorithm has an offline counterpart.","[u'All articles needing additional references', u'Articles needing additional references from June 2013', u'Online algorithms']","[u'Adversary (online algorithm)', u'Algorithm', u'Algorithms for calculating variance', u'Allan Borodin', u'Bandit problem', u'Canadian Traveller Problem', u'Competitive analysis (online algorithm)', u'Computer science', u'Dynamic algorithm', u'Greedy algorithm', u'Insertion sort', u'International Standard Book Number', u'Job shop scheduling', u'K-server problem', u'Linear search problem', u'List update problem', u'Metrical task systems', u'Odds algorithm', u'Offline learning', u'Online machine learning', u'PSPACE-complete', u'Page replacement algorithm', u'Perceptron', u'Real-time computing', u'Reservoir sampling', u'Search games', u'Secretary problem', u'Selection sort', u'Sequential algorithm', u'Shortest path problem', u'Ski rental problem', u'Sorting algorithms', u'Streaming algorithm', u""Ukkonen's algorithm""]"
Operator-precedence parser,"In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar. For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN).
Edsger Dijkstra's shunting yard algorithm is commonly used to implement operator precedence parsers. Other algorithms include the precedence climbing method and the top down operator precedence method.","[u'All articles with unsourced statements', u'Articles with example C code', u'Articles with unsourced statements from November 2010', u'Parsing algorithms']","[u'Bottom-up parsing', u'Calculator', u'Compiler compiler', u'Computer science', u'Digital object identifier', u'EBNF', u'Edsger Dijkstra', u'Friedrich L. Bauer', u'GNU Compiler Collection', u'Haskell (programming language)', u'Infix notation', u'International Standard Book Number', u'Klaus Samelson', u'LR parser', u'Nonterminal', u'Operator-precedence grammar', u'Order of operations', u'Parrot virtual machine', u'Parser Grammar Engine', u'Perl 6', u'Pratt parser', u'Recursive descent parser', u'Reverse Polish notation', u'Run time (program lifecycle phase)', u'Shift-reduce parser', u'Shunting yard algorithm']"
PPM compression algorithm,Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream. PPM algorithms can also be used to cluster data into predicted groupings in cluster analysis.,"[u'Articles with Russian-language external links', u'Lossless compression algorithms']","[u'A-law algorithm', u'Adaptive Huffman coding', u'Adaptive differential pulse-code modulation', u'Algebraic code-excited linear prediction', u'Arithmetic coding', u'Audio codec', u'Audio compression (data)', u'Average bitrate', u'Bit rate', u'Burrows\u2013Wheeler transform', u'Byte pair encoding', u'Canonical Huffman code', u'Chain code', u'Chroma subsampling', u'Cluster analysis', u'Code-excited linear prediction', u'Coding tree unit', u'Color space', u'Companding', u'Compression artifact', u'Constant bitrate', u'Context modeling', u'Context tree weighting', u'Convolution', u'DEFLATE', u'Dasher (software)', u'Data compression', u'Deblocking filter', u'Delta encoding', u'Dictionary coder', u'Differential pulse-code modulation', u'Digital object identifier', u'Discrete cosine transform', u'Display resolution', u'Dynamic Markov compression', u'Dynamic range', u'Elias gamma coding', u'Embedded Zerotrees of Wavelet transforms', u'Entropy (information theory)', u'Entropy encoding', u'Escape sequence', u'Exponential-Golomb coding', u'Fibonacci coding', u'Film frame', u'Fourier transform', u'Fractal compression', u'Frame rate', u'Golomb coding', u'Huffman coding', u'Huffman encoding', u'IEEE Computer Society Press', u'IEEE Transactions on Communications', u'Image compression', u'Image resolution', u'Information theory', u'Interlaced video', u'International Standard Book Number', u'Karhunen\u2013Lo\xe8ve theorem', u'Kevin Nanney', u'Kolmogorov complexity', u'LZ4 (compression algorithm)', u'LZ77 and LZ78', u'LZJB', u'LZRW', u'LZWL', u'LZX (algorithm)', u'Language model', u'Lapped transform', u'Latency (audio)', u'Lempel\u2013Ziv\u2013Markov chain algorithm', u'Lempel\u2013Ziv\u2013Oberhumer', u'Lempel\u2013Ziv\u2013Stac', u'Lempel\u2013Ziv\u2013Storer\u2013Szymanski', u'Lempel\u2013Ziv\u2013Welch', u'Levenshtein coding', u'Line spectral pairs', u'Linear predictive coding', u'Log area ratio', u'Lossless compression', u'Lossy compression', u'Macroblock', u'Modified Huffman coding', u'Modified discrete cosine transform', u'Motion compensation', u'Move-to-front transform', u'N-gram', u'Natural language', u'Nyquist\u2013Shannon sampling theorem', u'PAQ', u'Peak signal-to-noise ratio', u'Pixel', u'Prediction', u'Pseudocount', u'Psychoacoustics', u'PubMed Identifier', u'Pyramid (image processing)', u'Quantization (image processing)', u'Quantization (signal processing)', u'Random Access Memory', u'Range encoding', u'Ranking', u'Rate\u2013distortion theory', u'Redundancy (information theory)', u'Run-length encoding', u'Sampling (signal processing)', u'Set partitioning in hierarchical trees', u'Shannon coding', u'Shannon\u2013Fano coding', u'Shannon\u2013Fano\u2013Elias coding', u'Sound quality', u'Speech coding', u'Standard test image', u'Statistical Lempel\u2013Ziv', u'Statistics', u'Sub-band coding', u'Symbol', u'Timeline of information theory', u'Tunstall coding', u'Unary coding', u'Universal code (data compression)', u'Variable bitrate', u'Video', u'Video codec', u'Video compression', u'Video compression picture types', u'Video quality', u'Warped linear predictive coding', u'Wavelet compression', u'Zero-frequency problem', u'\u039c-law algorithm']"
Package-merge algorithm,"The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code for a given distribution on a given alphabet of size n, where no code word is longer than L. It is a greedy algorithm, and a generalization of Huffman's original algorithm. Package-merge works by reducing the code construction problem to the binary coin collector's problem.","[u'Coding theory', u'Lossless compression algorithms']","[u'ArXiv', u'Big O notation', u'Canonical Huffman code', u'Code word', u'Dan Hirschberg', u'Data compression', u'Digital object identifier', u'Graph theory', u'Greedy algorithm', u'Huffman coding', u'IEEE Transactions on Communications', u'Lawrence L. Larmore', u'Numismatics', u'SIAM Journal on Computing']"
Packrat parser,"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s. Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.
Unlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be parsed by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages, but not natural languages where their performance is comparable to general CFG algorithms such as the Earley algorithm.","[u'All accuracy disputes', u'All articles with unsourced statements', u'Articles with disputed statements from July 2014', u'Articles with unsourced statements from November 2011', u'Formal languages', u'Parsing algorithms', u'Wikipedia external links cleanup from September 2011', u'Wikipedia spam cleanup from September 2011']","[u'Addison-Wesley Longman', u'Ambiguous', u'Ambiguous grammar', u'Association for Computing Machinery', u'Backtracking', u'Bellman\u2013Ford algorithm', u'Boolean grammar', u'CYK algorithm', u'Circular definition', u'Commutativity', u'Comparison of parser generators', u'Computer science', u'Constructed language', u'Context-free grammar', u'Context-free grammars', u'Context-free language', u'Cut (logic programming)', u'Dangling else', u'Digital object identifier', u'Earley algorithm', u'Exponential time', u'Expression (mathematics)', u'Floyd\u2013Warshall algorithm', u'Formal grammar', u'Formal language', u'Function (mathematics)', u'GLR parser', u'Graph algorithms', u'Greedy algorithm', u'International Standard Book Number', u'LL parser', u'LR parser', u'Left recursion', u'Linear time', u'Logic programming', u'Lojban', u'Memoization', u'Mutual recursion', u'Natural language', u'Nonterminal symbol', u'OMeta', u'Parse tree', u'Parser combinators', u'Parsing', u'Portable Document Format', u'Recursion', u'Recursive descent parser', u'Regular expression', u'Regular expressions', u'Roberto Ierusalimschy', u'String (computer science)', u'Syntactic predicate', u'Terminal symbol', u'Tokenization (lexical analysis)', u'Top-down parsing language']"
PageRank,"PageRank is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google:

PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.

It is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known.","[u'All Wikipedia articles in need of updating', u'All articles lacking reliable references', u'All articles with unsourced statements', u'American inventions', u'Articles lacking reliable references from October 2012', u'Articles with example MATLAB/Octave code', u'Articles with inconsistent citation formats', u'Articles with unsourced statements from June 2013', u'Articles with unsourced statements from October 2015', u'Crowdsourcing', u'Google Search', u'Internet search algorithms', u'Link analysis', u'Markov models', u'Pages containing cite templates with deprecated parameters', u'Reputation management', u'Search engine optimization', u'Wikipedia articles in need of updating from February 2014']","[u'111 Eighth Avenue', u'AI Challenge', u'Aardvark (search engine)', u'AdMob', u'AdSense', u'AdWords', u'Adjacency matrix', u'Adscape', u'Al Gore', u'Alan Eustace', u'Alan Mulally', u'Algorithm', u'Alphabet Inc.', u'Amit Singhal', u'Analytic Hierarchy Process', u'Android (operating system)', u'Android Auto', u'Android Pay', u'Android TV', u'Android Wear', u'Android software development', u'Android version history', u'Ann Mather', u'Apache Wave', u'App Inventor for Android', u'ArXiv', u'Ask.com', u'AtGoogleTalks', u'Author Rank', u'Baidu', u'Bibcode', u'BigTable', u'Blog', u'Blogger (service)', u'Blogosphere', u'CLEVER project', u'Caja project', u'Calico (company)', u'Censorship by Google', u'CheiRank', u'Chrome OS', u'Chrome Web Store', u'Chrome Zone', u'Chromebit', u'Chromebook', u'Chromebox', u'Chromecast', u'Citation analysis', u'Cnn.com', u'Coverage of Google Street View', u'Criticism of Google', u'Dart (programming language)', u'David Drummond (Google)', u'Digital object identifier', u'Distributed algorithms', u'Dodgeball (service)', u""Don't be evil"", u'DoubleClick', u'DoubleClick for Publishers', u'EigenTrust', u'Eigenfactor', u'Eigenvalue', u'Eigenvector', u'Eigenvector centrality', u'Eric Schmidt', u'Eugene Garfield', u'Expected value', u'FeedBurner', u'GData', u'GNU Octave', u'GOOG-411', u'Game the system', u'Gears (software)', u'Gmail', u'Gmail interface', u'Go (programming language)', u'Google', u'Google+', u'Google.org', u'Google APIs', u'Google Account', u'Google Alerts', u'Google Analytics', u'Google Answers', u'Google App Engine', u'Google Apps Marketplace', u'Google Apps Script', u'Google Apps for Work', u'Google Art Project', u'Google Audio Indexing', u'Google Authenticator', u'Google Base', u'Google Blog Search', u'Google Bookmarks', u'Google Books', u'Google Books Library Project', u'Google Browser Sync', u'Google Business Groups', u'Google Buzz', u'Google Calendar', u'Google Cardboard', u'Google Checkout', u'Google China', u'Google Chrome', u'Google Chrome Apps', u'Google Chrome Experiments', u'Google Chrome Frame', u'Google Chrome extension', u'Google Chrome for Android', u'Google Chrome for iOS', u'Google Classroom', u'Google Closure Tools', u'Google Cloud Connect', u'Google Cloud Print', u'Google Code-in', u'Google Code Jam', u'Google Code Search', u'Google Compute Engine', u'Google Contact Lens', u'Google Contacts', u'Google Contributor', u'Google Current', u'Google Currents', u'Google Custom Search', u'Google Data Liberation Front', u'Google Desktop', u'Google Developer Day', u'Google Developer Expert', u'Google Developers', u'Google Dictionary', u'Google Directory', u'Google Docs, Sheets, and Slides', u'Google Domains', u'Google Doodle', u'Google Drive', u'Google Earth', u'Google Earth Engine', u'Google Earth Outreach', u'Google Express', u'Google Fast Flip', u'Google Fiber', u'Google File System', u'Google Finance', u'Google Fit', u'Google Flights', u'Google Friend Connect', u'Google Gadgets', u'Google Gadgets API', u'Google Glass', u'Google Goggles', u'Google Grants', u'Google Groups', u'Google Guava', u'Google Guice', u'Google Hangouts', u'Google Health', u'Google Highly Open Participation Contest', u'Google Hummingbird', u'Google I/O', u'Google IME', u'Google Image Labeler', u'Google Images', u'Google Inc.', u'Google Insights for Search', u'Google Japanese Input', u'Google Keep', u'Google Kythe', u'Google Labs', u'Google Latitude', u'Google Lively', u'Google Lunar X Prize', u'Google Map Maker', u'Google Maps', u'Google Mars', u'Google Mashup Editor', u'Google Moon', u'Google My Maps', u'Google Native Client', u'Google News', u'Google News & Weather', u'Google News Archive', u'Google Nexus', u'Google Notebook', u'Google Now', u'Google Offers', u'Google Pack', u'Google Page Creator', u'Google Panda', u'Google Partners', u'Google Patents', u'Google Penguin', u'Google Personalized Search', u'Google Photos', u'Google Pinyin', u'Google Places', u'Google Play', u'Google Play Books', u'Google Play Games', u'Google Play Movies & TV', u'Google Play Music', u'Google Play Newsstand', u'Google PowerMeter', u'Google Public DNS', u'Google Questions and Answers', u'Google Reader', u'Google Real-Time Search', u'Google Scholar', u'Google Science Fair', u'Google Script Converter', u'Google Search', u'Google SearchWiki', u'Google Search Appliance', u'Google Search Console', u'Google Searchology', u'Google Shopping', u'Google Sidewiki', u'Google Sites', u'Google Sky', u'Google Squared', u'Google Storage', u'Google Street View', u'Google Street View privacy concerns', u'Google Summer of Code', u'Google Swiffy', u'Google Sync', u'Google TV', u'Google Takeout', u'Google Talk', u'Google Text-to-Speech', u'Google Toolbar', u'Google Translate', u'Google Trends', u'Google Ventures', u'Google Video Marketplace', u'Google Videos', u'Google Voice', u'Google Voice Search', u'Google Wallet', u'Google Web Accelerator', u'Google Web History', u'Google Web Toolkit', u'Google Website Optimizer', u'Google WiFi', u'Google X', u'Google bomb', u'Google driverless car', u'Google for Work', u'Google logo', u'Google matrix', u'Google platform', u'Google search', u'Google search algorithm', u'Google transliteration', u'Googlebot', u'Googleplex', u'Googlization', u'Goojje', u'Graph (data structure)', u'HITS algorithm', u'HTML attribute', u'HTTP 302', u'Hilltop algorithm', u'History of Gmail', u'History of Google', u'Hyper Search', u'Hyperlink', u'IGoogle', u'Identity matrix', u'Impact factor', u'Inbox by Gmail', u'Incoming link', u'Institute for Scientific Information', u'International Standard Book Number', u'International Standard Serial Number', u'Jaiku', u'Jeff Dean (computer scientist)', u'John Doerr', u'John L. Hennessy', u'Jon Kleinberg', u'Keyhole Markup Language', u'Knol', u'Knowledge Graph', u'Knowledge Vault', u'Krishna Bharat', u'Larry Page', u'Lexical semantics', u'Link farm', u'Link love', u'List of Google Doodles (1998\u20132009)', u'List of Google Doodles in 2010', u'List of Google Doodles in 2011', u'List of Google Doodles in 2012', u'List of Google Doodles in 2013', u'List of Google Doodles in 2014', u'List of Google Doodles in 2015', u'List of Google domains', u'List of Google easter eggs', u'List of Google products', u'List of mergers and acquisitions by Google', u'List of street view services', u'Logarithmic scale', u'MATLAB', u'Made with Code', u'MapReduce', u'Markov chain', u'Markov process', u'Massimo Marchiori', u'Material Design', u'Matt Cutts', u'Mediabot', u'Meta tag', u'Methods of website linking', u'Monopoly City Streets', u'Motorola Mobility', u'Network theory', u'New Straits Times', u'Nofollow', u'Omid Kordestani', u'OpenRefine', u'OpenSocial', u'Orkut', u'Outline of Google', u'Panoramio', u'Patrick Pichette', u'Paul Otellini', u'Perron\u2013Frobenius theorem', u'Picasa', u'Picasa Web Albums', u'Picnik', u'Power iteration', u'Power method', u'Probability distribution', u'Project Ara', u'Project Loon', u'Project Sunroof', u'Project Tango', u'PubMed Identifier', u'Rachel Whetstone', u'Rajeev Motwani', u'Rajen Sheth', u'Ram Shriram', u'Rand Fishkin', u'RankDex', u'Ray Kurzweil', u'Reciprocal link', u'Recursion', u'Robin Li', u'Ruth Porat', u'SCImago', u'SEO', u'SafeSearch', u'Salar Kamangar', u'Scale-free network', u'Scientometrics', u'Search Engine Optimization Metrics', u'Search engine optimization', u'Search engine results page', u'Semantic link', u'Semantic similarity', u'Seomoz.org', u'Sergey Brin', u'Set (computer science)', u'Shirley M. Tilghman', u'SimRank', u'Sitemaps', u'Slide.com', u'Software patent', u'Spam in blogs', u'Spamdexing', u'Stanford University', u'Steady state', u'Stochastic matrix', u'Sundar Pichai', u'Susan Wojcicki', u'Swiftype', u'Synsets', u'Teoma', u'Terry Winograd', u'Thomas Saaty', u'Timeline of Google Search', u'Topic-Sensitive PageRank', u'Trade secret', u'TrustRank', u'Twitter', u'Uniform Resource Locator', u'United States dollar', u'Unity (cable system)', u'Urchin (software)', u'Urs H\xf6lzle', u'Usa.gov', u'Vevo', u'Vint Cerf', u'VisualRank', u'Wayback Machine', u'Web crawler', u'Web page', u'Webgraph', u'Website spoofing', u'Weighting', u'Wikipedia', u'WordNet', u'Word Sense Disambiguation', u'World Wide Web', u'YouTube', u'Zagat', u'ZygoteBody']"
Page replacement algorithms,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated. Paging happens when a page fault occurs and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.
When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.
The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known.","[u'All Wikipedia articles needing clarification', u'All articles with unsourced statements', u'Articles with unsourced statements from June 2008', u'Memory management algorithms', u'Online algorithms', u'Use dmy dates from August 2012', u'Virtual memory', u'Wikipedia articles needing clarification from August 2011', u'Wikipedia articles needing clarification from January 2014']","[u'ARM architecture', u'Adaptive Replacement Cache', u'Adaptive replacement cache', u'Amortized analysis', u""B\xe9l\xe1dy's anomaly"", u'Cache algorithms', u'Clairvoyance', u'Computer', u'Demand paging', u'Digital object identifier', u'Dirty bit', u'FreeBSD', u'Full duplex', u'Garbage collection (computer science)', u'Hash table', u'Intel i860', u'Journaling file system', u'Kernel (computer science)', u'Linux', u'Linux kernel', u'Locality of reference', u'L\xe1szl\xf3 B\xe9l\xe1dy', u'Memory management', u'Memory management (operating systems)', u'OS/390', u'Object-oriented programming', u'Online algorithm', u'Online problem', u'OpenVMS', u'Operating system', u'Page fault', u'Page replacement algorithm', u'Paging', u'Prefetch input queue', u'Scheduling (computing)', u'Solaris (operating system)', u'The LIRS caching algorithm', u'Tree data structure', u'VAX', u'VAX/VMS', u'Virtual memory', u'Working set']"
Painter's algorithm,"The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics. When projecting a 3D scene onto a 2D plane, it is necessary at some point to decide which polygons are visible, and which are hidden.
The name ""painter's algorithm"" refers to the technique employed by many painters of painting distant parts of a scene before parts which are nearer thereby covering some areas of distant parts. The painter's algorithm sorts all the polygons in a scene by their depth and then paints them in this order, farthest to closest. It will paint over the parts that are normally not visible — thus solving the visibility problem — at the cost of having painted invisible areas of distant objects. The ordering used by the algorithm is called a 'depth order', and does not have to respect the numerical distances to the parts of the scene: the essential property of this ordering is, rather, that if one object obscures part of another then the first object is painted after the object that it obscures. Thus, a valid ordering can be described as a topological ordering of a directed acyclic graph representing occlusions between objects.

The algorithm can fail in some cases, including cyclic overlap or piercing polygons. In the case of cyclic overlap, as shown in the figure to the right, Polygons A, B, and C overlap each other in such a way that it is impossible to determine which polygon is above the others. In this case, the offending polygons must be cut to allow sorting. Newell's algorithm, proposed in 1972, provides a method for cutting such polygons. Numerous methods have also been proposed in the field of computational geometry.
The case of piercing polygons arises when one polygon intersects another. As with cyclic overlap, this problem may be resolved by cutting the offending polygons.
In basic implementations, the painter's algorithm can be inefficient. It forces the system to render each point on every polygon in the visible set, even if that polygon is occluded in the finished scene. This means that, for detailed scenes, the painter's algorithm can overly tax the computer hardware.
A reverse painter's algorithm is sometimes used, in which objects nearest to the viewer are painted first — with the rule that paint must never be applied to parts of the image that are already painted (unless they are partially transparent). In a computer graphic system, this can be very efficient, since it is not necessary to calculate the colors (using lighting, texturing and such) for parts of the more distant scene that are hidden by nearby objects. However, the reverse algorithm suffers from many of the same problems as the standard version.
These and other flaws with the algorithm led to the development of Z-buffer techniques, which can be viewed as a development of the painter's algorithm, by resolving depth conflicts on a pixel-by-pixel basis, reducing the need for a depth-based rendering order. Even in such systems, a variant of the painter's algorithm is sometimes employed. As Z-buffer implementations generally rely on fixed-precision depth-buffer registers implemented in hardware, there is scope for visibility problems due to rounding error. These are overlaps or gaps at joins between polygons. To avoid this, some graphics engine implementations ""overrender"", drawing the affected edges of both polygons in the order given by painter's algorithm. This means that some pixels are actually drawn twice (as in the full painter's algorithm) but this happens on only small parts of the image and has a negligible performance effect.","[u'3D computer graphics', u'All articles with unsourced statements', u'Articles with unsourced statements from January 2008', u'Commons category with local link same as on Wikidata', u'Computer graphics algorithms']","[u'3D computer graphics', u'Addison-Wesley', u'Computational geometry', u'Computer Graphics: Principles and Practice', u'Directed acyclic graph', u'Hidden surface determination', u'International Standard Book Number', u'James D. Foley', u'John F. Hughes (computer scientist)', u""Newell's algorithm"", u'Polygon', u'Rendering (computer graphics)', u""Schlemiel the Painter's algorithm"", u'Topological ordering', u'Visibility problem', u'Z-buffering']"
Pancake sorting,"Pancake sorting is the colloquial term for the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it. A pancake number is the maximum number of flips required for a given number of pancakes. In this form, the problem was first discussed by American geometer Jacob E. Goodman. It is a variation of the sorting problem in which the only allowed operation is to reverse the elements of some prefix of the sequence. Unlike a traditional sorting algorithm, which attempts to sort with the fewest comparisons possible, the goal is to sort the sequence in as few reversals as possible. A variant of the problem is concerned with burnt pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.

","[u'Articles with inconsistent citation formats', u'Sorting algorithms', u'Use mdy dates from October 2014']","[u'Adaptive sort', u'American flag sort', u'ArXiv', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Big O notation', u'Bill Gates', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Carnegie Mellon University', u'Cartesian tree', u'Cascade merge sort', u'Christos Papadimitriou', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'DNA computing', u'David X. Cohen', u'Digital object identifier', u'Discrete Mathematics (journal)', u'Eric W. Weisstein', u'Escherichia coli', u'Flashsort', u'Futurama', u'Geometer', u'Gnome sort', u'Hal Sudborough', u'Harvard University', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'Introsort', u'JSort', u'Jacob E. Goodman', u'Library sort', u'List (computing)', u'Manuel Blum', u'Marek Karpinski', u'MathWorld', u'Mathematical Reviews', u'Merge sort', u'Microsoft', u'NP-complete', u'NP-hard', u'Neil Sloane', u'Odd\u2013even sort', u'On-Line Encyclopedia of Integer Sequences', u'Oscillating merge sort', u'Pairwise sorting network', u'Patience sorting', u'Permutation', u'Pigeonhole sort', u'Polyphase merge sort', u'Prefix (computer science)', u'Proxmap sort', u'PubMed Central', u'PubMed Identifier', u'Quicksort', u'Radix sort', u'Robert Tarjan', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Spatula', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'The Guardian', u'Theoretical Computer Science (journal)', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'United States of America', u'University of California, Berkeley', u'University of Texas at Dallas']"
Parsing expression grammar,"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s. Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.
Unlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be parsed by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages, but not natural languages where their performance is comparable to general CFG algorithms such as the Earley algorithm.","[u'All accuracy disputes', u'All articles with unsourced statements', u'Articles with disputed statements from July 2014', u'Articles with unsourced statements from November 2011', u'Formal languages', u'Parsing algorithms', u'Wikipedia external links cleanup from September 2011', u'Wikipedia spam cleanup from September 2011']","[u'Addison-Wesley Longman', u'Ambiguous', u'Ambiguous grammar', u'Association for Computing Machinery', u'Backtracking', u'Bellman\u2013Ford algorithm', u'Boolean grammar', u'CYK algorithm', u'Circular definition', u'Commutativity', u'Comparison of parser generators', u'Computer science', u'Constructed language', u'Context-free grammar', u'Context-free grammars', u'Context-free language', u'Cut (logic programming)', u'Dangling else', u'Digital object identifier', u'Earley algorithm', u'Exponential time', u'Expression (mathematics)', u'Floyd\u2013Warshall algorithm', u'Formal grammar', u'Formal language', u'Function (mathematics)', u'GLR parser', u'Graph algorithms', u'Greedy algorithm', u'International Standard Book Number', u'LL parser', u'LR parser', u'Left recursion', u'Linear time', u'Logic programming', u'Lojban', u'Memoization', u'Mutual recursion', u'Natural language', u'Nonterminal symbol', u'OMeta', u'Parse tree', u'Parser combinators', u'Parsing', u'Portable Document Format', u'Recursion', u'Recursive descent parser', u'Regular expression', u'Regular expressions', u'Roberto Ierusalimschy', u'String (computer science)', u'Syntactic predicate', u'Terminal symbol', u'Tokenization (lexical analysis)', u'Top-down parsing language']"
Particle swarm optimization,"In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. PSO optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position but, is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.
PSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart describes many philosophical aspects of PSO and swarm intelligence. An extensive survey of PSO applications is made by Poli.
PSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. More specifically, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods. PSO can therefore also be used on optimization problems that are partially irregular, noisy, change over time, etc.","[u'Evolutionary algorithms', u'Optimization algorithms and methods', u'Pages using citations with format and no URL']","[u'Active matter', u'Agent-based model', u'Agent-based model in biology', u'Allee effect', u'Altitudinal migration', u'Animal migration', u'Animal migration tracking', u'Animal navigation', u'Ant colony optimization algorithms', u'Ant robotics', u'Artificial Ants', u'Artificial Bee Colony Algorithm', u'Bait ball', u'Bat algorithm', u'Bees algorithm', u'Bird migration', u'Boids', u'Candidate solution', u'Cell migration', u'Clustering of self-propelled particles', u'Coded wire tag', u'Collective animal behavior', u'Collective intelligence', u'Collective motion', u'Combinatorial optimization', u'Computer science', u'Computer simulation', u'Constraint satisfaction', u'Convergent sequence', u'Convex programming', u'Crowd simulation', u'Decentralised system', u'Derivative-free optimization', u'Diel vertical migration', u'Differentiable', u'Digital object identifier', u'Empirical', u'Eusociality', u'Feeding frenzy', u'Firefly algorithm', u'Fish migration', u'Fish school', u'Flock (birds)', u'Flocking (behavior)', u'Formula', u'Genetic algorithm', u'Glowworm swarm optimization', u'Gradient', u'Gradient descent', u'Group size measures', u'Herd', u'Herd behavior', u'Homing (biology)', u'Infinite-dimensional optimization', u'Insect migration', u'Integer programming', u'Intelligent Small World Autonomous Robots for Micro-manipulation', u'International Standard Book Number', u'Iterative method', u'James Kennedy (social psychologist)', u'Lepidoptera migration', u'Lessepsian migration', u'Local optimum', u'Mathematical optimization', u'Meta-optimization', u'Metaheuristic', u'Microbial intelligence', u'Microbotics', u'Mixed-species foraging flock', u'Mobbing (animal behavior)', u'Monarch butterfly migration', u'Multi-objective optimization', u'Multi-swarm optimization', u'Multiobjective optimization', u'Mutualism (biology)', u'Natal homing', u'Nonlinear programming', u""Occam's razor"", u'Optimization (mathematics)', u'Pack (canine)', u'Pack hunter', u'Pareto efficiency', u'Particle filter', u'Patterns of self-organization in ants', u'Philopatry', u'Point particle', u'Position (vector)', u'Predator satiation', u'Premature convergence', u'Program correctness', u'PubMed Central', u'PubMed Identifier', u'Quadratic programming', u'Quasi-newton methods', u'Quorum sensing', u'Real number', u'Reverse migration (birds)', u'Riccardo Poli', u'Robust optimization', u'Row vector', u'Russell C. Eberhart', u'Salmon run', u'Sardine run', u'Schools of thought', u'Sea turtle migration', u'Self-propelled particles', u'Shoaling and schooling', u'Social behaviour', u'Sort sol (bird flock)', u'Spatial organization', u'Stigmergy', u'Stochastic programming', u'Swarm (simulation)', u'Swarm behaviour', u'Swarm intelligence', u'Swarm robotics', u'Swarming (honey bee)', u'Swarming (military)', u'Swarming motility', u'Symbrion', u'Symmetry breaking of escaping ants', u'Task allocation and partitioning of social insects', u'Uniform distribution (continuous)', u'Velocity', u'Vicsek model']"
Path-based strong component algorithm,"In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra's version was the first to achieve linear time.","[u'Graph algorithms', u'Graph connectivity']","[u'Algorithmica', u'Array (data type)', u'Depth-first search', u'Digital object identifier', u'Directed graph', u'Edsger Dijkstra', u'Graph theory', u'Kurt Mehlhorn', u'Linear time', u'Mathematical Reviews', u'Stack (data structure)', u'Strongly connected component', u""Tarjan's strongly connected components algorithm""]"
Path tracing,"Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. This integration procedure is repeated for every pixel in the output image. When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), and optically-correct cameras, path tracing can produce still images that are indistinguishable from photographs.
Path tracing naturally simulates many effects that have to be specifically added to other methods (conventional ray tracing or scanline rendering), such as soft shadows, depth of field, motion blur, caustics, ambient occlusion, and indirect lighting. Implementation of a renderer including these effects is correspondingly simpler. An extended version of the algorithm is realized by volumetric path tracing, which considers the light scattering of a scene.
Due to its accuracy and unbiased nature, path tracing is used to generate reference images when testing the quality of other rendering algorithms. In order to get high quality images from path tracing, a large number of rays must be traced to avoid visible noisy artifacts.",[u'Global illumination algorithms'],"[u'Algorithm', u'Ambient occlusion', u'Arithmetic mean', u'Arnold (software)', u'BRDF', u'Bidirectional reflectance distribution function', u'Bidirectional scattering distribution function', u'Blender (software)', u'CPU', u'CUDA', u'Caustic (optics)', u'Chromatic aberration', u'CiteSeer', u'Computer graphics', u'Depth of field', u'Fluorescence', u'GPGPU', u'GPU', u'Global illumination', u'Illuminance', u'Image', u'Image noise', u'Iridescence', u""Lambert's cosine law"", u'Lambertian', u'Leonidas J. Guibas', u'Light scattering', u'Luminance', u'Metropolis light transport', u'Monte Carlo integration', u'Monte Carlo method', u'Motion blur', u'Nvidia', u'Octane Render', u'OpenCL', u'OptiX', u'Pathological (mathematics)', u'Pixel', u'Probability Density Function', u'Radiance', u'Radiosity (computer graphics)', u'Ray tracing (graphics)', u'Rendering (computer graphics)', u'Rendering equation', u'Scanline rendering', u'Shadows', u'Simulate', u'Subsurface scattering', u'Traceroute', u'Tracing (disambiguation)', u'Unbiased rendering', u'Volumetric path tracing']"
Paxos algorithm,"Paxos is a family of protocols for solving consensus in a network of unreliable processors. Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.
Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred B. Schneider. State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely.
The Paxos protocol was first published in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece. It was later published as a journal article in 1998.
The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures. Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proven in a paper by Fischer, Lynch and Paterson), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.
Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica.","[u'All articles with unsourced statements', u'Articles with unsourced statements from September 2009', u'Distributed algorithms', u'Fault-tolerant computer systems']","[u'ACM Symposium on Principles of Distributed Computing', u'Amazon Web Services', u'Apache Mesos', u'Apache ZooKeeper', u'Barbara Liskov', u'BigTable', u'Bitcoin', u'Block chain (database)', u'Byzantine fault tolerance', u'Ceph (software)', u'Chandra\u2013Toueg consensus algorithm', u'Clustrix', u'Communications of the ACM', u'Commutative', u'Consensus (computer science)', u'Cynthia Dwork', u'Dahlia Malkhi', u'Digital object identifier', u'Distributed algorithm', u'Distributed lock manager', u'Fred B. Schneider', u'Gbcast', u'Google Spanner', u'IBM SAN Volume Controller', u'If and only if', u'International Conference on Dependable Systems and Networks', u'Isis2 (programming library)', u'Journal of the Association for Computing Machinery', u'Ken Birman', u'Larry Stockmeyer', u'Lease (computer science)', u'Leslie Lamport', u'NAK (protocol message)', u'Nancy Lynch', u'Neo4j', u'Nutanix', u'Paxi', u'Raft (computer science)', u'State machine replication', u'Storage virtualization', u'Viewstamped replication', u'Virtual synchrony', u'WANdisco', u'XtreemFS']"
Perceptron,"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.
The perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced.","[u'All accuracy disputes', u'Articles with disputed statements from August 2015', u'Articles with disputed statements from June 2014', u'Articles with example Python code', u'Artificial neural networks', u'Classification algorithms']","[u'AI winter', u'Anomaly detection', u'Artificial intelligence', u'Artificial neural network', u'Artificial neuron', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Backpropagation', u'Bayesian network', u'Bernard Widrow', u'Bias-variance dilemma', u'Binary Space Partition', u'Binary classification', u'Binary function', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Cadmium sulfide', u'Canonical correlation analysis', u'Cluster analysis', u'Computational learning theory', u'Conditional random field', u'Convolutional neural network', u'Cornell Aeronautical Laboratory', u'DBSCAN', u'Data mining', u'Decision boundary', u'Decision tree learning', u'Deep learning', u'Delta rule', u'Digital object identifier', u'Dimensionality reduction', u'Dot product', u'Empirical risk minimization', u'Ensemble learning', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Feature vector', u'Feedforward neural network', u'Frank Rosenblatt', u'Gaussian distribution', u'Grammar induction', u'Graphical model', u'Heaviside step function', u'Hidden Markov model', u'Hierarchical clustering', u'History of artificial intelligence', u'Hyperbolic function', u'Hyperplane', u'Hyperplane separation theorem', u'IBM 704', u'Independent component analysis', u'International Standard Book Number', u'JSTOR', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Kernel perceptron', u'Kernel trick', u'Learning to rank', u'Linear classifier', u'Linear discriminant analysis', u'Linear predictor function', u'Linear regression', u'Linearly separable', u'Local outlier factor', u'Logistic regression', u'Machine Learning (journal)', u'Machine learning', u'Marvin Minsky', u'Mean-shift', u'Mehryar Mohri', u'Michael Collins (computational linguist)', u'Multiclass classification', u'Multilayer perceptron', u'Naive Bayes classifier', u'National Diet Library', u'Natural language processing', u'Neural network', u'Non-negative matrix factorization', u'OPTICS algorithm', u'Office of Naval Research', u'Offline learning', u'Online algorithm', u'Online machine learning', u'Overfitting', u'Part-of-speech tagging', u'Perceptron', u'Perceptrons', u'Perceptrons (book)', u'Photocell', u'Potentiometer', u'Principal component analysis', u'Probably approximately correct learning', u'Python (programming language)', u'Random forest', u'Random matrix', u'Ra\xfal Rojas', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Robert Schapire', u'Robustness (computer science)', u'Self-organizing map', u'Semi-supervised learning', u'Seymour Papert', u'Sheffer stroke', u'Sigma-pi unit', u'Statistical classification', u'Statistical learning theory', u'Stephen Grossberg', u'Stochastic gradient descent', u'Structured prediction', u'Supervised classification', u'Supervised learning', u'Support vector machine', u'Syntactic parsing', u'T-distributed stochastic neighbor embedding', u'The New York Times', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Vector space', u'Winnow (algorithm)', u'XOR', u'Yoav Freund']"
Peterson's algorithm,"Peterson's algorithm (AKA Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two processes to share a single-use resource without conflict, using only shared memory for communication. It was formulated by Gary L. Peterson in 1981. While Peterson's original formulation worked with only two processes, the algorithm can be generalized for more than two, as shown below.","[u'All articles with unsourced statements', u'Articles with example C code', u'Articles with unsourced statements from May 2015', u'Concurrency control algorithms']","[u'Algorithm', u'Atomic operation', u'Bounded bypass', u'Compare-and-swap', u'Concurrent programming', u'Critical section', u'DEC Alpha', u""Dekker's algorithm"", u'Eisenberg & McGuire algorithm', u'Gary L. Peterson', u'International Standard Book Number', u""Lamport's bakery algorithm"", u'Liveness', u'Load-link/store-conditional', u'MIPS architecture', u'Mathematical induction', u'Maurice Herlihy', u'Memory barrier', u'Memory ordering', u'Michel Raynal', u'Mutual exclusion', u'Nir Shavit', u'PowerPC', u'Semaphore (programming)', u'Symmetric multiprocessing', u""Szymanski's Algorithm"", u'Test-and-set', u'X86', u'Xbox 360']"
Phonetic algorithm,"A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language; consequently, applying the rules to words in other languages might not give a meaningful result.
They are necessarily complex algorithms with many rules and exceptions, because English spelling and pronunciation is complicated by historical changes in pronunciation and words borrowed from many languages.
Among the best-known phonetic algorithms are:
Soundex, which was developed to encode surnames for use in censuses. Soundex codes are four-character strings composed of a single letter followed by three numbers.
Daitch–Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. Daitch–Mokotoff Soundex codes are strings composed of six numeric digits.
Kölner Phonetik: This is similar to Soundex, but more suitable for German words.
Metaphone, Double Metaphone, and Metaphone 3 which are suitable for use with most English words, not just names. Metaphone algorithms are the basis for many popular spell checkers.
New York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. The result is a string that can be pronounced by the reader without decoding.
Match Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.
Caverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand.","[u'Algorithms and data structures stubs', u'All articles needing additional references', u'All stub articles', u'Articles needing additional references from August 2009', u'Computer science stubs', u'Phonetic algorithms', u'Phonetics stubs', u'Phonology']","[u'Algorithm', u'Approximate string matching', u'Caverphone', u'Clojure', u'Daitch\u2013Mokotoff Soundex', u'Damerau\u2013Levenshtein distance', u'Data structure', u'Dictionary of Algorithms and Data Structures', u'Double Metaphone', u'English language', u'Hamming distance', u'Index (publishing)', u'Language', u'Levenshtein distance', u'Loanword', u'Match Rating Approach', u'Metaphone', u'National Institute of Standards and Technology', u'New York State Identification and Intelligence System', u'Phonemes', u'Phonetics', u'Pronunciation', u'R (programming language)', u'Scala programming language', u'Search engine technology', u'Soundex', u'Spell checkers', u'Spelling', u'Word']"
Photon mapping,"In computer graphics, photon mapping is a two-pass global illumination algorithm developed by Henrik Wann Jensen that approximately solves the rendering equation. Rays from the light source and rays from the camera are traced independently until some termination criterion is met, then they are connected in a second step to produce a radiance value. It is used to realistically simulate the interaction of light with different objects. Specifically, it is capable of simulating the refraction of light through a transparent substance such as glass or water, diffuse interreflection between illuminated objects, the subsurface scattering of light in translucent materials, and some of the effects caused by particulate matter such as smoke or water vapor. It can also be extended to more accurate simulations of light such as spectral rendering.
Unlike path tracing, bidirectional path tracing, volumetric path tracing and Metropolis light transport, photon mapping is a ""biased"" rendering algorithm, which means that averaging many renders using this method does not converge to a correct solution to the rendering equation. However, since it is a consistent method, any desired accuracy can be achieved by increasing the number of photons.","[u'3D computer graphics', u'Global illumination algorithms', u'Infographics']","[u'Bidirectional reflectance distribution function', u'Bidirectional surface scattering reflectance distribution function', u'Caustic (optics)', u'Color bleeding (computer graphics)', u'Computer graphics', u'Diffuse interreflection', u'Global illumination', u'Henrik Wann Jensen', u'Irradiance', u'Irradiance caching', u'K-nearest neighbor algorithm', u'Kd-tree', u'Lambertian', u'Light', u'Metropolis light transport', u'Monte Carlo method', u'Path tracing', u'Radiance', u'Radiosity (computer graphics)', u'Ray tracing (graphics)', u'Refraction', u'Rendering equation', u'Scanline rendering', u'Spectral rendering', u'Specular reflection', u'Subsurface scattering', u'Volumetric path tracing', u'Worcester Polytechnic Institute']"
Pigeonhole sort,"Pigeonhole sorting, also known as count sort (not to be confused with counting sort), is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the number of possible key values (N) are approximately the same. It requires O(n + N) time.
The pigeonhole algorithm works as follows:
Given an array of values to be sorted, set up an auxiliary array of initially empty ""pigeonholes,"" one pigeonhole for each key through the range of the original array.
Going over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.
Iterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array.","[u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole principle', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Range (computer science)', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Pohlig–Hellman algorithm,"In number theory, the Pohlig–Hellman algorithm sometimes credited as the Silver–Pohlig–Hellman algorithm is a special-purpose algorithm for computing discrete logarithms in a multiplicative group whose order is a smooth integer.
The algorithm was discovered by Roland Silver, but first published by Stephen Pohlig and Martin Hellman (independent of Silver).
We will explain the algorithm as it applies to the group Z*p consisting of all the elements of Zp which are coprime to p, and leave it to the advanced reader to extend the algorithm to other groups by using Lagrange's theorem.
Input Integers p, g, e.
Output An Integer x, such that e ≡ gx (mod p) (if one exists).

Determine the prime factorization of the order of the group  :
(All the pi are considered small since the group order is smooth.)
From the Chinese remainder theorem it will be sufficient to determine the values of x modulo each prime power dividing the group order. Suppose for illustration that p1 divides this order but p12 does not. Then we need to determine x mod p1, that is, we need to know the ending coefficient b1 in the base-p1 expansion of x, i.e. in the expansion x = a1 p1 + b1. We can find the value of b1 by examining all the possible values between 0 and p1-1. (We may also use a faster algorithm such as baby-step giant-step when the order of the group is prime.) The key behind the examination is that:

(using Euler's theorem). With everything else now known, we may try each value of b1 to see which makes the equation be true. If , then there is exactly one b1, and that b1 is the value of x modulo p1. (An exception arises if  since then the order of g is less than φ(p). The conclusion in this case depends on the value of  on the left: if this quantity is not 1, then no solution x exists; if instead this quantity is also equal to 1, there will be more than one solution for x less than φ(p), but since we are attempting to return only one solution x, we may use b1=0.)
The same operation is now performed for p2 through pn.
A minor modification is needed where a prime number is repeated. Suppose we are seeing pi for the (k + 1)st time. Then we already know ci in the equation x = ai pik+1 + bi pik + ci, and we find either bi or ci the same way as before, depending on whether .
With all the bi known, we have enough simultaneous congruences to determine x using the Chinese remainder theorem.",[u'Number theoretic algorithms'],"[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alfred Menezes', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'CRC Press', u'Chakravala method', u'Chinese remainder theorem', u""Cipolla's algorithm"", u'Congruence relation', u'Continued fraction factorization', u'Coprime', u""Cornacchia's algorithm"", u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u""Euler's theorem"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'IEEE', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Karatsuba algorithm', u""Lagrange's theorem (group theory)"", u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Martin Hellman', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Multiplicative group', u'Number theory', u'Paul van Oorschot', u""Pocklington's algorithm"", u'Pocklington primality test', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Scott Vanstone', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth integer', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Stephen Pohlig', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Pollard's kangaroo algorithm,"In computational number theory and computational algebra, Pollard's kangaroo algorithm (aka Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem. The algorithm was introduced in 1978 by the number theorist J. M. Pollard, in the same paper  as his better-known ρ algorithm for solving the same problem. Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime p, it is in fact a generic discrete logarithm algorithm—it will work in any finite cyclic group.

","[u'Computer algebra', u'Logarithms', u'Number theoretic algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'Binary digit', u'Chakravala method', u""Cipolla's algorithm"", u'Computational algebra', u'Computational complexity theory', u'Computational number theory', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Exponential time', u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Greek letter', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'John Pollard (mathematician)', u'Kangaroo', u'Karatsuba algorithm', u'Lambda', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u'Pseudorandom', u'Public key cryptosystem', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'RSA (algorithm)', u'Rainbow table', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Scientific American', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Subexponential time', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Treadmill', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Pollard's p − 1 algorithm,"Pollard's p − 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974. It is a special-purpose algorithm, meaning that it is only suitable for integers with specific types of factors; it is the simplest example of an algebraic-group factorisation algorithm.
The factors it finds are ones for which the number preceding the factor, p − 1, is powersmooth; the essential observation is that, by working in the multiplicative group modulo a composite number N, we are also working in the multiplicative groups modulo all of N's factors.
The existence of this algorithm leads to the concept of safe primes, being primes for which p − 1 is two times a Sophie Germain prime q and thus minimally smooth. These primes are sometimes construed as ""safe for cryptographic purposes"", but they might be unsafe — in current recommendations for cryptographic strong primes (e.g. ANSI X9.31), it is necessary but not sufficient that p − 1 has at least one large prime factor. Most sufficiently large primes are strong; if a prime used for cryptographic purposes turns out to be non-strong, it is much more likely to be through malice than through an accident of random number generation. This terminology is considered obsolete by the cryptography industry. [1]",[u'Integer factorization algorithms'],"[u'AKS primality test', u'ANSI X9.31', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algebraic-group factorisation algorithm', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Digital object identifier', u'Discrete logarithm', u""Dixon's factorization method"", u""Dixon's theorem"", u'Elliptic curve method', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u""Fermat's little theorem"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Great Internet Mersenne Prime Search', u'Greatest common divisor', u'Index calculus algorithm', u'Integer', u'Integer factorization', u'Integer square root', u'John Pollard (mathematician)', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MPrime', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Multiplication algorithm', u'Natural logarithm', u'Necessary but not sufficient', u'Number theory', u'Obsolete', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u'Prime95', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Random number generation', u'Rational sieve', u'Safe prime', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Solovay\u2013Strassen primality test', u'Sophie Germain prime', u'Special number field sieve', u'Strong prime', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Pollard's rho algorithm,Pollard's rho algorithm is a special-purpose integer factorization algorithm. It was invented by John Pollard in 1975. It is particularly effective for a composite number having a small prime factor.,[u'Integer factorization algorithms'],"[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'Birthday paradox', u'Birthday problem', u'Chakravala method', u'Charles E. Leiserson', u""Cipolla's algorithm"", u'Clifford Stein', u'Composite number', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cycle detection', u'Digital object identifier', u'Directed graph', u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Eric W. Weisstein', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat number', u'Fermat primality test', u""Floyd's cycle-finding algorithm"", u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Introduction to Algorithms', u'John Pollard (mathematician)', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MathWorld', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u'Pseudo-random sequence', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u'Richard Brent (scientist)', u'Ronald L. Rivest', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Thomas H. Cormen', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'UNIVAC', u'UNIVAC 1110', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Pollard's rho algorithm for logarithms,"Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem.
The goal is to compute  such that , where  belongs to a cyclic group  generated by . The algorithm computes integers , , , and  such that . Assuming, for simplicity, that the underlying group is cyclic of order , we can calculate  as a solution of the equation .
To find the needed , , , and  the algorithm uses Floyd's cycle-finding algorithm to find a cycle in the sequence , where the function  is assumed to be random-looking and thus is likely to enter into a loop after approximately  steps. One way to define such a function is to use the following rules: Divide  into three disjoint subsets of approximately equal size: , , and . If  is in  then double both  and ; if  then increment , if  then increment .","[u'Logarithms', u'Number theoretic algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'C++', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cyclic group', u'Digital object identifier', u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u""Floyd's cycle-finding algorithm"", u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'John Pollard (mathematician)', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Mathematics of Computation', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Polynomial time,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm.","[u'All articles with unsourced statements', u'Analysis of algorithms', u'Articles with unsourced statements from July 2014', u'Computational complexity theory', u'Computational resources', u'Use dmy dates from September 2010']","[u'2-EXPTIME', u'3SAT', u'AKS primality test', u'Abstract machine', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alan Cobham', u'Alexander Schrijver', u'Algorithm', u'Amortized time', u'Approximation algorithm', u'Approximation algorithms', u'ArXiv', u'Array data structure', u'Average-case complexity', u'Avi Wigderson', u'BPP (complexity)', u'BQP', u'Big O notation', u'Binary numeral system', u'Binary search', u'Binary search algorithm', u'Binary tree', u'Binary tree sort', u'Bounded-error probabilistic polynomial', u'Boyer\u2013Moore string search algorithm', u'Brute-force search', u'Bubble sort', u'Christos H. Papadimitriou', u""Cobham's thesis"", u'Cole-Vishkin algorithm', u'Comparison sort', u'Complexity Zoo', u'Complexity class', u'Computable function', u'Computation model', u'Computational complexity of mathematical operations', u'Computational complexity theory', u'Computer science', u'Conjunctive normal form', u'Content-addressable memory', u'Convolution theorem', u'DLOGTIME', u'DTIME', u'Decision problem', u'Deterministic Turing machine', u'Digital object identifier', u'Disjoint set data structure', u'Double exponential function', u'Dynamic programming', u'EXP', u'EXPTIME', u'E (complexity)', u'Element (math)', u'Elsevier', u'Euclidean algorithm', u'Exponential time hypothesis', u'Fast Fourier transform', u'Formal language', u'General number field sieve', u'Graph (mathematics)', u'Graph isomorphism problem', u'Greatest common divisor', u""Grover's algorithm"", u'Gr\xf6bner basis', u'Heapsort', u'In-place merge sort', u'Infra-exponential', u'Insertion sort', u'Instruction (computer science)', u'Integer factorization', u'International Standard Book Number', u'International Standard Serial Number', u'Introsort', u'Inverse Ackermann function', u'Iterated logarithm', u'Journal of Computer and System Sciences', u""Karmarkar's algorithm"", u'Kd-tree', u'L-notation', u'Lance Fortnow', u'Lecture Notes in Computer Science', u'Linear', u'Linear programming', u'Logarithm', u'Logarithmic growth', u'Logarithmic identities', u'L\xe1szl\xf3 Babai', u'L\xe1szl\xf3 Lov\xe1sz', u'Matrix chain multiplication', u'Maximum matching', u'Merge sort', u'Michael Sipser', u'Monge array', u'NP-complete', u'NP-hard', u'NP (complexity)', u'Noam Nisan', u'Non-deterministic Turing machine', u'Optimization (mathematics)', u'P (complexity)', u'P versus NP', u'P versus NP problem', u'P \u2260 NP', u'Parallel Random Access Machine', u'Parallel algorithm', u'Parallel computing', u'Parameterized complexity', u'Partial correlation', u'Patience sorting', u'Polygon triangulation', u'Polynomial expression', u'Presburger arithmetic', u'Priority queue', u'Probabilistic Turing machine', u'Product (mathematics)', u'Property testing', u'Pseudo-polynomial time', u'Quantifier elimination', u'Quantum Turing machine', u'Quantum algorithm', u'Quasilinear time', u'Quicksort', u'RP (complexity)', u'Raimund Seidel', u'Randomized algorithm', u'Real closed field', u'Recurrence relation', u'Reduction (complexity)', u'Repeated squaring', u'Robustness (computer science)', u'Running Time (film)', u'Running time', u'Russell Impagliazzo', u'Self-balancing binary search tree', u'Set cover', u'Shell sort', u'Smoothsort', u'Society for Industrial and Applied Mathematics', u'Sorting algorithm', u'Space complexity', u'Springer-Verlag', u'Steiner tree problem', u""Stirling's approximation"", u'String (computer science)', u'Traveling salesman problem', u'Travelling salesman problem', u'Turing machine', u""Ukkonen's Algorithm"", u'Upper bound', u'Worst-case complexity', u'ZPP (complexity)']"
Postman sort,"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, and is a cousin of radix sort in the most to least significant digit flavour. Bucket sort is a generalization of pigeonhole sort. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.
Bucket sort works as follows:
Set up an array of initially empty ""buckets"".
Scatter: Go over the original array, putting each object in its bucket.
Sort each non-empty bucket.
Gather: Visit the buckets in order and put all elements back into the original array.","[u'All articles needing expert attention', u'Articles needing expert attention from November 2008', u'Articles needing expert attention with no reason or talk parameter', u'Articles with example pseudocode', u'Computer science articles needing expert attention', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'American flag sort', u'Analysis of algorithms', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket (computing)', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Dictionary of Algorithms and Data Structures', u'Distribution sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introduction to Algorithms', u'Introsort', u'JSort', u'J sort', u'Library sort', u'List (computing)', u'Merge sort', u'Mergesort', u'National Institute of Standards and Technology', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Post office', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Ronald L. Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Thomas H. Cormen', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Prim's algorithm,"In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.
The algorithm was developed in 1930 by Czech mathematician Vojtěch Jarník and later rediscovered and republished by computer scientists Robert C. Prim in 1957 and Edsger W. Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm, Jarník's algorithm, the Prim–Jarník algorithm, or the Prim–Dijkstra algorithm.
Other well-known algorithms for this problem include Kruskal's algorithm and Borůvka's algorithm. These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest. In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms. However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.

","[u'Articles containing proofs', u'Articles containing video clips', u'CS1 Czech-language sources (cs)', u'Graph algorithms', u'Spanning tree']","[u'A* search algorithm', u'Adjacency list', u'Adjacency matrix', u'Alpha\u2013beta pruning', u'Array data structure', u'Asymptotic computational complexity', u'B*', u'Backtracking', u'Beam search', u'Bell System Technical Journal', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u'Big-O notation', u'Binary heap', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Computer science', u'Computer scientist', u'Connected component (graph theory)', u'Czech people', u'D*', u'D-ary heap', u'David Cheriton', u'Dense graph', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u""Dijkstra's algorithm"", u'Donald B. Johnson', u'Dynamic programming', u'Edge (graph theory)', u""Edmonds' algorithm"", u'Edsger W. Dijkstra', u'Fibonacci heap', u'Flag value', u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Graph theory', u'Graph traversal', u'Greedy algorithm', u'Grid graph', u'Heap (data structure)', u'Hill climbing', u'International Standard Book Number', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lexicographic breadth-first search', u'Linear time', u'Linked list', u'List of algorithms', u'Mathematical Reviews', u'Maze generation', u'Minimum spanning tree', u'Priority queue', u'Pseudocode', u'Robert C. Prim', u'Robert Sedgewick (computer scientist)', u'Robert Tarjan', u'SIAM Journal on Computing', u'SMA*', u'Search game', u'Shortest path problem', u'Society for Industrial and Applied Mathematics', u'Sparse graph', u'Time complexity', u'Tree (graph theory)', u'Tree traversal', u'Undirected graph', u'Vertex (graph theory)', u'Vojt\u011bch Jarn\xedk', u'Weighted graph']"
Primality test,"A primality test is an algorithm for determining whether an input number is prime. Amongst other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might be called compositeness tests instead of primality tests.","[u'All articles needing additional references', u'All articles with specifically marked weasel-worded phrases', u'Articles needing additional references from August 2013', u'Articles with specifically marked weasel-worded phrases from April 2010', u'Asymmetric-key algorithms', u'Pages containing cite templates with deprecated parameters', u'Primality tests']","[u'AC0', u'AKS algorithm', u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u""Agrawal's conjecture"", u'Algorithm', u'Analytic number theory', u'Ancient Egyptian multiplication', u'ArXiv', u'Baby-step giant-step', u'Baillie-PSW primality test', u'Baillie\u2013PSW primality test', u'Big O notation', u'Binary GCD algorithm', u'Carl Pomerance', u'Carmichael number', u'Chakravala method', u'Charles E. Leiserson', u'Christos Papadimitriou', u""Cipolla's algorithm"", u'Clifford Stein', u'Co-NP', u'Composite number', u'Computational complexity theory', u'Continued fraction factorization', u'Coprime', u""Cornacchia's algorithm"", u'Counterexample', u'Cryptography', u'Deterministic algorithm', u'Digital object identifier', u'Discrete logarithm', u'Divisibility', u'Divisor', u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Eric W. Weisstein', u'Euclidean algorithm', u""Euler's factorization method"", u'Euler pseudoprime', u'Extended Euclidean algorithm', u'Factorization', u""Fermat's factorization method"", u""Fermat's little theorem"", u'Fermat primality test', u'Fibonacci number', u'Frobenius pseudoprime', u'Function field sieve', u""F\xfcrer's algorithm"", u'Gary L. Miller (mathematician)', u'General number field sieve', u'Generalized Riemann hypothesis', u'Generating primes', u'Greatest common divisor', u'Hans Riesel', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Introduction to Algorithms', u'Jacobi symbol', u'Jahrbuch \xfcber die Fortschritte der Mathematik', u'John L. Selfridge', u'John Selfridge', u'Journal of Computer and System Sciences', u'Karatsuba algorithm', u'L (complexity)', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Leonard Adleman', u'Long multiplication', u'Lucas primality test', u'Lucas pseudoprime', u'Lucas sequence', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Manindra Agrawal', u'MathWorld', u'Mathematical Reviews', u'Mathematics', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Multiplication algorithm', u'Multiplicative order', u'NC (complexity)', u'NP (complexity)', u'Neeraj Kayal', u'Nitin Saxena', u'Number theory', u'P-complete', u'P (complexity)', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial time', u'Primality certificate', u'Prime factor', u'Prime number', u'Primitive root modulo n', u'Primorial', u'Probable prime', u""Proth's theorem"", u'Pseudocode', u'Pseudoprime', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Quantum computer', u'Quasi-polynomial time', u'RP (complexity)', u'RSA (algorithm)', u'Randomized algorithm', u'Rational sieve', u'Recursion', u'Remainder', u'Richard Crandall', u'Ronald L. Rivest', u'Run-time complexity', u'Sample space', u'Samuel S. Wagstaff, Jr.', u'Samuel Wagstaff', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Sophie Germain prime', u'Special number field sieve', u'Springer-Verlag', u'Strong pseudoprime', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Vaughan Pratt', u'Wheel factorization', u""Williams' p + 1 algorithm"", u""Wilson's theorem"", u'ZPP (complexity)', u'Zentralblatt MATH']"
Prime-factor FFT algorithm,"The prime-factor algorithm (PFA), also called the Good–Thomas algorithm (1958/1963), is a fast Fourier transform (FFT) algorithm that re-expresses the discrete Fourier transform (DFT) of a size N = N1N2 as a two-dimensional N1×N2 DFT, but only for the case where N1 and N2 are relatively prime. These smaller transforms of size N1 and N2 can then be evaluated by applying PFA recursively or by using some other FFT algorithm.
PFA should not be confused with the mixed-radix generalization of the popular Cooley–Tukey algorithm, which also subdivides a DFT of size N = N1N2 into smaller transforms of size N1 and N2. The latter algorithm can use any factors (not necessarily relatively prime), but it has the disadvantage that it also requires extra multiplications by roots of unity called twiddle factors, in addition to the smaller transforms. On the other hand, PFA has the disadvantages that it only works for relatively prime factors (e.g. it is useless for power-of-two sizes) and that it requires a more complicated re-indexing of the data based on the Chinese remainder theorem (CRT). Note, however, that PFA can be combined with mixed-radix Cooley–Tukey, with the former factorizing N into relatively prime components and the latter handling repeated factors.
PFA is also closely related to the nested Winograd FFT algorithm, where the latter performs the decomposed N1 by N2 transform via more sophisticated two-dimensional convolution techniques. Some older papers therefore also call Winograd's algorithm a PFA FFT.
(Although the PFA is distinct from the Cooley–Tukey algorithm, Good's 1958 work on the PFA was cited as inspiration by Cooley and Tukey in their famous 1965 paper, and there was initially some confusion about whether the two algorithms were different. In fact, it was the only prior FFT work cited by them, as they were not then aware of the earlier research by Gauss and others.)",[u'FFT algorithms'],"[u'Bijection', u""Bluestein's FFT algorithm"", u'Chinese remainder theorem', u'Cooley\u2013Tukey FFT algorithm', u'Digital object identifier', u'Discrete Fourier transform', u'Fast Fourier transform', u'In-place algorithm', u'JSTOR', u'Modular arithmetic', u'Modular multiplicative inverse', u'Power of two', u""Rader's FFT algorithm"", u'Recursion', u'Relatively prime', u'Twiddle factor', u'Winograd FFT algorithm']"
Prime factorization algorithm,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.","[u'Computational hardness assumptions', u'Integer factorization algorithms', u'Unsolved problems in computer science']","[u'AKS primality test', u'Abundant number', u'Achilles number', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Advanced Micro Devices', u'Algebraic-group factorisation algorithms', u'Algebraic number theory', u'Algorithm', u'Aliquot sequence', u'Almost perfect number', u'Amicable numbers', u'Ancient Egyptian multiplication', u'Arithmetic number', u'BQP', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Betrothed numbers', u'Big O notation', u'Binary GCD algorithm', u'Binary search', u'Bit', u'Canonical representation of a positive integer', u'Carl Pomerance', u'Chakravala method', u""Cipolla's algorithm"", u'Co-NP', u'Co-NP-complete', u'Colossally abundant number', u'Complexity class', u'Composite number', u'Computational complexity theory', u'Computer science', u'Congruence of squares', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cryptography', u'David Bressoud', u'Decision problem', u'Deficient number', u'Digital object identifier', u'Discrete logarithm', u'Discriminant of a quadratic form', u'Divisor', u'Divisor function', u""Dixon's algorithm"", u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve', u'Elliptic curve method', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Empty product', u'Equidigital number', u'Erd\u0151s\u2013Nicolas number', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u'Extravagant number', u'FNP (complexity)', u'FP (complexity)', u'Factorization', u'Fast Fourier transform', u""Fermat's factorization method"", u'Fermat primality test', u'Friendly number', u'Frugal number', u'Function field sieve', u'Function problem', u'Fundamental theorem of arithmetic', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generalized Riemann hypothesis', u'Generating primes', u'Generating set of a group', u'Greatest common divisor', u'Group (mathematics)', u'Harmonic divisor number', u'Hemiperfect number', u'Highly abundant number', u'Highly composite number', u'Hyperperfect number', u'Ideal class group', u'Index calculus algorithm', u'Integer factorization records', u'Integer square root', u'International Association for Cryptologic Research', u'International Standard Book Number', u'Karatsuba algorithm', u'Kronecker symbol', u'L-notation', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'List of unsolved problems in computer science', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Manindra Agrawal', u'Mathematical Reviews', u'Mathematics', u'Maurice Kraitchik', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Multiplicative partition', u'Multiply perfect number', u'NP-complete', u'NP-intermediate', u'NP (complexity)', u'Nature (journal)', u'Number theory', u'Opteron', u'P (complexity)', u'Partition (number theory)', u'Perfect number', u'Perfect power', u'Peter Shor', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial time', u'Powerful number', u'Practical number', u'Primality test', u'Prime decomposition', u'Prime decomposition (3-manifold)', u'Prime factor', u'Prime number', u'Primitive abundant number', u'Probabilistic algorithm', u'Pronic number', u""Proth's theorem"", u'Public-key', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic form', u'Quadratic residue', u'Quadratic sieve', u'Quantum computer', u'Quasiperfect number', u'RSA-768', u'RSA (algorithm)', u'RSA number', u'RSA problem', u'Randomized algorithm', u'Rational sieve', u'Regular number', u'Richard Crandall', u'Rough number', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Semiperfect number', u'Semiprime', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Sociable number', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Sphenic number', u'Square-free integer', u'Stan Wagon', u'Sublime number', u'Superabundant number', u'Superior highly composite number', u'Superperfect number', u'Sylow theorems', u'The Art of Computer Programming', u'Time complexity', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'UP (complexity)', u'Unitary divisor', u'Unitary perfect number', u'Untouchable number', u'Unusual number', u'Weird number', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'YouTube']"
Push–relabel algorithm,"In mathematical optimization, the push–relabel algorithm (alternatively, preflow–push algorithm) is an algorithm for computing maximum flows. The name ""push–relabel"" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a ""preflow"" and gradually converts it into a maximum flow by moving flow locally between neighboring vertices using push operations under the guidance of an admissible network maintained by relabel operations. In comparison, the Ford–Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.
The push–relabel algorithm is considered one of the most efficient maximum flow algorithms. The generic algorithm has a strongly polynomial O(V2E) time complexity, which is asymptotically more efficient than the O(VE2) Edmonds–Karp algorithm. Specific variants of the algorithms achieve even lower time complexities. The variant based on the highest label vertex selection rule has O(V2√E) time complexity and is generally regarded as the benchmark for maximum flow algorithms. Subcubic O(VE log (V2/E)) time complexity can be achieved using dynamic trees, although in practice it is less efficient.
The push–relabel algorithm has been extended to compute minimum cost flows. The idea of distance labels has led to a more efficient augmenting path algorithm, which in turn can be incorporated back into the push–relabel algorithm to create a variant with even higher empirical performance.","[u'Graph algorithms', u'Network flow']","[u'Andrew V. Goldberg', u'Breadth-first search', u'C (programming language)', u'Charles E. Leiserson', u'Clifford Stein', u'Digital object identifier', u'Edmonds\u2013Karp algorithm', u'FIFO (computing and electronics)', u'Flow network', u'Ford\u2013Fulkerson algorithm', u'Function (mathematics)', u'International Standard Book Number', u'Introduction to Algorithms', u'Link-cut tree', u'Mathematical optimization', u'Max-flow min-cut theorem', u'Maximum flow', u'Minimum cost flow', u'Potential method', u'Python (programming language)', u'Real number', u'Robert Tarjan', u'Ron Rivest', u'Strongly polynomial', u'Thomas H. Cormen', u'Topological sorting']"
Quadratic sieve,"The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve). It is still the fastest for integers under 100 decimal digits or so, and is considerably simpler than the number field sieve. It is a general-purpose factorization algorithm, meaning that its running time depends solely on the size of the integer to be factored, and not on special structure or properties. It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve.",[u'Integer factorization algorithms'],"[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Aurifeuillian factorization', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Bellcore', u'Binary GCD algorithm', u'Bitset', u'Bitwise operation', u'Block Wiedemann algorithm', u'Carl Pomerance', u'Central processing unit', u'Chakravala method', u""Cipolla's algorithm"", u'Congruence of squares', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Discrete logarithm', u'Division (mathematics)', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u'Fundamental theorem of arithmetic', u""F\xfcrer's algorithm"", u'Gaussian elimination', u'General number field sieve', u'Generating primes', u'Gigabyte', u'Greatest common divisor', u'Index calculus algorithm', u'Integer', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Karatsuba algorithm', u'L-notation', u'Left null space', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Linear algebra', u'Linear dependency', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MIPS-year', u'Magma computer algebra system', u'MasPar', u'Matrix (mathematics)', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Multi-precision', u'Multiplication algorithm', u'Number theory', u'PARI/GP', u'Parallel algorithm', u'Parallel computing', u'Parity (mathematics)', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Pollard rho', u'Polynomial', u'Primality test', u'Prime number', u'Processors', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'RSA-129', u'RSA-130', u'RSA number', u'Rational sieve', u'Real number', u'Richard Crandall', u'SQUFOF', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Semiprime', u""Shanks' square forms factorization"", u'Shanks\u2013Tonelli algorithm', u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Sieve theory', u'Single-precision', u'Smooth number', u'Software for Algebra and Geometry Experimentation', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Square number', u'Telcordia Technologies', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Vector (mathematics)', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'Zero vector']"
Quantum algorithm,"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.
All problems which can be solved on a quantum computer can be solved on a classical computer. In particular, problems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.
The most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task.","[u'All articles with unsourced statements', u'Articles with unsourced statements from December 2014', u'Quantum algorithms', u'Quantum computing', u'Quantum information science', u'Theoretical computer science', u'Use dmy dates from September 2011']","[u'Abelian group', u'Adiabatic quantum computation', u'Algorithm', u'Algorithmic cooling', u'Algorithmica', u'Amplitude amplification', u'Andris Ambainis', u'ArXiv', u'Association for Computing Machinery', u'BQP', u'Bibcode', u'Black-box', u'Black box group', u'Boson', u'Cambridge University Press', u'Cavity quantum electrodynamics', u'Charge qubit', u'Chern-Simons', u'Circuit quantum electrodynamics', u'Classical capacity', u'Clique (graph theory)', u'Cluster state', u'Communications in Mathematical Physics', u'Commutativity', u'Computer', u'Cris Moore', u'Deutsch\u2013Jozsa algorithm', u'Digital object identifier', u'Dihedral group', u'Discrete Fourier transform', u'Discrete logarithm', u'EQP (complexity)', u'Element distinctness problem', u'Entanglement-assisted classical capacity', u'Entanglement-assisted stabilizer formalism', u'Entanglement distillation', u'Exponential sum', u'Flux qubit', u'Fock state', u'Gauss sum', u'General number field sieve', u'Graph isomorphism', u""Grover's algorithm"", u'HOMFLY', u'Hadamard transform', u'Hamiltonian oracle model', u'Hidden subgroup problem', u'IEEE', u'Igor Pak', u'Integer factorization', u'International Journal of Theoretical Physics', u'International Standard Book Number', u'Jones polynomial', u'Kane quantum computer', u'LMS Journal of Computation and Mathematics', u'LOCC', u'Lattice problems', u'Linear optical quantum computing', u'Loss\u2013DiVincenzo quantum computer', u'Measurement', u'NP-complete', u'Nitrogen-vacancy center', u'Nuclear magnetic resonance quantum computer', u'One-way quantum computer', u'Optical lattice', u'Oracle machine', u'P (complexity)', u""Pell's equation"", u'Phase kick-back', u'Phase qubit', u'Physical Review A', u'Physical Review Letters', u'PostBQP', u'Primality test', u'Principal ideal', u'Probability distribution', u'Proceedings of the National Academy of Sciences of the United States of America', u'PubMed Central', u'PubMed Identifier', u'QMA', u'Quantum Fourier transform', u'Quantum Information and Computation', u'Quantum Turing machine', u'Quantum annealing', u'Quantum capacity', u'Quantum channel', u'Quantum circuit', u'Quantum complexity theory', u'Quantum computation', u'Quantum computer', u'Quantum computing', u'Quantum convolutional code', u'Quantum cryptography', u'Quantum decoherence', u'Quantum energy teleportation', u'Quantum entanglement', u'Quantum error correction', u'Quantum gate', u'Quantum information', u'Quantum information science', u'Quantum invariant', u'Quantum key distribution', u'Quantum network', u'Quantum optics', u'Quantum phase estimation algorithm', u'Quantum programming', u'Quantum sort', u'Quantum superposition', u'Quantum teleportation', u'Quantum walk', u'Qubit', u'Random walk', u'Reviews of Modern Physics', u'Ring (mathematics)', u'SIAM Journal on Computing', u'SIAM Journal on Scientific and Statistical Computing', u""Shor's Algorithm"", u""Shor's algorithm"", u""Simon's algorithm"", u""Simon's problem"", u'Spin (physics)', u'Springer-Verlag', u'Stabilizer code', u'Superconducting quantum computing', u'Superdense coding', u'Symmetric group', u'Symposium on Foundations of Computer Science', u'Theory of Computing', u'Timeline of quantum computing', u'Topological quantum computer', u'Topological quantum field theory', u'Trapped ion quantum computer', u'Triangle finding problem', u'Turaev-Viro invariant', u'Ultracold atom', u'Undecidable problem', u'Unitarity', u'Universal quantum simulator', u'Yaoyun Shi']"
Quicksort,"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959, with his work published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.
Quicksort is a comparison sort, meaning that it can sort items of any type for which a ""less-than"" relation (formally, a total order) is defined. In efficient implementations it is not a stable sort, meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting.
Mathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.

","[u'1961 in science', u'Accuracy disputes from August 2015', u'Accuracy disputes from July 2015', u'Articles with example pseudocode', u'Comparison sorts', u'Pages with DOIs inactive since 2015', u'Sorting algorithms', u'Use dmy dates from January 2012']","[u'ALGOL', u'Adaptive sort', u'American flag sort', u'Analysis of algorithms', u'ArXiv', u'Array data structure', u'Association for Computing Machinery', u'Autocode', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary search tree', u'Binary tree sort', u'Binomial coefficient', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'C standard library', u'Call stack', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'CiteSeer', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Communications of the ACM', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'David Musser', u'Digital object identifier', u'Disk storage', u'Divide and conquer algorithm', u'Donald Knuth', u'Douglas McIlroy', u'Dutch national flag problem', u'Expected value', u'Faron Moller', u'Flashsort', u'GNU libc', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer overflow', u'Integer sorting', u'International Standard Book Number', u'International Standard Serial Number', u'Introduction to Algorithms', u'Introsort', u'JSort', u'Java (programming language)', u'Java version history', u'Jon Bentley', u'Library sort', u'Linked list', u'List (computing)', u'MIT Press', u'Machine translation', u'Magnetic tape data storage', u'Main memory', u'Master theorem', u'McGraw-Hill', u'Median', u'Median of medians', u'Merge sort', u'Mergesort', u'Moscow State University', u'Multi-key quicksort', u'National Physical Laboratory, UK', u'Network attached storage', u'Ninther', u'Object (computer science)', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel Random Access Machine', u'Parallel algorithm', u'Partial sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Prefix sum', u'Primitive data type', u'Programming Pearls', u'Proxmap sort', u'Pseudocode', u'Qsort', u'Quickselect', u'Radix sort', u'Recurrence relation', u'Recursion (computer science)', u'Robert Sedgewick (computer scientist)', u'Ron Rivest', u'Ronald L. Rivest', u'SIAM Journal on Computing', u'Samplesort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Soviet Union', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Steven Skiena', u""Stirling's approximation"", u'Stooge sort', u'Strand sort', u'Swansea University', u'Tail call', u'Tail recursion', u'Task parallelism', u'The Computer Journal', u'Thomas H. Cormen', u'Timsort', u'Tony Hoare', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Uniform distribution (discrete)', u'Unix', u'Version 7 Unix']"
RANSAC,"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981.They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.
A basic assumption is that the data consists of ""inliers"", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and ""outliers"" which are data that do not fit the model. The outliers can come, e.g., from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.","[u'All articles needing additional references', u'All articles with unsourced statements', u'Articles needing additional references from September 2014', u'Articles with example pseudocode', u'Articles with unsourced statements from September 2014', u'Geometry in computer vision', u'Robust statistics', u'SRI International', u'Statistical algorithms', u'Statistical outliers', u'Wikipedia articles needing clarification from April 2014', u'Wikipedia articles needing clarification from September 2014']","[u'Andrew Zisserman', u'Computer vision', u'Conference on Computer Vision and Pattern Recognition', u'Confidence interval', u'Correspondence problem', u'Digital object identifier', u'Fundamental matrix (computer vision)', u'Hough transform', u'International Standard Book Number', u'Iterative method', u'Loss function', u'MATLAB', u'Ordinary least squares', u'Outlier', u'Outliers', u'Regression analysis', u'Robust statistics', u'SRI International', u'Standard deviation']"
Rabin–Karp string search algorithm,"In computer science, the Rabin–Karp algorithm or Karp–Rabin algorithm is a string searching algorithm created by Richard M. Karp and Michael O. Rabin (1987) that uses hashing to find any one of a set of pattern strings in a text. For text of length n and p patterns of combined length m, its average and best case running time is O(n+m) in space O(p), but its worst-case time is O(nm). In contrast, the Aho–Corasick string matching algorithm has asymptotic worst-time complexity O(n+m) in space O(m).
A practical application of the algorithm is detecting plagiarism. Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical.","[u'Hashing', u'String matching algorithms']","[u'ASCII', u'Aho\u2013Corasick algorithm', u'Aho\u2013Corasick string matching algorithm', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Big-O notation', u'Bitap algorithm', u'Bloom filter', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Cambridge, Massachusetts', u'Charles E. Leiserson', u'CiteSeer', u'Clifford Stein', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computer science', u'Damerau\u2013Levenshtein distance', u'Data type', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Edit distance', u'Generalized suffix tree', u'Hamming distance', u'Hash collision', u'Hash function', u'Hash value', u""Hirschberg's algorithm"", u'International Standard Book Number', u'Introduction to Algorithms', u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Michael O. Rabin', u'Modular arithmetic', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Parsing', u'Pattern matching', u'Plagiarism', u'Prime number', u'Rabin fingerprint', u'Rabin\u2013Karp string search algorithm', u'Regular expression', u'Regular tree grammar', u'Richard Karp', u'Richard M. Karp', u'Rolling hash', u'Ronald L. Rivest', u'Rope (data structure)', u'Sequence alignment', u'Sequential pattern mining', u'Set data structure', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String metric', u'String searching algorithm', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u'Thomas H. Cormen', u""Thompson's construction"", u'Trie', u'Wagner\u2013Fischer algorithm']"
Rader's FFT algorithm,"Rader's algorithm (1968) is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution).
Since Rader's algorithm only depends upon the periodicity of the DFT kernel, it is directly applicable to any other transform (of prime order) with a similar property, such as a number-theoretic transform or the discrete Hartley transform.
The algorithm can be modified to gain a factor of two savings for the case of DFTs of real data, using a slightly modified re-indexing/permutation to obtain two half-size cyclic convolutions of real data (Chu & Burrus, 1982); an alternative adaptation for DFTs of real data, using the discrete Hartley transform, was described by Johnson & Frigo (2007).
Winograd extended Rader's algorithm to include prime-power DFT sizes  (Winograd 1976; Winograd 1978), and today Rader's algorithm is sometimes described as a special case of Winograd's FFT algorithm, also called the multiplicative Fourier transform algorithm (Tolimieri et al., 1997), which applies to an even larger class of sizes. However, for composite sizes such as prime powers, the Cooley–Tukey FFT algorithm is much simpler and more practical to implement, so Rader's algorithm is typically only used for large-prime base cases of Cooley–Tukey's recursive decomposition of the DFT (Frigo and Johnson, 2005).",[u'FFT algorithms'],"[u'Base case', u'Big O notation', u'Bijection', u""Bluestein's FFT algorithm"", u'Composite number', u'Convolution', u'Convolution theorem', u'Cooley\u2013Tukey FFT algorithm', u'Cunningham chain', u'Discrete Fourier transform', u'Discrete Hartley transform', u'Fast Fourier transform', u'Generating set of a group', u'Group (mathematics)', u'Modular arithmetic', u'Modular multiplicative inverse', u'Number-theoretic transform', u'Number theory', u'Power of two', u'Prime number', u'Primitive root modulo n', u'Recursion (computer science)', u'Sophie Germain prime', u""Winograd's FFT algorithm""]"
Radiosity (3D computer graphics),"In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code ""LD*E"") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.
Radiosity methods were first developed in about 1950 in the engineering field of heat transfer. They were later refined specifically for the problem of rendering computer graphics in 1984 by researchers at Cornell University.
Notable commercial radiosity engines are Enlighten by Geomerics (used for games including Battlefield 3 and Need for Speed: The Run); 3ds Max; form•Z; LightWave 3D and the Electric Image Animation System.","[u'3D computer graphics', u'All articles needing expert attention', u'All articles that are too technical', u'All articles with unsourced statements', u'Articles needing expert attention from July 2009', u'Articles with unsourced statements from March 2011', u'Global illumination algorithms', u'Heat transfer', u'Wikipedia articles that are too technical from July 2009']","[u'3D computer graphics', u'3ds Max', u'Algorithm', u'Ambient occlusion', u'Battlefield 3', u'Binary space partitioning', u'Cache (computing)', u'Computation', u'Computer Graphics (Publication)', u'Cornell University', u'Diffuse reflection', u'Dimensionless number', u'Electric Image Animation System', u'False radiosity', u'Finite element method', u'Form-Z', u'Form factor (radiative transfer)', u'Gauss\u2013Seidel method', u'Geomerics', u'Global illumination', u'Graphics accelerator', u'Heat', u'Heat transfer', u'Hemicube (computer graphics)', u'Hidden surface removal', u'J. Turner Whitted', u'Jacobi iteration', u""Lambert's cosine law"", u'LightWave 3D', u'Lightmap', u'Low-pass filter', u'Monte Carlo method', u'Need for Speed: The Run', u'OpenGL', u'Parlance', u'Path-tracing', u'Path tracing', u'Perspective transform', u'Quadratic function', u'Radiant exitance', u'Radiosity (heat transfer)', u'Ray tracing (graphics)', u'Raytracing', u'Rendering (computer graphics)', u'Rendering equation', u'Specular reflection', u'Texture mapping', u'Utah teapot', u'View factor']"
Radix sort,"In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines.
Most digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits of integer representations by groups of binary digit representations is most convenient. Two classifications of radix sorts are least significant digit (LSD) radix sorts and most significant digit (MSD) radix sorts. LSD radix sorts process the integer representations starting from the least digit and move towards the most significant digit. MSD radix sorts work the other way around.
LSD radix sorts typically use the following sorting order: short keys come before longer keys, and keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.
MSD radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations. A sequence such as ""b, c, d, e, f, g, h, i, j, ba"" would be lexicographically sorted as ""b, ba, c, d, e, f, g, h, i, j"". If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key for the purpose of determining sorted order.","[u'Articles with example C code', u'Sorting algorithms', u'Stable sorts', u'Use dmy dates from January 2012']","[u'Adaptive sort', u'American flag sort', u'Arithmetic sequence', u'Array data type', u'Base (exponentiation)', u'Batcher', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary search', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Breadth-first search', u'Bubble sort', u'Bucket (computing)', u'Bucket sort', u'Burstsort', u'Byte', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Concatenate', u'Counting sort', u'Cycle sort', u'Depth-first search', u'Directed acyclic graph', u'Donald Knuth', u""Dr Dobb's Journal"", u'Fanout', u'Flashsort', u'Gnome sort', u'Harold H. Seward', u'Heapsort', u'Herman Hollerith', u'Hybrid algorithm', u'IBM 80 series Card Sorters', u'IEEE floating-point standard', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Internal node', u'Introduction to Algorithms', u'Introsort', u'JSort', u'JavaScript', u'Leaf node', u'Least significant digit', u'Lexicographic order', u'Library sort', u'Linear search', u'Linked list', u'List (computing)', u'Massachusetts Institute of Technology', u'Maze solving algorithm', u'Merge sort', u'Mergesort', u'Most significant digit', u'Novosibirsk', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel Random Access Machine', u'Parallel computing', u'Parent node', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Positional notation', u'Post-order traversal', u'Pre-order traversal', u'Proxmap sort', u'Punched card', u'Queue (data structure)', u'Quicksort', u'Radix', u'Radix sort', u'Random-access machine', u'Recursion', u'Ronald L. Rivest', u'Root node', u'Selection algorithm', u'Selection sort', u'Set (mathematics)', u'Shellsort', u'Significant figures', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'String (computer science)', u'Tabulating machines', u'Thomas H. Cormen', u'Threaded binary tree', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Trie', u'Wojciech Rytter', u'Word size']"
Ramer–Douglas–Peucker algorithm,"The Ramer–Douglas–Peucker algorithm (RDP) is an algorithm for reducing the number of points in a curve that is approximated by a series of points. The initial form of the algorithm was independently suggested in 1972 by Urs Ramer and 1973 by David Douglas and Thomas Peucker and several others in the following decade. This algorithm is also known under the names Douglas–Peucker algorithm, iterative end-point fit algorithm and split-and-merge algorithm.","[u'Articles with example pseudocode', u'Computer graphics algorithms', u'Digital signal processing', u'Geometric algorithms']","[u'Algorithm', u'Cartographic generalization', u'Digital object identifier', u'Hausdorff distance', u'Lang simplification algorithm', u'Laser rangefinder', u'Master theorem', u'Opheim simplification algorithm', u'Polygonal chain', u'Recursion', u'Reumann\u2013Witkam algorithm', u'Vector graphics', u'Visvalingam\u2013Whyatt algorithm', u'Zhao Saalfeld algorithm']"
Random-restart hill climbing,"In computer science, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by incrementally changing a single element of the solution. If the change produces a better solution, an incremental change is made to the new solution, repeating until no further improvements can be found.
For example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.
Hill climbing is good for finding a local optimum (a solution that cannot be improved by considering a neighbouring configuration) but it is not necessarily guaranteed to find the best possible solution (the global optimum) out of all possible solutions (the search space). In convex problems, hill-climbing is optimal. Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.
The characteristic that only local optima are guaranteed can be cured by using restarts (repeated local search), or more complex schemes based on iterations, like iterated local search, on memory, like reactive search optimization and tabu search, or memory-less stochastic modifications, like simulated annealing.
The relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Choice of next node and starting node can be varied to give a list of related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems. It is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends.","[u'All articles that may contain original research', u'Articles that may contain original research from September 2007', u'Optimization algorithms and methods', u'Search algorithms']","[u'A* search algorithm', u'Alpha\u2013beta pruning', u'Anytime algorithm', u'Approximation algorithm', u'Artificial intelligence', u'Augmented Lagrangian method', u'B*', u'Backtracking', u'Barrier function', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u'Binary search', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Breadth-first search', u'British Museum algorithm', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Candidate solution', u'Combinatorial optimization', u'Comparison of optimization software', u'Computer science', u'Conjugate gradient method', u'Convex minimization', u'Convex optimization', u'Coordinate descent', u'Criss-cross algorithm', u'Cutting-plane method', u'D*', u'Davidon\u2013Fletcher\u2013Powell formula', u'Depth-first search', u'Depth-limited search', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Free On-line Dictionary of Computing', u'Fringe search', u'Function (mathematics)', u'GNU Free Documentation License', u'Gauss\u2013Newton algorithm', u'Genetic algorithm', u'Global optimum', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph (mathematics)', u'Graph algorithm', u'Graph traversal', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hillclimbing (disambiguation)', u'Incremental heuristic search', u'Integer programming', u'International Standard Book Number', u'Iterated local search', u'Iterative deepening A*', u'Iterative deepening depth-first search', u'Iterative method', u""Johnson's algorithm"", u'Jump point search', u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Lexicographic breadth-first search', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'List of algorithms', u'Local convergence', u'Local maximum', u'Local minimum', u'Local optimum', u'Local search (optimization)', u'Mathematical optimization', u'Matroid', u'Maxima and minima', u'Mean-shift', u'Meta-algorithm', u'Metaheuristic', u'Minimum spanning tree', u'Motorsport', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization (mathematics)', u'Optimization algorithm', u'Penalty method', u'Peter Norvig', u""Powell's method"", u""Prim's algorithm"", u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Random optimization', u'Random walk', u'Reactive search optimization', u'Revised simplex algorithm', u'SMA*', u'Search game', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Springer Science+Business Media', u'Steven Skiena', u'Stochastic hill climbing', u'Stuart J. Russell', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Tabu search', u'Travelling salesman problem', u'Tree traversal', u'Truncated Newton method', u'Trust region', u'Vertex (graph theory)', u'Walrasian auction', u'Wolfe conditions']"
Random forest,"Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set.
The algorithm for inducing a random forest was developed by Leo Breiman and Adele Cutler, and ""Random Forests"" is their trademark. The method combines Breiman's ""bagging"" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance.
The selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement classification proposed by Eugene Kleinberg.","[u'All articles to be merged', u'All articles with unsourced statements', u'Articles to be merged from May 2015', u'Articles with unsourced statements from June 2015', u'CS1 errors: external links', u'Classification algorithms', u'Decision trees', u'Ensemble learning', u'Pages using duplicate arguments in template calls']","[u'Annals of Statistics', u'Anomaly detection', u'Artificial neural network', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Bayesian network', u'Bias-variance dilemma', u'Bias\u2013variance dilemma', u'Bias\u2013variance tradeoff', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Bootstrapping (statistics)', u'Canonical correlation analysis', u'Classification and regression tree', u'Cluster analysis', u'Computational learning theory', u'Conditional random field', u'Convolutional neural network', u'Correlation', u'DBSCAN', u'Data mining', u'Decision tree', u'Decision tree learning', u'Deep learning', u'Digital object identifier', u'Dimensionality reduction', u'Donald Geman', u'Empirical risk minimization', u'Ensemble learning', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Generalization error', u'Gini impurity', u'Gradient boosting', u'Grammar induction', u'Graphical model', u'Hidden Markov model', u'Hierarchical clustering', u'Independent component analysis', u'Information gain', u'International Standard Book Number', u'Jerome H. Friedman', u'K-means clustering', u'K-nearest neighbor algorithm', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Kernel random forest', u'Learning to rank', u'Lecture Notes in Computer Science', u'Leo Breiman', u'Linear discriminant analysis', u'Linear regression', u'Linear subspace', u'Local outlier factor', u'Logistic regression', u'Machine Learning (journal)', u'Machine learning', u'Mathematical Reviews', u'Mean-shift', u'Mode (statistics)', u'Multilayer perceptron', u'Multinomial logistic regression', u'Naive Bayes classifier', u'Neural Computation', u'Non-negative matrix factorization', u'Non-parametric statistics', u'OPTICS algorithm', u'Online machine learning', u'Overfitting', u'Partial permutation', u'Perceptron', u'Principal component analysis', u'Probably approximately correct learning', u'PubMed Identifier', u'R (programming language)', u'R programming language', u'Random subspace method', u'Random tree (disambiguation)', u'Randomized algorithm', u'Recurrent neural network', u'Regression analysis', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Robert Tibshirani', u'Self-organizing map', u'Semi-supervised learning', u'Statistical classification', u'Statistical learning theory', u'Structured prediction', u'Supervised learning', u'Support vector machine', u'T-distributed stochastic neighbor embedding', u'Trademark', u'Trevor Hastie', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory']"
Rate-monotonic scheduling,"In computer science, rate-monotonic scheduling (RMS) is a scheduling algorithm used in real-time operating systems (RTOS) with a static-priority scheduling class. The static priorities are assigned on the basis of the cycle duration of the job: the shorter the cycle duration is, the higher is the job's priority.
These operating systems are generally preemptive and have deterministic guarantees with regard to response times. Rate monotonic analysis is used in conjunction with those systems to provide scheduling guarantees for a particular application.
^ Liu, C. L.; Layland, J. (1973), ""Scheduling algorithms for multiprogramming in a hard real-time environment"", Journal of the ACM 20 (1): 46–61, doi:10.1145/321738.321743 .
^ Bovet, Daniel P.; Cesati, Marco, Understanding the Linux Kernel , http://oreilly.com/catalog/linuxkernel/chapter/ch10.html#85347.","[u'All articles with unsourced statements', u'Articles with unsourced statements from October 2007', u'Processor scheduling algorithms', u'Real-time computing']","[u'Busy waiting', u'Butler Lampson', u'Central processing unit', u'Chung Laung Liu', u'Computer hardware', u'Computer science', u'Deadline-monotonic scheduling', u'Deadlock', u'Deos', u'Digital object identifier', u'Dynamic priority scheduling', u'Earliest deadline first scheduling', u'Infinity', u'International Standard Book Number', u'Linux kernel', u'Lock-free and wait-free algorithms', u'Mars Pathfinder', u'MicroC/OS-II', u'Preemption (computing)', u'Priority ceiling protocol', u'Priority inheritance', u'Priority inversion', u'RTEMS', u'Real-time operating system', u'Round-robin scheduling', u'Scheduling (computing)', u'Scheduling algorithm', u'Semaphore (programming)', u'Time-sharing', u'VxWorks']"
Raymond's Algorithm,"Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system. It imposes a logical structure (a K-ary tree) on distributed resources. As defined, each node has only a single parent, to which all requests to attain the token are made.

",[u'Concurrency control algorithms'],"[u'Critical section', u'Distributed system', u'FIFO (computing and electronics)', u'K-ary tree', u""Lamport's Distributed Mutual Exclusion Algorithm"", u""Lamport's bakery algorithm"", u""Maekawa's Algorithm"", u'Mutual exclusion', u""Naimi-Trehel's Algorithm"", u'Ricart-Agrawala algorithm', u""Suzuki-Kasami's Algorithm""]"
Reinforcement Learning,"Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.
Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.","[u'Belief revision', u'CS1 errors: missing author or editor', u'Machine learning algorithms', u'Markov models', u'Pages containing cite templates with deprecated parameters']","[u'Abhijit Gosavi', u'Action selection', u'Actor critic', u'Andrew G. Barto', u'Andrew W. Moore', u'Anomaly detection', u'Artificial neural network', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Backgammon', u'Bart De Schutter', u'Barto, Andrew G.', u'Bayesian network', u'Behaviorism', u'Bellman equations', u'Bias-variance dilemma', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Bounded rationality', u'Brute-force search', u'Canonical correlation analysis', u'Checkers', u'Christopher J.C.H. Watkins', u'Cluster analysis', u'Computational learning theory', u'Conditional random field', u'Control theory', u'Convolutional neural network', u'Cross-entropy method', u'Csaba Szepesvari', u'DBSCAN', u'Damien Ernst', u'Data mining', u'Decision tree learning', u'Deep learning', u'Delft University of Technology', u'Digital object identifier', u'Dimensionality reduction', u'Dimitri P. Bertsekas', u'Direct policy search', u'Distributed artificial intelligence', u'Dopamine', u'Dynamic programming', u'Dynamic treatment regimes', u'Economics', u'Empirical risk minimization', u'Ensemble learning', u'Error-driven learning', u'Evolutionary computation', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Fictitious play', u'Game theory', u'Genetic algorithm', u'Gerhard Neumann', u'Gradient descent', u'Grammar induction', u'Graphical model', u'G\xfcnther Palm', u'Hidden Markov model', u'Hierarchical clustering', u'IROS', u'Independent component analysis', u'Information theory', u'International Conference on Robotics and Automation', u'International Standard Book Number', u'Istvan Szita', u'Jan Peters', u'Jan Peters (researcher)', u'John Tsitsiklis', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Lazy evaluation', u'Learning classifier system', u'Learning to rank', u'Least-squares temporal difference method', u'Leslie P. Kaelbling', u'Likelihood ratio method', u'Linear discriminant analysis', u'Linear regression', u'Local outlier factor', u'Local search (optimization)', u'Logistic regression', u'Lucian Busoniu', u'Machine learning', u'Marc Peter Deisenroth', u'Markov decision process', u'Mean-shift', u'Michael L. Littman', u'Michel Tokic', u'Monte-Carlo tree search', u'Monte Carlo sampling', u'Multi-agent system', u'Multi-armed bandit', u'Multilayer perceptron', u'Naive Bayes classifier', u'Non-negative matrix factorization', u'Nonparametric statistics', u'OPTICS algorithm', u'Online machine learning', u'Operant conditioning', u'Operations research', u'Optimal control', u'Optimal control theory', u'Orange (software)', u'Partially observable Markov decision process', u'Perceptron', u'Peter Auer', u'Policy iteration', u'Predictive State Representation', u'Principal component analysis', u'Probably approximately correct learning', u'Q-Learning', u'Q-learning', u'Random forest', u'Recurrent neural network', u'Regression analysis', u'Reinforcement', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Richard S. Sutton', u'Robert Babuska', u'Robot control', u'Ronald J. Williams', u'Ronald Ortner', u'Self-organizing map', u'Semi-supervised learning', u'Sethu Vijayakumar', u'Simulated annealing', u'Simulation-based optimization', u'Software agent', u'State-Action-Reward-State-Action', u'Statistical classification', u'Statistical learning theory', u'Statistics', u'Stefan Schaal', u'Steven J. Bradtke', u'Stochastic', u'Stochastic optimization', u'Structured prediction', u'Supervised learning', u'Support vector machine', u'Swarm intelligence', u'T-distributed stochastic neighbor embedding', u'Telecommunications', u'Temporal difference', u'Temporal difference learning', u'Thomas Jaksch', u'Unsupervised learning', u'Value function approaches', u'Value iteration', u'Vapnik\u2013Chervonenkis theory']"
Relevance Vector Machine,"In mathematics, a relevance vector machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification. The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
It is actually equivalent to a Gaussian process model with covariance function:

where  is the kernel function (usually Gaussian),'s as the variances of the prior on the weight vector  ,and  are the input vectors of the training set.
Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).
The relevance vector machine is patented in the United States by Microsoft.","[u'All articles with unsourced statements', u'Articles with unsourced statements from February 2010', u'Classification algorithms', u'Kernel methods for machine learning', u'Nonparametric Bayesian statistics']","[u'Bayesian inference', u'Covariance function', u'Expectation maximization', u'Gaussian process', u'Journal of Machine Learning Research', u'Kernel function', u'Kernel trick', u'Machine learning', u'Mathematics', u'Microsoft', u'Parsimony', u'Platt scaling', u'Probabilistic classification', u'Regression analysis', u'Sequential minimal optimization', u'Software patents under United States patent law', u'Support vector machine', u'Training set']"
Restoring division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output.","[u'All articles to be expanded', u'All articles with unsourced statements', u'All pages needing factual verification', u'Articles to be expanded from September 2012', u'Articles with example pseudocode', u'Articles with unsourced statements from February 2012', u'Articles with unsourced statements from February 2014', u'Binary arithmetic', u'Computer arithmetic', u'Computer arithmetic algorithms', u'Division (mathematics)', u'Wikipedia articles needing clarification from July 2015', u'Wikipedia articles needing factual verification from June 2015']","[u'AMD', u'Algorithm', u'Analysis of algorithms', u'Approximation', u'Barrett reduction', u'Binomial theorem', u'Chunking (division)', u'Cryptography', u'Digital object identifier', u'Division (mathematics)', u'Double precision', u'Equioscillation theorem', u""Euclid's Elements"", u'Euclidean division', u'Extended precision', u'Fixed point arithmetic', u'Floating point', u'Fused multiply\u2013add', u'Greatest common divisor', u'Hexadecimal', u'Integer (computer science)', u'International Standard Book Number', u'Karatsuba algorithm', u'Long division', u'Lookup table', u'Microprocessor', u'Modular arithmetic', u'Multiplication algorithm', u'Multiplicative inverse', u""Newton's method"", u'OCLC', u'Original Intel Pentium (P5 microarchitecture)', u'Output-sensitive algorithm', u'Pentium FDIV bug', u'Power of two', u'Precision (computer science)', u'Quotient', u'Radix', u'Rate of convergence', u'Relative error', u'Remainder', u'Remez algorithm', u'Round-off error', u'Sch\xf6nhage\u2013Strassen algorithm', u'Short division', u'Single precision', u'Toom\u2013Cook multiplication']"
Reverse-delete algorithm,"The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph. It first appeared in Kruskal (1956), but it should not be confused with Kruskal's algorithm which appears in the same paper. If the graph is disconnected, this algorithm will find a minimum spanning tree for each disconnected part of the graph. The set of these minimum spanning trees is called a minimum spanning forest, which contains every vertex in the graph.
This algorithm is a greedy algorithm, choosing the best choice given any situation. It is the reverse of Kruskal's algorithm, which is another greedy algorithm to find a minimum spanning tree. Kruskal’s algorithm starts with an empty graph and adds edges while the Reverse-Delete algorithm starts with the original graph and deletes edges from it. The algorithm works as follows:
Start with graph G, which contains a list of edges E.
Go through E in decreasing order of edge weights.
For each edge, check if deleting the edge will further disconnect the graph.
Perform any deletion that does not lead to additional disconnection.","[u'Graph algorithms', u'Spanning tree']","[u'Algorithm', u'Big-O notation', u""Bor\u016fvka's algorithm"", u'Digital object identifier', u""Dijkstra's algorithm"", u'Graph theory', u'Greedy algorithm', u'JSTOR', u'Joseph Kruskal', u""Kruskal's algorithm"", u'Mikkel Thorup', u'Minimum spanning tree', u""Prim's algorithm"", u'Proceedings of the American Mathematical Society', u'Symposium on Theory of Computing', u'Weighted graph', u'\xc9va Tardos']"
Ricart-Agrawala Algorithm,"The Ricart-Agrawala Algorithm is an algorithm for mutual exclusion on a distributed system. This algorithm is an extension and optimization of Lamport's Distributed Mutual Exclusion Algorithm, by removing the need for  messages. It was developed by Glenn Ricart and Ashok Agrawala.

","[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Distributed algorithms']","[u'Ashok Agrawala', u'Distributed system', u'Glenn Ricart', u""Lamport's Distributed Mutual Exclusion Algorithm"", u""Lamport's bakery algorithm"", u'Logical clock', u""Maekawa's Algorithm"", u'Mutual exclusion', u""Naimi-Trehel's Algorithm"", u""Raymond's Algorithm"", u'Suzuki-Kasami algorithm']"
Ridder's method,"In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method and the use of an exponential function to successively approximate a root of a function f. The method is due to C. Ridders.
Ridders' method is simpler than Muller's method or Brent's method but with similar performance. The formula below converges quadratically when the function is well-behaved, which implies that the number of additional significant digits found at each step approximately doubles; but the function has to be evaluated twice for each step, so the overall order of convergence of the method is √2. If the function is not well-behaved, the root remains bracketed and the length of the bracketing interval at least halves on each iteration, so convergence is guaranteed. The algorithm also makes use of square roots, which are slower than basic floating point operations.","[u'All stub articles', u'Applied mathematics stubs', u'Root-finding algorithms']","[u'Applied mathematics', u""Brent's method"", u'Digital object identifier', u'Exponential function', u'False position method', u'Function (mathematics)', u'International Standard Book Number', u""Muller's method"", u'Numerical Recipes', u'Numerical analysis', u'Order of convergence', u'Root-finding algorithm', u'Root of a function']"
Root-finding algorithm,"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.
This article is concerned with finding scalar, real or complex roots, approximated as floating point numbers. Finding integer roots or exact algebraic roots are separate problems, whose algorithms have little in common with those discussed here. (See: Diophantine equation for integer roots)
Finding a root of f(x) − g(x) = 0 is the same as solving the equation f(x) = g(x). Here, x is called the unknown in the equation. Conversely, any equation can take the canonical form f(x) = 0, so equation solving is the same thing as computing (or finding) a root of a function.
Numerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limit, which is a root. The first values of this series are initial guesses. Many methods computes subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points.
The behaviour of root-finding algorithms is studied in numerical analysis. Algorithms perform best when they take advantage of known characteristics of the given function. Thus an algorithm to find isolated real roots of a low-degree polynomial in one variable may bear little resemblance to an algorithm for complex roots of a ""black-box"" function which is not even known to be differentiable. Questions include ability to separate close roots, robustness against failures of continuity and differentiability, reliability despite inevitable numerical errors, and rate of convergence.","[u'Root-finding algorithms', u'Vague or ambiguous time from February 2014']","[u'Abel\u2013Ruffini theorem', u'Aberth method', u'Algorithm', u""Bairstow's method"", u""Bernoulli's method"", u""Birge\u2013Vieta's method"", u'Bisection method', u'Bit', u""Brent's method"", u""Broyden's method"", u""Budan's theorem"", u'Canonical form', u'Companion matrix', u'Complex number', u'Computational complexity theory', u'Continuous function', u'Cryptographically secure pseudorandom number generator', u'Degree of a polynomial', u'Derivative', u""Descartes' rule of signs"", u'Differentiability', u'Digital object identifier', u'Diophantine equation', u'Durand\u2013Kerner method', u'Eigenvalue algorithm', u'Equation', u'Equation solving', u'False position method', u'Fast Fourier transform', u'Finite difference', u'Fixed point (mathematics)', u'Floating point', u'Function (mathematics)', u'GNU Scientific Library', u""Graeffe's method"", u""Halley's method"", u""Horner's method"", u""Householder's method"", u'Ill-conditioned', u'Integer', u'International Standard Book Number', u'Interval arithmetic', u'Inverse function', u'Inverse power method', u'Inverse quadratic interpolation', u'Iteration', u'Jenkins\u2013Traub algorithm', u'Jenkins\u2013Traub method', u""Laguerre's method"", u'Lehmer\u2013Schur algorithm', u'Limit of a sequence', u'Linear interpolation', u'MPSolve', u'Maple (software)', u'Mathematica', u""Muller's method"", u'Multiplicity (mathematics)', u""Newton's method"", u'Nikolai Ivanovich Lobachevsky', u'Nth root algorithm', u'Numerical analysis', u'Polynomial', u'Polynomial greatest common divisor', u'Polynomial interpolation', u'Polynomial transformations', u'Power method', u'Precision (arithmetic)', u'Quadratic equation', u'Quadratic formula', u'Rational number', u'Real number', u""Ridders' method"", u'Root-finding algorithm', u'Root of a function', u'Ruffini rule', u'Sage (mathematics software)', u'Scalar (mathematics)', u'Secant method', u'Sequence', u""Sidi's method"", u'Splitting circle method', u'Square-free factorization', u""Sturm's theorem"", u'SymPy', u'System of polynomial equations', u""Vincent's theorem"", u""Wilkinson's polynomial"", u'Xcas']"
Round-robin scheduling,"Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing. As the term is generally used, time slices are assigned to each process in equal portions and in circular order, handling all processes without priority (also known as cyclic executive). Round-robin scheduling is simple, easy to implement, and starvation-free. Round-robin scheduling can also be applied to other scheduling problems, such as data packet scheduling in computer networks. It is an Operating System concept.
The name of the algorithm comes from the round-robin principle known from other fields, where each person takes an equal share of something in turn.","[u'All articles needing additional references', u'Articles needing additional references from April 2015', u'CS1 errors: external links', u'Network scheduling algorithms', u'Processor scheduling algorithms']","[u'Abraham Silberschatz', u'Best-effort', u'CPU', u'Channel access', u'Computing', u'Cyclic executive', u'Deficit round robin', u'Fair queuing', u'First-come first-served', u'International Standard Book Number', u'John Wiley & Sons', u'Link adaptation', u'Max-min fairness', u'Maximum throughput scheduling', u'Multilevel queue', u'Multiple access', u'Network scheduler', u'Operating System Concepts', u'Operating system', u'Packet switching', u'Polling (computer science)', u'Preemption (computing)', u'Process scheduler', u'Proportionally fair', u'Resource starvation', u'Round-robin (disambiguation)', u'Scheduling starvation', u'Statistical multiplexing', u'System spectrum efficiency', u'Time-sharing', u'Token passing', u'Token ring', u'Weighted fair queuing', u'Weighted round robin', u'Work-conserving']"
SEQUITUR algorithm,Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997 that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols. The algorithm operates in linear space and time. It can be used in data compression software applications.,[u'Lossless compression algorithms'],"[u'ArXiv', u'Context-free grammar', u'Craig Nevill-Manning', u'Data compression', u'Digital object identifier', u'Digram', u'Ian H. Witten', u'Lossless compression', u'Nonterminal symbol', u'Straight-line grammar', u'Terminal symbol']"
SHA-1,"In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST. SHA-1 is considered insecure against well-funded opponents, and it is recommended to use SHA-2 or SHA-3 instead.
SHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 digits long.
SHA-1 is a member of the Secure Hash Algorithm family. The four SHA algorithms are structured differently and are named SHA-0, SHA-1, SHA-2, and SHA-3. SHA-0 is the original version of the 160-bit hash function published in 1993 under the name SHA: it was not adopted by many applications. Published in 1995, SHA-1 is very similar to SHA-0, but alters the original SHA hash specification to correct weaknesses that were unknown to the public at that time. SHA-2, published in 2001, is significantly different from the SHA-1 hash function.
In 2005, cryptanalysts found attacks on SHA-1 suggesting that the algorithm might not be secure enough for ongoing use. NIST required many applications in federal agencies to move to SHA-2 after 2010 because of the weakness. Although no successful attacks have yet been reported on SHA-2, it is algorithmically similar to SHA-1. In 2012, following a long-running competition, NIST selected an additional algorithm, Keccak, for standardization under SHA-3.
Microsoft, Google and Mozilla have all announced that their respective browsers will stop accepting SHA-1 SSL certificates by 2017. Windows XP SP2 and earlier, and Android 2.2 and earlier, do not support SHA2 certificates.","[u'All articles containing potentially dated statements', u'All articles needing additional references', u'All articles with specifically marked weasel-worded phrases', u'All articles with unsourced statements', u'Articles containing potentially dated statements from 2013', u'Articles containing potentially dated statements from October 2015', u'Articles needing additional references from May 2013', u'Articles with Chinese-language external links', u'Articles with DMOZ links', u'Articles with example pseudocode', u'Articles with specifically marked weasel-worded phrases from September 2015', u'Articles with unsourced statements from August 2012', u'Articles with unsourced statements from June 2015', u'Broken hash functions', u'CS1 errors: dates', u'Checksum algorithms', u'Cryptographic hash functions', u'National Security Agency cryptography']","[u'AMD Opteron', u'ASCII', u'ASIACRYPT', u'Algorithm', u'Amazon Elastic Compute Cloud', u'Andrew Yao', u'Antoine Joux', u'Authenticated encryption', u'Avalanche effect', u'BLAKE (hash function)', u'BOINC', u'Base64', u'Bcrypt', u'Big O notation', u'Birthday attack', u'Bit', u'Block cipher', u'Boomerang attack', u'Booting', u'Brute-force search', u'Brute force attack', u'Byte', u'CBC-MAC', u'CCM mode', u'CMAC', u'CRYPTO', u'CRYPTO (conference)', u'CRYPTREC', u'CWC mode', u'Circular shift', u'Collision (computer science)', u'Collision attack', u'Communications Security Establishment', u'Comparison of cryptographic hash functions', u'Crypt (C)', u'Cryptanalysis', u'Cryptlib', u'Crypto++', u'Cryptographic Module Validation Program', u'Cryptographic hash function', u'Cryptographically secure pseudorandom number generator', u'Cryptography', u'Cryptosystem', u'DMOZ', u'Data Authentication Algorithm', u'Data corruption', u'Digital Signature Algorithm', u'Digital timestamping', u'Distributed revision control', u'EAX mode', u'Eli Biham', u'Elisabeth Oswald', u'Elliptic curve only hash', u'Endianness', u'Fast Syndrome Based Hash', u'Federal Information Processing Standard', u'Florent Chabaud', u'Frances Yao', u'GOST (hash function)', u'Galois/Counter Mode', u'Git (software)', u'Google', u'Graz University of Technology', u'Gr\xf8stl', u'HAS-160', u'HAVAL', u'HMAC', u'Hash collision', u'Hash function security summary', u'Hashcash', u'Helena Handschuh', u'Henri Gilbert', u'Hexadecimal', u'History of cryptography', u'Hongbo Yu', u'IAPM (mode)', u'IPsec', u'Intelligence agency', u'International Association for Cryptologic Research', u'Itanium 2', u'JH (hash function)', u'Key derivation function', u'Key stretching', u'Kupyna', u'LM hash', u'Length extension attack', u'Libgcrypt', u'Linus Torvalds', u'MD2 (cryptography)', u'MD4', u'MD5', u'MD6', u'MDC-2', u'Massachusetts Institute of Technology', u'Md5deep', u'Mebibyte', u'Mercurial', u'Mercurial (software)', u'Merkle tree', u'Merkle\u2013Damg\xe5rd construction', u'Message authentication', u'Message authentication code', u'Message digest', u'Microsoft', u'Modular arithmetic', u'Monotone (software)', u'Mozilla', u'N-Hash', u'NESSIE', u'NIST', u'NIST hash function competition', u'NSA', u'NVIDIA', u'National Institute of Standards and Technology', u'National Security Agency', u'Nintendo', u'Nothing up my sleeve number', u'OCB mode', u'One-key MAC', u'One-way compression function', u'OpenSSL', u'Outline of cryptography', u'PBKDF2', u'PMAC (cryptography)', u'Password strength', u'PolarSSL', u'Poly1305', u'Portable Document Format', u'Preimage attack', u'Pretty Good Privacy', u'Pseudocode', u'Public-key cryptography', u'RIPEMD', u'RIPEMD-160', u'RadioGat\xfan', u'Rainbow table', u'Revision control', u'Ron Rivest', u'S/MIME', u'SHA-0', u'SHA-2', u'SHA-3', u'SHACAL', u'SSL certificate', u'SWIFFT', u'Salt (cryptography)', u'Scrypt', u'Second preimage resistance', u'Secure Hash Algorithm', u'Secure Hash Standard', u'Secure Shell', u'Secure Sockets Layer', u'Selected Areas in Cryptography', u'Sha1sum', u'Shandong University', u'Sic', u'Side-channel attack', u'SipHash', u'Skein (hash function)', u'Snefru', u'Steganography', u'Stream cipher', u'Streaming SIMD Extensions', u'Streebog', u'Supercomputer', u'Symmetric-key algorithm', u'Tiger (cryptography)', u'Transport Layer Security', u'U.S. Government', u'UMAC', u'Usenet', u'Usenet newsgroup', u'VMAC', u'Very smooth hash', u'Vincent Rijmen', u'Whirlpool (cryptography)', u'Wii', u'Word (data type)', u'X86', u'Xiaoyun Wang', u'Yiqun Lisa Yin', u'YouTube']"
SHA-2,"SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the NSA. SHA stands for Secure Hash Algorithm. Cryptographic hash functions are mathematical operations run on digital data; by comparing the computed ""hash"" (the output from execution of the algorithm) to a known and expected hash value, a person can determine the data's integrity. For example, computing the hash of a downloaded file and comparing the result to a previously published hash result can show whether the download has been modified or tampered with. A key aspect of cryptographic hash functions is their collision resistance: nobody should be able to find two different input values that result in the same hash output.
SHA-2 includes significant changes from its predecessor, SHA-1. The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256.
SHA-256 and SHA-512 are novel hash functions computed with 32-bit and 64-bit words, respectively. They use different shift amounts and additive constants, but their structures are otherwise virtually identical, differing only in the number of rounds. SHA-224 and SHA-384 are simply truncated versions of the first two, computed with different initial values. SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, but the initial values are generated using the method described in FIPS PUB 180-4. SHA-2 was published in 2001 by the NIST as a U.S. federal standard (FIPS). The SHA-2 family of algorithms are patented in US 6829355 . The United States has released the patent under a royalty-free license.
In 2005, an algorithm emerged for finding SHA-1 collisions in about 2000-times fewer steps than was previously thought possible. Although (as of 2015) no example of a SHA-1 collision has been published yet, the security margin left by SHA-1 is weaker than intended, and its use is therefore no longer recommended for applications that depend on collision resistance, such as digital signatures. Although SHA-2 bears some similarity to the SHA-1 algorithm, these attacks have not been successfully extended to SHA-2.
Currently, the best public attacks break preimage resistance 52 rounds of SHA-256 or 57 rounds of SHA-512, and collision resistance for 46 rounds of SHA-256, as shown in the Cryptanalysis and validation section below.","[u'All articles containing potentially dated statements', u'All articles with dead external links', u'Articles containing potentially dated statements from 2013', u'Articles with dead external links from November 2012', u'Articles with example pseudocode', u'Checksum algorithms', u'Cryptographic hash functions', u'National Security Agency cryptography']","[u'AMD Opteron', u'ASIACRYPT', u'ASIC', u'Authenticated encryption', u'Avalanche effect', u'BLAKE (hash function)', u'Bcrypt', u'Biclique attack', u'Birthday attack', u'Bit', u'Bitcoin', u'Bitwise operation', u'Block cipher', u'Brute-force attack', u'Brute force attack', u'CBC-MAC', u'CCM mode', u'CMAC', u'CRYPTREC', u'CWC mode', u'Collision (computer science)', u'Collision attack', u'Collision resistance', u'Communications Security Establishment', u'Comparison of cryptographic hash functions', u'Crypt (C)', u'Cryptanalysis', u'Cryptocurrency', u'Cryptographic Module Validation Program', u'Cryptographic hash function', u'Cryptographically secure pseudorandom number generator', u'Cryptography', u'DKIM', u'DNSSEC', u'Data Authentication Algorithm', u'Debian GNU/Linux', u'Differential cryptanalysis', u'Digital object identifier', u'Digital signature', u'Digital timestamping', u'EAX mode', u'EUROCRYPT', u'Elliptic curve only hash', u'Fast Software Encryption', u'Fast Syndrome Based Hash', u'Federal Information Processing Standard', u'GOST (hash function)', u'Galois/Counter Mode', u'Google Chrome', u'Gr\xf8stl', u'HAS-160', u'HAVAL', u'HMAC', u'Hash function security summary', u'Hashcash', u'History of cryptography', u'IAPM (mode)', u'IPsec', u'International Association for Cryptologic Research', u'International Criminal Tribunal for Rwanda', u'International Standard Book Number', u'International Standard Serial Number', u'Ivy Bridge (microarchitecture)', u'JH (hash function)', u'Key derivation function', u'Key stretching', u'Kupyna', u'LM hash', u'Length extension attack', u'MD2 (cryptography)', u'MD4', u'MD5', u'MD6', u'MDC-2', u'Mebibyte', u'Meet-in-the-middle attack', u'Merkle\u2013Damg\xe5rd construction', u'Message authentication', u'Message authentication code', u'Message digest', u'Modular arithmetic', u'N-Hash', u'NESSIE', u'NIST', u'NIST hash function competition', u'National Institute of Standards and Technology', u'National Security Agency', u'OCB mode', u'One-key MAC', u'Outline of cryptography', u'PBKDF2', u'PMAC (cryptography)', u'Password strength', u'Piledriver (microarchitecture)', u'Poly1305', u'Portable Document Format', u'Preimage attack', u'Preimage resistance', u'Pretty Good Privacy', u'Proof-of-stake', u'Proof-of-work', u'Pseudocode', u'Public-key cryptography', u'RIPEMD', u'RadioGat\xfan', u'Rainbow table', u'S/MIME', u'SHA-0', u'SHA-1', u'SHA-3', u'SWIFFT', u'Salt (cryptography)', u'Scrypt', u'Secure Hash Algorithm', u'Secure Hash Standard', u'Secure Shell', u'Secure Sockets Layer', u'Selected Areas in Cryptography', u'Sha1sum', u'Shadow password', u'Side-channel attack', u'SipHash', u'Skein (hash function)', u'Snefru', u'Sony', u'Steganography', u'Stream cipher', u'Streebog', u'Symmetric-key algorithm', u'The quick brown fox jumps over the lazy dog', u'Tiger (cryptography)', u'Transport Layer Security', u'Triple DES', u'U.S. Government', u'UMAC', u'United States', u'University of Illinois at Chicago', u'VMAC', u'Very smooth hash', u'Whirlpool (cryptography)', u'X86', u'X86-64', u'XORed']"
SRT division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output.","[u'All articles to be expanded', u'All articles with unsourced statements', u'All pages needing factual verification', u'Articles to be expanded from September 2012', u'Articles with example pseudocode', u'Articles with unsourced statements from February 2012', u'Articles with unsourced statements from February 2014', u'Binary arithmetic', u'Computer arithmetic', u'Computer arithmetic algorithms', u'Division (mathematics)', u'Wikipedia articles needing clarification from July 2015', u'Wikipedia articles needing factual verification from June 2015']","[u'AMD', u'Algorithm', u'Analysis of algorithms', u'Approximation', u'Barrett reduction', u'Binomial theorem', u'Chunking (division)', u'Cryptography', u'Digital object identifier', u'Division (mathematics)', u'Double precision', u'Equioscillation theorem', u""Euclid's Elements"", u'Euclidean division', u'Extended precision', u'Fixed point arithmetic', u'Floating point', u'Fused multiply\u2013add', u'Greatest common divisor', u'Hexadecimal', u'Integer (computer science)', u'International Standard Book Number', u'Karatsuba algorithm', u'Long division', u'Lookup table', u'Microprocessor', u'Modular arithmetic', u'Multiplication algorithm', u'Multiplicative inverse', u""Newton's method"", u'OCLC', u'Original Intel Pentium (P5 microarchitecture)', u'Output-sensitive algorithm', u'Pentium FDIV bug', u'Power of two', u'Precision (computer science)', u'Quotient', u'Radix', u'Rate of convergence', u'Relative error', u'Remainder', u'Remez algorithm', u'Round-off error', u'Sch\xf6nhage\u2013Strassen algorithm', u'Short division', u'Single precision', u'Toom\u2013Cook multiplication']"
SSS*,"SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm.
SSS* is based on the notion of solution trees. Informally, a solution tree can be formed from any arbitrary game tree by pruning the number of branches at each MAX node to one. Such a tree represents a complete strategy for MAX, since it specifies exactly one MAX action for every possible sequence of moves might be made by the opponent. Given a game tree, SSS* searches through the space of partial solution trees, gradually analyzing larger and larger subtrees, eventually producing a single solution tree with the same root and Minimax value as the original game tree. SSS* never examines a node that alpha-beta pruning would prune, and may prune some branches that alpha-beta would not. Stockman speculated that SSS* may therefore be a better general algorithm than alpha-beta. However, Igor Roizen and Judea Pearl have shown that the savings in the number of positions that SSS* evaluates relative to alpha/beta is limited and generally not enough to compensate for the increase in other resources (e.g., the storing and sorting of a list of nodes made necessary by the best-first nature of the algorithm). However, Aske Plaat, Jonathan Schaeffer, Wim Pijls and Arie de Bruin have shown that a sequence of null-window alpha-beta calls is equivalent to SSS* (i.e., it expands the same nodes in the same order) when alpha-beta is used with a transposition table, as is the case in all game-playing programs for chess, checkers, etc. Now the storing and sorting of the OPEN list were no longer necessary. This allowed the implementation of (an algorithm equivalent to) SSS* in tournament quality game-playing programs. Experiments showed that it did indeed perform better than Alpha-Beta in practice, but that it did not beat NegaScout.
The reformulation of a best-first algorithm as a sequence of depth-first calls prompted the formulation of a class of null-window alpha-beta algorithms, of which MTD-f is the best known example.","[u'All articles needing additional references', u'Articles needing additional references from February 2010', u'Search algorithms']","[u'A* search algorithm', u'Alpha-Beta', u'Alpha-beta pruning', u'Aske Plaat', u'Best-first search', u""Dewey's notation"", u'Digital object identifier', u'Game tree', u'Igor Roizen', u'Jonathan Schaeffer', u'Judea Pearl', u'MTD-f', u'Minimax', u'NegaScout', u'Priority queue', u'Search algorithm', u'Solution tree', u'State space search', u'Transposition table']"
SUBCLU,"SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger. It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.

","[u'All articles needing additional references', u'All articles needing expert attention', u'Articles needing additional references from February 2010', u'Articles needing expert attention from February 2010', u'Articles needing expert attention with no reason or talk parameter', u'Data clustering algorithms', u'Statistics articles needing expert attention']","[u'Apriori algorithm', u'Axis-parallel', u'Cluster analysis', u'Clustering high-dimensional data', u'DBSCAN', u'Environment for DeveLoping KDD-Applications Supported by Index-Structures', u'Greedy algorithm', u'Hans-Peter Kriegel', u'Monotonicity', u'Subspace clustering', u'Top-down and bottom-up design']"
Samplesort,"Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing m -1 elements from the result. These elements (called splitters) then divide the sample into m equal-sized buckets. Samplesort is described in the 1970 paper, ""Samplesort: A Sampling Approach to Minimal Storage Tree Sorting"", by W D Frazer and A C McKellar. In recent years, the algorithm has been adapted to implement randomized sorting on parallel computers.","[u'All articles needing expert attention', u'Articles needing expert attention from April 2009', u'Articles needing expert attention with no reason or talk parameter', u'Computer science articles needing expert attention', u'Sorting algorithms']","[u'Buckets', u'Divide and conquer algorithm', u'Flashsort', u'Parallel computer', u'Quicksort', u'Randomized sorting', u'Sorting algorithm']"
Scanline rendering,"Scanline rendering is an algorithm for visible surface determination, in 3D computer graphics, that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis. All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, then each row or scanline of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.
The main advantage of this method is that sorting vertices along the normal of the scanning plane reduces the number of comparisons between edges. Another advantage is that it is not necessary to translate the coordinates of all vertices from the main memory into the working memory—only vertices defining edges that intersect the current scan line need to be in active memory, and each vertex is read in only once. The main memory is often very slow compared to the link between the central processing unit and cache memory, and thus avoiding re-accessing vertices in main memory can provide a substantial speedup.
This kind of algorithm can be easily integrated with many other graphics techniques, such as the Phong reflection model or the Z-buffer algorithm.","[u'3D rendering', u'Computer graphics algorithms', u'Optics']","[u'3D computer graphics', u'Binary space partitioning', u""Bresenham's line algorithm"", u'Bubble sort', u'Cache memory', u'Caulking (video games)', u'Cell (microprocessor)', u'Deferred shading', u'Dreamcast', u'Evans & Sutherland', u'Framebuffer', u'Hidden surface determination', u'Ivan Sutherland', u'Nintendo DS', u""Painter's algorithm"", u'Phong reflection model', u'Pixel', u'PlayStation 3', u'Polygon', u'PowerVR', u'Raster scan', u'Ray tracing (graphics)', u'Salt Lake City', u'Scanline', u'Sprite (computer graphics)', u'Tiled rendering', u'University of Utah', u'Vertex (geometry)', u'Z-buffer', u'Z-buffering']"
Schensted algorithm,"In mathematics, the Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson–Schensted–Knuth correspondence, and a further generalization to pictures by Zelevinsky.
The simplest description of the correspondence is using the Schensted algorithm (Schensted 1961), a procedure that constructs one tableau by successively inserting the values of the permutation according to a specific rule, while the other tableau records the evolution of the shape during construction. The correspondence had been described, in a rather different form, much earlier by Robinson (Robinson 1938), in an attempt to prove the Littlewood–Richardson rule. The correspondence is often referred to as the Robinson–Schensted algorithm, although the procedure used by Robinson is radically different from the Schensted–algorithm, and almost entirely forgotten. Other methods of defining the correspondence include a nondeterministic algorithm in terms of jeu de taquin.
The bijective nature of the correspondence relates it to the enumerative identity:

where  denotes the set of partitions of n (or of Young diagrams with n squares), and tλ denotes the number of standard Young tableaux of shape λ.","[u'Algebraic combinatorics', u'Combinatorial algorithms', u'Permutations', u'Representation theory of finite groups']","[u'American Journal of Mathematics', u'Bijection', u'Cambridge University Press', u'Canadian Journal of Mathematics', u'Combinatorics', u'Craige Schensted', u'Digital object identifier', u'Donald Ervin Knuth', u'Donald Knuth', u'Encyclopedia of Mathematics', u'Enumerative combinatorics', u'Gilbert de Beauregard Robinson', u'International Standard Book Number', u'Involution (mathematics)', u'JSTOR', u'James Alexander Green', u'Jeu de taquin', u'Littlewood\u2013Richardson rule', u'Longest increasing subsequence', u'Mathematical Reviews', u'Mathematics', u'Nondeterministic algorithm', u'Pacific Journal of Mathematics', u'Partition (number theory)', u'Permutation', u'Picture (mathematics)', u'Pseudocode', u'Representation theory', u'Richard P. Stanley', u'Robinson\u2013Schensted\u2013Knuth correspondence', u'Sch\xfctzenberger involution', u'Springer-Verlag', u'Springer Science+Business Media', u'The Art of Computer Programming', u""Viennot's geometric construction"", u'William Fulton (mathematician)', u'Young diagram', u'Young tableaux', u'Zentralblatt MATH']"
Schönhage–Strassen algorithm,"The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Schönhage and Volker Strassen in 1971. The run-time bit complexity is, in Big O notation, O(n log n log log n) for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 22n + 1 elements, a specific type of number theoretic transform.
The Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, Fürer's algorithm, was announced with lower asymptotic complexity; however, Fürer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.
In practice the Schönhage–Strassen algorithm starts to outperform older methods such as Karatsuba and Toom–Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits). The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture. There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.
Applications of the Schönhage–Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of π, as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.","[u'Computer arithmetic algorithms', u'Multiplication']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Approximations of \u03c0', u'Arnold Sch\xf6nhage', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big O notation', u'Binary GCD algorithm', u'Bit complexity', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u'Convolution theorem', u'Cooley\u2013Tukey FFT algorithm', u""Cornacchia's algorithm"", u'Cyclic convolution', u'Discrete Fourier transform', u'Discrete Fourier transform (general)', u'Discrete logarithm', u'Discrete weighted transform', u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u'Fast Fourier transform', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'GNU Multi-Precision Library', u'General number field sieve', u'Generating primes', u'Great Internet Mersenne Prime Search', u'Greatest common divisor', u'Index calculus algorithm', u'Integer', u'Integer factorization', u'Integer square root', u'Karatsuba algorithm', u'Karatsuba multiplication', u'Kronecker substitution', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Locality of reference', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Mathematical empiricism', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Negacyclic convolution', u'Number-theoretic transform', u'Number theory', u'Order (group theory)', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Positional notation', u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u'Ring (mathematics)', u'Root of unity', u""Schoof's algorithm"", u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'The Art of Computer Programming', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Volker Strassen', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Secant method,"In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predated the latter by over 3,000 years.",[u'Root-finding algorithms'],"[u'Bisection method', u""Broyden's method"", u'Eric W. Weisstein', u'False position method', u'Finite difference', u'Function (mathematics)', u'Golden ratio', u'International Standard Book Number', u'John Wiley & Sons', u'MathWorld', u'Matlab', u""Newton's method"", u'Numerical analysis', u'Order of convergence', u'Quadratic convergence', u'Quasi-Newton method', u'Recurrence relation', u'Root-finding algorithm', u'Root of a function', u'Secant line']"
Selection algorithm,"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n) (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.
The simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median, and this necessarily takes n/2 storage. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is quickselect, which is related to quicksort; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.","[u'All articles with unsourced statements', u'Articles with unsourced statements from April 2014', u'Selection algorithms']","[u'Algorithm', u'Almost certain', u'Amortized analysis', u'Array data structure', u'C++', u'CPAN', u'Charles E. Leiserson', u'Clifford Stein', u'Computer science', u'Counting sort', u'Decrease and conquer', u'Descriptive statistics', u'Dictionary of Algorithms and Data Structures', u'Digital object identifier', u'Donald E. Knuth', u'Donald Knuth', u'Frequency tables', u'Geometric series', u'Hash table', u'Heap (data structure)', u'Introduction to Algorithms', u'Introselect', u'Introsort', u'Lazy evaluation', u'List (abstract data type)', u'Manuel Blum', u'Maximum', u'Median', u'Median of medians', u'Minimum', u'National Institute of Standards and Technology', u'Nearest neighbor problem', u'Odds algorithm', u'Online algorithm', u'Order statistic', u'Order statistic tree', u'Ordinal optimization', u'Partial sorting', u'Perl', u'Python (programming language)', u'Quickselect', u'Quicksort', u'Radix sort', u'Random access', u'Range Queries', u'Reduction (complexity)', u'Robert Floyd', u'Robert Tarjan', u'Robert W. Floyd', u'Ron Rivest', u'Ronald L. Rivest', u'Secretary problem', u'Selection (genetic algorithm)', u'Selection sort', u'Self-balancing binary search tree', u'Shortest path', u'Sorting algorithm', u'Sublinear time', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Vaughan Pratt']"
Selection sort,"In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(n2) time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.
The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.","[u'Comparison sorts', u'Sorting algorithms']","[u'Adaptive sort', u'American flag sort', u'Arithmetic progression', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Data structure', u'Dictionary of Algorithms and Data Structures', u'Divide and conquer algorithm', u'Donald Knuth', u'EEPROM', u'Flash memory', u'Flashsort', u'Gnome sort', u'Heap (data structure)', u'Heapsort', u'Hybrid algorithm', u'Implicit data structure', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'Linked list', u'List (computing)', u'Merge sort', u'Mergesort', u'National Institute of Standards and Technology', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Pascal (programming language)', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Real-time computing', u'Robert Sedgewick (computer scientist)', u'Selection algorithm', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'The Art of Computer Programming', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Sethi-Ullman algorithm,"In computer science, the Sethi–Ullman algorithm is an algorithm named after Ravi Sethi and Jeffrey D. Ullman, its inventors, for translating abstract syntax trees into machine code that uses as few registers as possible.","[u'Compiler construction', u'Graph algorithms']","[u'Abstract syntax tree', u'Algorithm', u'Associativity', u'Code generation (compiler)', u'Commutativity', u'Compiler', u'Computer science', u'Digital object identifier', u'Jeffrey D. Ullman', u'Journal of the Association for Computing Machinery', u'Machine code', u'Order of evaluation', u'Processor register', u'RISC', u'Ravi Sethi', u'Register allocation', u'Register spilling', u'Strahler number']"
Shamir's Secret Sharing,"Shamir's Secret Sharing is an algorithm in cryptography created by Adi Shamir. It is a form of secret sharing, where a secret is divided into parts, giving each participant its own unique part, where some of the parts or all of them are needed in order to reconstruct the secret.
Counting on all participants to combine the secret might be impractical, and therefore sometimes the threshold scheme is used where any  of the parts are sufficient to reconstruct the original secret.","[u'All articles needing expert attention', u'All articles that are too technical', u'Articles needing expert attention from March 2014', u'Information-theoretically secure algorithms', u'Secret sharing', u'Wikipedia articles that are too technical from March 2014']","[u'Adi Shamir', u'Algorithm', u'Alice and Bob', u'Chung Laung Liu', u'Cryptography', u'Cubic function', u'Curve fitting', u'Degree of a polynomial', u'Digital object identifier', u'Donald Knuth', u'Finite field', u'Finite field arithmetic', u'Homomorphic secret sharing', u'Information theoretic security', u'Lagrange polynomial', u'Line (geometry)', u'Natural numbers', u'Parabola', u'Partial Password', u'Point (geometry)', u'Polynomial', u'Safe', u'Secret sharing', u'The Art of Computer Programming', u'Two-man rule']"
Shifting nth-root algorithm,"The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division.","[u'All articles lacking sources', u'Articles lacking sources from May 2010', u'Computer arithmetic algorithms', u'Root-finding algorithms']","[u'Algorithm', u'Binary search', u'Decimal precision', u'Integer', u'Invariant (computer science)', u'Long division', u'Nth root', u'Numerical digit', u'Radix', u'Real number']"
Shoelace algorithm,"The shoelace formula or shoelace algorithm (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by ordered pairs in the plane. The user cross-multiplies corresponding coordinates to find the area encompassing the polygon, and subtracts it from the surrounding polygon to find the area of the polygon within. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like tying shoelaces. It is also sometimes called the shoelace method. It has applications in surveying and forestry, among other areas.
The formula was described by Meister (1724-1788) in 1769 and by Gauss in 1795. It can be verified by dividing the polygon into triangles, but it can also be seen as a special case of Green's theorem.
The area formula is derived by taking each edge AB, and calculating the (signed) area of triangle ABO with a vertex at the origin O, by taking the cross-product (which gives the area of a parallelogram) and dividing by 2. As one wraps around the polygon, these triangles with positive and negative area will overlap, and the areas between the origin and the polygon will be cancelled out and sum to 0, while only the area inside the reference triangle remains. This is why the formula is called the Surveyor's Formula, since the ""surveyor"" is at the origin; if going counterclockwise, positive area is added when going from left to right and negative area is added when going from right to left, from the perspective of the origin.
The area formula is valid for any non-self-intersecting (simple) polygon, which can be convex or concave.","[u'CS1 Latin-language sources (la)', u'Geometric algorithms', u'Surveying']","[u'Absolute value', u'Algorithm', u'Area', u'Carl Friedrich Gauss', u'Determinant', u'Digital object identifier', u'Eric W. Weisstein', u""Green's Theorem"", u""Green's theorem"", u'International Standard Book Number', u'Matrix (mathematics)', u'Ordered pair', u'Pentagon', u'Planimeter', u'Polygon', u'Quadrilateral', u'Simple polygon', u'Surveying', u'Triangle']"
Shor's algorithm,"Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally it solves the following problem: given an integer N, find its prime factors.
On a quantum computer, to factor an integer N, Shor's algorithm runs in polynomial time (the time taken is polynomial in log N, which is the size of the input). Specifically it takes quantum gates of order O((log N)2(log log N)(log log log N)) using fast multiplication, demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is thus in the complexity class BQP. This is substantially faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time — about O(e1.9 (log N)1/3 (log log N)2/3). The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.
If a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.
In 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 × 5, using an NMR implementation of a quantum computer with 7 qubits. After IBM's implementation, two independent groups, one at the University of Science and Technology of China, and the other one at the University of Queensland, have implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits. In 2012, the factorization of 15 was repeated. Also in 2012, the factorization of 21 was achieved, setting the record for the largest number factored with a quantum computer. In April 2012, the factorization of 143 was achieved, although this used adiabatic quantum computation rather than Shor's algorithm. It was discovered in November 2014 that this adiabatic quantum computation in 2012 had in fact also factored larger numbers, the largest being 56153, which is currently the record for the largest integer factored on a quantum device.","[u'All articles lacking in-text citations', u'All articles needing expert attention', u'All articles that are too technical', u'Articles containing proofs', u'Articles lacking in-text citations from September 2010', u'Articles needing expert attention from February 2014', u'Integer factorization algorithms', u'Post-quantum cryptography', u'Quantum algorithms', u'Quantum information science', u'Wikipedia articles that are too technical from February 2014']","[u'AKS primality test', u'Adiabatic quantum computation', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Algorithmic cooling', u'Ancient Egyptian multiplication', u'ArXiv', u'BQP', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Bibcode', u'Big O notation', u'Binary GCD algorithm', u""B\xe9zout's identity"", u'Cavity quantum electrodynamics', u'Chakravala method', u'Charge qubit', u'Chinese remainder theorem', u""Cipolla's algorithm"", u'Circuit quantum electrodynamics', u'Classical capacity', u'Cluster state', u'Complexity class', u'Composite number', u'Continued fraction', u'Continued fraction factorization', u'Coprime', u""Cornacchia's algorithm"", u'Deutsch\u2013Jozsa algorithm', u'Digital object identifier', u'Discrete logarithm', u'Divides', u""Dixon's factorization method"", u'EQP (complexity)', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Entanglement-assisted classical capacity', u'Entanglement-assisted stabilizer formalism', u'Entanglement distillation', u'Euclidean algorithm', u""Euler's factorization method"", u""Euler's totient function"", u'Exponentiating by squaring', u'Exponentiation by squaring', u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Flux qubit', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Group (mathematics)', u'Group homomorphism', u""Grover's algorithm"", u'Hadamard transform', u'Hidden subgroup problem', u'Imaginary unit', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'Interference (wave propagation)', u'Irreducible fraction', u'Isaac Chuang', u'Kane quantum computer', u'Karatsuba algorithm', u'LOCC', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Linear optical quantum computing', u'Long multiplication', u'Loss\u2013DiVincenzo quantum computer', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Measurement in quantum mechanics', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Modulo operation', u'Multiplication algorithm', u'Multiplicative group of integers modulo n', u'Nature (journal)', u'Nicholas Rush', u'Nitrogen-vacancy center', u'No cloning theorem', u'Noise', u'Nontrivial', u'Nuclear magnetic resonance (NMR) quantum computing', u'Nuclear magnetic resonance quantum computer', u'Number theory', u'One-way quantum computer', u'Optical lattice', u'Order (group theory)', u'Periodic function', u'Peter Shor', u'Phase qubit', u'Physical Review Letters', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Polynomial', u'Polynomial time', u'Positive real axis', u'Post-quantum cryptography', u'PostBQP', u'Primality test', u'Primality testing', u'Prime factor', u""Proth's theorem"", u'Pseudo-polynomial time', u'PubMed Identifier', u'Public-key cryptography', u""P\xe9pin's test"", u'QMA', u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Quantum Fourier transform', u'Quantum Turing machine', u'Quantum algorithm', u'Quantum annealing', u'Quantum capacity', u'Quantum channel', u'Quantum circuit', u'Quantum cloning', u'Quantum complexity theory', u'Quantum computer', u'Quantum convolutional code', u'Quantum cryptography', u'Quantum decoherence', u'Quantum energy teleportation', u'Quantum error correction', u'Quantum gate', u'Quantum information', u'Quantum information science', u'Quantum key distribution', u'Quantum network', u'Quantum optics', u'Quantum phase estimation algorithm', u'Quantum programming', u'Quantum superposition', u'Quantum teleportation', u'Qubit', u'Qubits', u'RSA (algorithm)', u'Rational sieve', u'Reversible computing', u'Root of unity', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u'Scott Aaronson', u""Shanks' square forms factorization"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u""Simon's problem"", u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Spin (physics)', u'Stabilizer code', u'Stargate Universe', u'Sub-exponential time', u'Superconducting quantum computing', u'Superdense coding', u'The Bat Jar Conjecture', u'The Big Bang Theory', u'Timeline of quantum computing', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Topological quantum computer', u'Trapped ion quantum computer', u'Trial division', u'Ultracold atom', u'Universal quantum simulator', u'University of California, Berkeley', u'University of Queensland', u'University of Science and Technology of China', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Shortest job next,"Shortest job next (SJN), also known as Shortest Job First (SJF) or Shortest Process Next (SPN), is a scheduling policy that selects the waiting process with the smallest execution time to execute next. SJN is a non-preemptive algorithm. Shortest remaining time is a preemptive variant of SJN.
Shortest job next is advantageous because of its simplicity and because it minimizes the average amount of time each process has to wait until its execution is complete. However, it has the potential for process starvation for processes which will require a long time to complete if short processes are continually added. Highest response ratio next is similar but provides a solution to this problem.
Another disadvantage of using shortest job next is that the total execution time of a job must be known before execution. While it is not possible to perfectly predict execution time, several methods can be used to estimate the execution time for a job, such as a weighted average of previous execution times.
Shortest job next can be effectively used with interactive processes which generally follow a pattern of alternating between waiting for a command and executing it. If the execution burst of a process is regarded as a separate ""job"", past behaviour can indicate which process to run next, based on an estimate of its running time.
Shortest job next is used in specialized environments where accurate estimates of running time are available. Estimating the running time of queued processes is sometimes done using a technique called aging.
^ Arpaci-Dusseau, Remzi H.; Arpaci-Dusseau, Andrea C. (2014), Operating Systems: Three Easy Pieces [Chapter Scheduling Introduction] (PDF), Arpaci-Dusseau Books 
^ Silberschatz, A.; Galvin, P.B.; Gagne, G. (2005). Operating Systems Concepts (7th ed.). Wiley. p. 161. ISBN 0-471-69466-5. 
^ Tanenbaum, A. S. (2008). Modern Operating Systems (3rd ed.). Pearson Education, Inc. p. 156. ISBN 0-13-600663-9.",[u'Processor scheduling algorithms'],"[u'Adversarial queueing network', u'Aging (scheduling)', u'Arrival theorem', u'BCMP network', u'Balance equation', u'Bene\u0161 method', u'Bulk queue', u""Burke's theorem"", u""Buzen's algorithm"", u'Continuous-time Markov chain', u'D/M/1 queue', u'Data buffer', u'Decomposition method (queueing theory)', u'Erlang (unit)', u'Erlang distribution', u'FIFO (computing and electronics)', u'Flow-equivalent server method', u'Flow control (data)', u'Fluid limit', u'Fluid queue', u'Fork\u2013join queue', u'G-network', u'G/G/1 queue', u'G/M/1 queue', u'Gordon\u2013Newell theorem', u'Heavy traffic approximation', u'Highest response ratio next', u'Information system', u'International Standard Book Number', u'Jackson network', u'Kelly network', u""Kendall's notation"", u""Kingman's formula"", u'LIFO (computing)', u'Layered queueing network', u'Lindley equation', u""Little's law"", u'Loss network', u'M/D/1 queue', u'M/D/c queue', u'M/G/1 queue', u'M/G/k queue', u'M/M/1 queue', u'M/M/c queue', u'M/M/\u221e queue', u'Markovian arrival process', u'Matrix analytic method', u'Mean field theory', u'Mean value analysis', u'Message queue', u'Network congestion', u'Network scheduler', u'Pipeline (software)', u'Poisson process', u'Pollaczek\u2013Khinchine formula', u'Polling system', u'Preemption (computing)', u'Process (computing)', u'Processor sharing', u'Product-form solution', u'Quality of service', u'Quasireversibility', u'Queueing theory', u'Rational arrival process', u'Reflected Brownian motion', u'Retrial queue', u'Scheduling (computing)', u'Scheduling algorithm', u'Shortest job first', u'Shortest remaining time', u'Starvation (computing)', u'Teletraffic engineering', u'Traffic equations']"
Shortest remaining time,"Shortest remaining time, also known as shortest remaining time first (SRTF), is a scheduling method that is a preemptive version of shortest job next scheduling. In this scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. Since the currently executing process is the one with the shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of time.
Shortest remaining time is advantageous because short processes are handled very quickly. The system also requires very little overhead since it only makes a decision when a process completes or a new process is added, and when a new process is added the algorithm only needs to compare the currently executing process with the new process, ignoring all other processes currently waiting to execute.
Like shortest job first, it has the potential for process starvation; long processes may be held off indefinitely if short processes are continually added. This threat can be minimal when process times follow a heavy-tailed distribution.
Like shortest job next scheduling, shortest remaining time scheduling is rarely used outside of specialized environments because it requires accurate estimations of the runtime of all processes that are waiting to execute.",[u'Processor scheduling algorithms'],"[u'Adversarial queueing network', u'Arrival theorem', u'BCMP network', u'Balance equation', u'Bene\u0161 method', u'Bulk queue', u""Burke's theorem"", u""Buzen's algorithm"", u'Continuous-time Markov chain', u'D/M/1 queue', u'Data buffer', u'Decomposition method (queueing theory)', u'Digital object identifier', u'Erlang (unit)', u'Erlang distribution', u'FIFO (computing and electronics)', u'Flow-equivalent server method', u'Flow control (data)', u'Fluid limit', u'Fluid queue', u'Fork\u2013join queue', u'G-network', u'G/G/1 queue', u'G/M/1 queue', u'Gordon\u2013Newell theorem', u'Heavy-tailed distribution', u'Heavy traffic approximation', u'Information system', u'Jackson network', u'Kelly network', u""Kendall's notation"", u""Kingman's formula"", u'LIFO (computing)', u'Layered queueing network', u'Lindley equation', u""Little's law"", u'Loss network', u'M/D/1 queue', u'M/D/c queue', u'M/G/1 queue', u'M/G/k queue', u'M/M/1 queue', u'M/M/c queue', u'M/M/\u221e queue', u'Markovian arrival process', u'Matrix analytic method', u'Mean field theory', u'Mean value analysis', u'Message queue', u'Network congestion', u'Network scheduler', u'Pipeline (software)', u'Poisson process', u'Pollaczek\u2013Khinchine formula', u'Polling system', u'Preemption (computing)', u'Process (computing)', u'Processor sharing', u'Product-form solution', u'Quality of service', u'Quasireversibility', u'Queueing theory', u'Rational arrival process', u'Reflected Brownian motion', u'Retrial queue', u'Scheduling (computing)', u'Shortest job first', u'Shortest job next', u'Starvation (computing)', u'Teletraffic engineering', u'Traffic equations']"
Shortest seek first,"Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.

","[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Disk scheduling algorithms']","[u'Disk storage', u'Elevator algorithm', u'FCFS (computing and electronics)', u'I/O scheduling', u'Resource starvation']"
Shunting yard algorithm,"In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the ""shunting yard"" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61.
Like the evaluation of RPN, the shunting yard algorithm is stack-based. Infix expressions are the form of mathematical notation most people are used to, for instance 3+4 or 3+4*(2−1). For the conversion there are two text variables (strings), the input and the output. There is also a stack that holds operators not yet added to the output queue. To convert, the program reads each symbol in order and does something based on that symbol.
The shunting-yard algorithm has been later generalized into operator-precedence parsing.","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from August 2013', u'Dutch inventions', u'Parsing algorithms']","[u'Abstract syntax tree', u'Algorithm', u'Classification yard', u'Computer science', u'Constant folding', u'Edsger Dijkstra', u'Function (mathematics)', u'Infix notation', u'Interpreter (computing)', u'Mathematisch Centrum', u'Operator-precedence parser', u'Operator associativity', u'Order of operations', u'Polish notation', u'Queue (data structure)', u'Reverse Polish Notation', u'Reverse Polish notation', u'Stack (data structure)', u'String (computer science)', u'Token (parser)', u'Tokenize', u'Variable (programming)']"
Sieve of Eratosthenes,"In mathematics, the sieve of Eratosthenes (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit. It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the multiples of 2.
The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.
The sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes. It is named after Eratosthenes of Cyrene, a Greek mathematician; although none of his works have survived, the sieve was described and attributed to Eratosthenes in the Introduction to Arithmetic by Nicomachus.
The sieve may be used to find primes in arithmetic progressions.","[u'Algorithms', u'All articles needing additional references', u'Articles containing Ancient Greek-language text', u'Articles containing Greek-language text', u'Articles needing additional references from June 2015', u'Articles with example pseudocode', u'Pages using citations with accessdate and no URL', u'Primality tests', u'Sieve theory']","[u'1 (number)', u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Analysis of algorithms', u'Ancient Egyptian multiplication', u'Ancient Greek', u'Arithmetic progression', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big O Notation', u'Binary GCD algorithm', u'Bit complexity', u'CPU cache', u'Chakravala method', u""Cipolla's algorithm"", u'Composite number', u'Continued fraction factorization', u'Coprime', u""Cornacchia's algorithm"", u'David Turner (computer scientist)', u'Digital object identifier', u'Discrete logarithm', u'Divisor', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Eratosthenes', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u'Functional programming', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Greek mathematics', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Introduction to Arithmetic', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'List (computing)', u'Locality of reference', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Mathematics', u'Meissel\u2013Mertens constant', u""Mertens' theorems"", u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Natural number', u'Nicomachus', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u'Prime-counting function', u'Prime harmonic series', u'Prime number', u'Prime number theorem', u'Proof of the Euler product formula for the Riemann zeta function', u""Proth's theorem"", u'Pseudo-polynomial time', u'Pseudocode', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Random access machine', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Sorenson', u'Sieve of Sundaram', u'Sieve theory', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'Wolfram Demonstrations Project']"
Simon's algorithm,"In computational complexity theory and quantum computing, Simon's problem is a computational problem in the model of decision tree complexity or query complexity, conceived by Daniel Simon in 1994. Simon exhibited a quantum algorithm, usually called Simon's algorithm, that solves the problem exponentially faster than any (deterministic or probabilistic) classical algorithm.
Simon's algorithm uses  queries to the black box, whereas the best classical probabilistic algorithm necessarily needs at least  queries. It is also known that Simon's algorithm is optimal in the sense that any quantum algorithm to solve this problem requires  queries. This problem yields an oracle separation between BPP and BQP, unlike the separation provided by the Deutsch-Jozsa algorithm, which separates P and EQP.
Although the problem itself is of little practical value it is interesting because it provides an exponential speedup over any classical algorithm. Moreover, it was also the inspiration for Shor's algorithm. Both problems are special cases of the abelian hidden subgroup problem, which is now known to have efficient quantum algorithms.","[u'All articles with unsourced statements', u'Articles with unsourced statements from May 2012', u'Quantum algorithms']","[u'Adiabatic quantum computation', u'Algorithmic cooling', u'ArXiv', u'BPP (complexity)', u'BQP', u'Cavity quantum electrodynamics', u'Charge qubit', u'Circuit quantum electrodynamics', u'Classical capacity', u'Cluster state', u'Computational complexity theory', u'Coset', u'Decision tree complexity', u'Destructive interference', u'Deutsch-Jozsa algorithm', u'Deutsch\u2013Jozsa algorithm', u'Digital object identifier', u'EQP (complexity)', u'Entanglement-assisted classical capacity', u'Entanglement-assisted stabilizer formalism', u'Entanglement distillation', u'Flux qubit', u""Grover's algorithm"", u'Hadamard transform', u'Hidden subgroup problem', u'Hilbert space', u'Kane quantum computer', u'LOCC', u'Linear optical quantum computing', u'Loss\u2013DiVincenzo quantum computer', u'Nitrogen-vacancy center', u'Nuclear magnetic resonance quantum computer', u'One-way quantum computer', u'Optical lattice', u'P (complexity)', u'Phase qubit', u'PostBQP', u'Probabilistic algorithms', u'QMA', u'Quantum Fourier transform', u'Quantum Turing machine', u'Quantum algorithm', u'Quantum annealing', u'Quantum capacity', u'Quantum channel', u'Quantum circuit', u'Quantum complexity theory', u'Quantum computer', u'Quantum computing', u'Quantum convolutional code', u'Quantum cryptography', u'Quantum decoherence', u'Quantum energy teleportation', u'Quantum error correction', u'Quantum gate', u'Quantum information', u'Quantum information science', u'Quantum key distribution', u'Quantum network', u'Quantum optics', u'Quantum phase estimation algorithm', u'Quantum programming', u'Quantum teleportation', u'Qubit', u""Shor's algorithm"", u'Spin (physics)', u'Stabilizer code', u'Superconducting quantum computing', u'Superdense coding', u'Timeline of quantum computing', u'Topological quantum computer', u'Trapped ion quantum computer', u'Ultracold atom', u'Universal quantum simulator', u'XOR']"
Simple LR parser,"In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm. As with other types of LR(1) parser, an SLR parser is quite efficient at finding the single correct bottom-up parse in a single left-to-right scan over the input stream, without guesswork or backtracking. The parser is mechanically generated from a formal grammar for the language.
SLR and the more-general methods LALR parser and Canonical LR parser have identical methods and similar tables at parse time; they differ only in the mathematical grammar analysis algorithms used by the parser generator tool. SLR and LALR generators create tables of identical size and identical parser states. SLR generators accept fewer grammars than do LALR generators like yacc and Bison. Many computer languages don't readily fit the restrictions of SLR, as is. Bending the language's natural grammar into SLR grammar form requires more compromises and grammar hackery. So LALR generators have become much more widely used than SLR generators, despite being somewhat more complicated tools. SLR methods remain a useful learning step in college classes on compiler theory.
SLR and LALR were both developed by Frank DeRemer as the first practical uses of Donald Knuth's LR parser theory. The tables created for real grammars by full LR methods were impractically large, larger than most computer memories of that decade, with 100 times or more parser states than the SLR and LALR methods.

","[u'All articles lacking sources', u'All articles with unsourced statements', u'Articles lacking sources from December 2012', u'Articles with unsourced statements from January 2015', u'Articles with unsourced statements from June 2012', u'Parsing algorithms']","[u'Bottom-up parsing', u'Canonical LR parser', u'Computer science', u'Donald Knuth', u'Formal grammar', u'Frank DeRemer', u'GNU bison', u'LALR parser', u'LL parser', u'LR parser', u'SLR grammar', u'Yacc']"
Simplex algorithm,"In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming. The journal Computing in Science and Engineering listed it as one of the top 10 algorithms of the twentieth century.
The name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin. Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones, and these become proper simplices with an additional constraint. The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a polytope. The shape of this polytope is defined by the constraints applied to the objective function.","[u'1947 in computer science', u'Exchange algorithms', u'Linear programming', u'Operations research', u'Optimization algorithms and methods']","[u'Albert W. Tucker', u'Alexander Schrijver', u'Algorithm', u'Approximation algorithm', u'ArXiv', u'Augmented Lagrangian method', u'Baire category theory', u'Barrier function', u'Bellman\u2013Ford algorithm', u'Best, worst and average case', u""Bland's rule"", u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Canonical form', u'Charles E. Leiserson', u'Christos H. Papadimitriou', u'CiteSeer', u'Clifford Stein', u'Combinatorial optimization', u'Comparison of optimization software', u'Computer science', u'Computing in Science and Engineering', u'Cone (geometry)', u'Convex minimization', u'Convex optimization', u'Convex polytope', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Elementary matrix', u'Ellipsoid method', u'Ellipsoidal algorithm', u'Evolutionary algorithm', u'Exchange algorithm', u'Exponential time', u'Feasible region', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Fourier\u2013Motzkin elimination', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'General topology', u'George B. Dantzig', u'George Dantzig', u'George J. Minty', u'Gilbert Strang', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Identity matrix', u'Integer programming', u'Interior point method', u'International Standard Book Number', u'International Standard Serial Number', u'Iterative method', u'JSTOR', u'Jerzy Neyman', u""Johnson's algorithm"", u'Karmarkar', u""Karmarkar's algorithm"", u'Katta G. Murty', u'Khachiyan', u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear-fractional programming', u'Linear complementarity problem', u'Linear functional', u'Linear independence', u'Linear programming', u'Local convergence', u'Local search (optimization)', u'Mathematical Reviews', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Michael J. Todd (mathematician)', u'Minimum spanning tree', u'Mixed complementarity problem', u'Mixed linear complementarity problem', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear complementarity problem', u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Normal distribution', u'Operations research', u'Optimization (mathematics)', u'Optimization algorithm', u'Oriented matroid', u'Penalty method', u'Polynomial time', u'Polytope', u'Porous set', u""Powell's method"", u'Probability distribution', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Random matrix', u'Random vector', u'Revised simplex algorithm', u'Ronald L. Rivest', u'SIAM Review', u'Sequential quadratic programming', u'Shanghua Teng', u'Simplex', u'Simulated annealing', u'Smoothed complexity', u'Sparse matrix', u'Structural stability', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'System of linear inequalities', u'Tabu search', u'The Mathematical Intelligencer', u'Theodore Motzkin', u'Thomas H. Cormen', u'Truncated Newton method', u'Trust region', u'Vertex (geometry)', u'Victor Klee', u'Wassily Leontief', u'Wolfe conditions']"
Simulated annealing,"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic for approximate global optimization in a large search space. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding the precise global optimum is less important than finding an acceptable global optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as brute-force search or gradient descent.
The name and inspiration come from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce defects. Both are attributes of the material that depend on its thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy. While the same amount of cooling brings the same decrease in temperature, the rate of cooling dictates the magnitude of decrease in the thermodynamic free energy, with slower cooling producing a bigger decrease. Simulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution.
The method was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, and by Vlado Černý in 1985. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953.","[u'All articles needing additional references', u'All articles with unsourced statements', u'Articles needing additional references from December 2009', u'Articles with inconsistent citation formats', u'Articles with unsourced statements from June 2011', u'Heuristic algorithms', u'Monte Carlo methods', u'Optimization algorithms and methods']","[u'Adaptive simulated annealing', u'Amorphous solid', u'Annealing (metallurgy)', u'Ant colony optimization', u'Automatic label placement', u'Bibcode', u'Brute-force search', u'Colour', u'Combinatorial optimization', u'Constraint satisfaction', u'Convex programming', u'Cross-entropy method', u'Crystal', u'Crystalline solid', u'Crystallographic defect', u'Diameter (graph theory)', u'Digital object identifier', u'Drainage basin', u'Dual-phase evolution', u'Factorial', u'Function (mathematics)', u'Genetic algorithms', u'Global optimization', u'Global optimum', u'Gradient descent', u'Graduated optimization', u'Graph cuts in computer vision', u'Graph theory', u'Greedy algorithm', u'Hamiltonian (quantum mechanics)', u'Harmony search', u'Heuristic', u'Hill climbing', u'Hill climbing algorithm', u'Infinite-dimensional optimization', u'Integer programming', u'Intelligent Water Drops', u'Internal energy', u'International Standard Book Number', u'JSTOR', u'Local optimum', u'Markov chain', u'Marshall Rosenbluth', u'Mathematical optimization', u'Metaheuristic', u'Metropolis-Hastings algorithm', u'Metropolis\u2013Hastings algorithm', u'Molecular dynamics', u'Monte Carlo method', u'Multidisciplinary optimization', u'Multiobjective optimization', u'Nicholas Metropolis', u'Nonlinear programming', u'Optimization (mathematics)', u'Parallel tempering', u'Particle filter', u'Particle swarm optimization', u'Permutation', u'Physical system', u'Pixel', u'Place and route', u'Potential energy', u'Probabilistic', u'Probabilistic algorithm', u'Procedural parameter', u'Pseudocode', u'PubMed Identifier', u'Quadratic programming', u'Quantum annealing', u'Quintillion', u'Reactive search optimization', u'Robust optimization', u'Solution space', u'State transition', u'Steepest descent', u'Stochastic gradient descent', u'Stochastic optimization', u'Stochastic programming', u'Stochastic tunneling', u'Tabu search', u'Thermodynamic equilibrium', u'Thermodynamic free energy', u'Thermodynamic state', u'Traveling salesman problem', u'Uniform distribution (continuous)']"
Smith–Waterman algorithm,"The Smith–Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings or nucleotide or protein sequences. Instead of looking at the total sequence, the Smith–Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.
The algorithm was first proposed by Temple F. Smith and Michael S. Waterman in 1981. Like the Needleman–Wunsch algorithm, of which it is a variation, Smith–Waterman is a dynamic programming algorithm. As such, it has the desirable property that it is guaranteed to find the optimal local alignment with respect to the scoring system being used (which includes the substitution matrix and the gap-scoring scheme). The main difference to the Needleman–Wunsch algorithm is that negative scoring matrix cells are set to zero, which renders the (thus positively scoring) local alignments visible. Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment. One does not actually implement the algorithm as described because improved alternatives are now available that have better scaling (Gotoh, 1982)  and are more accurate (Altschul and Erickson, 1986).","[u'All articles with dead external links', u'All articles with unsourced statements', u'Articles with dead external links from October 2010', u'Articles with unsourced statements from November 2011', u'Bioinformatics algorithms', u'CS1 errors: external links', u'Computational phylogenetics', u'Dynamic programming', u'Sequence alignment algorithms']","[u'AMD', u'Aho\u2013Corasick algorithm', u'Alphabet', u'Altivec', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'BLAST', u'BMC Bioinformatics', u'Backtracking', u'Big O notation', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Bulletin of Mathematical Biology', u'CLC bio', u'CUDA', u'Cell Broadband Engine', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Core (microarchitecture)', u'Cray', u'Damerau\u2013Levenshtein distance', u'Deterministic acyclic finite state automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Dynamic programming', u'Edit distance', u'European Bioinformatics Institute', u'Expectation value', u'FASTA', u'Field-programmable gate array', u'GNU Affero General Public License', u'GPU', u'Gap penalty', u'GeForce 8 Series', u'Generalized suffix tree', u'Graphics processing units', u'Hamming distance', u""Hirschberg's algorithm"", u'Homology (biology)', u'IBM BladeCenter', u'Intel', u'Jaro\u2013Winkler distance', u'Joint Genome Institute', u'Journal of Molecular Biology', u'Journal of molecular biology', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lawrence Livermore National Laboratory', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'MMX (instruction set)', u'Mathematical optimization', u'Matrix (mathematics)', u'Michael S. Waterman', u'NVIDIA', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Nucleotide sequences', u'Parsing', u'Pattern matching', u'Pentium (brand)', u'PlayStation 3', u'PowerPC', u'Protein sequence', u'PubMed Central', u'PubMed Identifier', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Reconfigurable computing', u'Regular expression', u'Regular tree grammar', u'Rope (data structure)', u'SSE2', u'SSE4', u'SSSE3', u'Scalability', u'Sequence alignment', u'Sequence database', u'Sequence mining', u'Sequential pattern mining', u'String (computer science)', u'String metric', u'String searching algorithm', u'Substitution matrix', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Temple F. Smith', u'Ternary search tree', u""Thompson's construction"", u'Trie', u'Wagner\u2013Fischer algorithm', u'Xeon']"
Smoothsort,"Smoothsort is a comparison-based sorting algorithm. It is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort. The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state.","[u'Articles with example Java code', u'Comparison sorts', u'Dutch inventions', u'Heaps (data structures)', u'Sorting algorithms']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary heap', u'Bit vector', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Edsger Dijkstra', u'Edsger W. Dijkstra', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Leonardo numbers', u'Library sort', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stable sort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Transdichotomous model', u'Tree sort', u'University of Texas at Austin']"
Snapshot algorithm,"The snapshot algorithm is an algorithm used in distributed systems for recording a consistent global state of an asynchronous system. The algorithm discussed here is also known as the Chandy–Lamport algorithm, after Leslie Lamport and K. Mani Chandy.

",[u'Distributed algorithms'],"[u'Algorithm', u'Asynchronous communication', u'Distributed systems', u'FIFO (computing and electronics)', u'Internet protocol suite', u'K. Mani Chandy', u'Leslie Lamport', u'University of Texas at Austin']"
Sort-Merge Join,"The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system.
The basic problem of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which display that value. The key idea of the Sort-merge algorithm is to first sort the relations by the join attribute, so that interleaved linear scans will encounter these sets at the same time.
In practice, the most expensive part of performing a sort-merge join is arranging for both inputs to the algorithm to be presented in sorted order. This can be achieved via an explicit sort operation (often an external sort), or by taking advantage of a pre-existing ordering in one or both of the join relations. The latter condition can occur because an input to the join might be produced by an index scan of a tree-based index, another merge join, or some other plan operator that happens to produce output sorted on an appropriate key.
Let's say that we have two relations  and  and .  fits in  pages memory and  fits in  pages memory. So, in the worst case Sort-Merge Join will run in  I/Os. In the case that  and  are not ordered the worst case time cost will contain additional terms of sorting time: , which equals  (as linearithmic terms outweigh the linear terms, see Big O notation – Orders of common functions).","[u'All articles lacking sources', u'Articles lacking sources from December 2009', u'Join algorithms']","[u'Big O notation', u'Database management system', u'External sort', u'Join (SQL)', u'Join algorithm', u'Linearithmic time', u'Relational database', u'Tuple']"
Sorted list,"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (reordering) of the input.
Further, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.
Since the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.
Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.","[u'All articles lacking in-text citations', u'All articles with specifically marked weasel-worded phrases', u'All articles with unsourced statements', u'Articles lacking in-text citations from September 2009', u'Articles with specifically marked weasel-worded phrases from September 2015', u'Articles with unsourced statements from December 2010', u'Commons category with local link same as on Wikidata', u'Data processing', u'Sorting algorithms']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Array data type', u'Average performance', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Best-case performance', u'Big O notation', u'Binary tree', u'Binary tree sort', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Byte Magazine', u'Canonicalization', u'Cartesian tree', u'Cascade merge sort', u'Central Processing Unit', u'Charles E. Leiserson', u'CiteSeer', u'Clifford Stein', u'Cocktail sort', u'Collation', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer bus', u'Counting sort', u'Cubesort', u'Cycle sort', u'Data structure', u'Decrease and conquer', u'Digital object identifier', u'Distributed algorithm', u'Divide and conquer algorithm', u'Domain of a function', u'Donald Knuth', u'Donald Shell', u'Endre Szemer\xe9di', u'External sorting', u'Fisher\u2013Yates shuffle', u'Flashsort', u'Ford-Johnson algorithm', u'Gnome sort', u'Heap (data structure)', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'In-place merge sort', u'Information Processing Letters', u'Insertion sort', u'Integer sorting', u'International Conference on Theory and Applications of Models of Computation', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'Inversion (discrete mathematics)', u'JDK7', u'JSort', u'Java (programming language)', u'Java version history', u'J\xe1nos Koml\xf3s (mathematician)', u'Least significant digit', u'Lecture Notes in Computer Science', u'Lexicographical order', u'Library sort', u'Linearithmic', u'List (computing)', u'Longest increasing subsequence', u'Median', u'Median of medians', u'Merge algorithm', u'Merge sort', u'Mergesort', u'Michael T. Goodrich', u'Mikkel Thorup', u'Mikl\xf3s Ajtai', u'Most significant digit', u'Na\xefve algorithm', u'Niklaus Wirth', u'Odd-even sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Partial sorting', u'Patience sorting', u'Perl', u'Permutation', u'Pigeonhole sort', u'Polyphase merge sort', u'Postman sort', u'Proxmap sort', u'Python (programming language)', u'Quantum sort', u'Quickselect', u'Quicksort', u'Radix sort', u'Random access', u'Randomized algorithm', u'Relational database', u'Roberto Tamassia', u'Ron Rivest', u'Samplesort', u'Schwartzian transform', u'Search algorithm', u'Selection algorithm', u'Selection sort', u'Self-balancing binary search tree', u'Shell sort', u'Shellsort', u'Shuffling algorithm', u'Smoothsort', u'Sort (C++)', u'Sorting', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Standard Template Library', u'Stooge sort', u'Strand sort', u'Swap (computer science)', u'Symposium on Foundations of Computer Science', u'Symposium on Theory of Computing', u'The Computer Journal', u'Thomas H. Cormen', u'Time-space tradeoff', u'Time complexity', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Unstable sort', u'Upper and lower bounds', u'Virtual memory', u'Worst-case performance']"
Sorting algorithm,"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (reordering) of the input.
Further, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.
Since the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.
Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.","[u'All articles lacking in-text citations', u'All articles with specifically marked weasel-worded phrases', u'All articles with unsourced statements', u'Articles lacking in-text citations from September 2009', u'Articles with specifically marked weasel-worded phrases from September 2015', u'Articles with unsourced statements from December 2010', u'Commons category with local link same as on Wikidata', u'Data processing', u'Sorting algorithms']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Array data type', u'Average performance', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Best-case performance', u'Big O notation', u'Binary tree', u'Binary tree sort', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Byte Magazine', u'Canonicalization', u'Cartesian tree', u'Cascade merge sort', u'Central Processing Unit', u'Charles E. Leiserson', u'CiteSeer', u'Clifford Stein', u'Cocktail sort', u'Collation', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer bus', u'Counting sort', u'Cubesort', u'Cycle sort', u'Data structure', u'Decrease and conquer', u'Digital object identifier', u'Distributed algorithm', u'Divide and conquer algorithm', u'Domain of a function', u'Donald Knuth', u'Donald Shell', u'Endre Szemer\xe9di', u'External sorting', u'Fisher\u2013Yates shuffle', u'Flashsort', u'Ford-Johnson algorithm', u'Gnome sort', u'Heap (data structure)', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'In-place merge sort', u'Information Processing Letters', u'Insertion sort', u'Integer sorting', u'International Conference on Theory and Applications of Models of Computation', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'Inversion (discrete mathematics)', u'JDK7', u'JSort', u'Java (programming language)', u'Java version history', u'J\xe1nos Koml\xf3s (mathematician)', u'Least significant digit', u'Lecture Notes in Computer Science', u'Lexicographical order', u'Library sort', u'Linearithmic', u'List (computing)', u'Longest increasing subsequence', u'Median', u'Median of medians', u'Merge algorithm', u'Merge sort', u'Mergesort', u'Michael T. Goodrich', u'Mikkel Thorup', u'Mikl\xf3s Ajtai', u'Most significant digit', u'Na\xefve algorithm', u'Niklaus Wirth', u'Odd-even sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Partial sorting', u'Patience sorting', u'Perl', u'Permutation', u'Pigeonhole sort', u'Polyphase merge sort', u'Postman sort', u'Proxmap sort', u'Python (programming language)', u'Quantum sort', u'Quickselect', u'Quicksort', u'Radix sort', u'Random access', u'Randomized algorithm', u'Relational database', u'Roberto Tamassia', u'Ron Rivest', u'Samplesort', u'Schwartzian transform', u'Search algorithm', u'Selection algorithm', u'Selection sort', u'Self-balancing binary search tree', u'Shell sort', u'Shellsort', u'Shuffling algorithm', u'Smoothsort', u'Sort (C++)', u'Sorting', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Standard Template Library', u'Stooge sort', u'Strand sort', u'Swap (computer science)', u'Symposium on Foundations of Computer Science', u'Symposium on Theory of Computing', u'The Computer Journal', u'Thomas H. Cormen', u'Time-space tradeoff', u'Time complexity', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Unstable sort', u'Upper and lower bounds', u'Virtual memory', u'Worst-case performance']"
Sorting algorithms,"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (reordering) of the input.
Further, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.
Since the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.
Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.","[u'All articles lacking in-text citations', u'All articles with specifically marked weasel-worded phrases', u'All articles with unsourced statements', u'Articles lacking in-text citations from September 2009', u'Articles with specifically marked weasel-worded phrases from September 2015', u'Articles with unsourced statements from December 2010', u'Commons category with local link same as on Wikidata', u'Data processing', u'Sorting algorithms']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Array data type', u'Average performance', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Best-case performance', u'Big O notation', u'Binary tree', u'Binary tree sort', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Byte Magazine', u'Canonicalization', u'Cartesian tree', u'Cascade merge sort', u'Central Processing Unit', u'Charles E. Leiserson', u'CiteSeer', u'Clifford Stein', u'Cocktail sort', u'Collation', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer bus', u'Counting sort', u'Cubesort', u'Cycle sort', u'Data structure', u'Decrease and conquer', u'Digital object identifier', u'Distributed algorithm', u'Divide and conquer algorithm', u'Domain of a function', u'Donald Knuth', u'Donald Shell', u'Endre Szemer\xe9di', u'External sorting', u'Fisher\u2013Yates shuffle', u'Flashsort', u'Ford-Johnson algorithm', u'Gnome sort', u'Heap (data structure)', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'In-place merge sort', u'Information Processing Letters', u'Insertion sort', u'Integer sorting', u'International Conference on Theory and Applications of Models of Computation', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'Inversion (discrete mathematics)', u'JDK7', u'JSort', u'Java (programming language)', u'Java version history', u'J\xe1nos Koml\xf3s (mathematician)', u'Least significant digit', u'Lecture Notes in Computer Science', u'Lexicographical order', u'Library sort', u'Linearithmic', u'List (computing)', u'Longest increasing subsequence', u'Median', u'Median of medians', u'Merge algorithm', u'Merge sort', u'Mergesort', u'Michael T. Goodrich', u'Mikkel Thorup', u'Mikl\xf3s Ajtai', u'Most significant digit', u'Na\xefve algorithm', u'Niklaus Wirth', u'Odd-even sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Partial sorting', u'Patience sorting', u'Perl', u'Permutation', u'Pigeonhole sort', u'Polyphase merge sort', u'Postman sort', u'Proxmap sort', u'Python (programming language)', u'Quantum sort', u'Quickselect', u'Quicksort', u'Radix sort', u'Random access', u'Randomized algorithm', u'Relational database', u'Roberto Tamassia', u'Ron Rivest', u'Samplesort', u'Schwartzian transform', u'Search algorithm', u'Selection algorithm', u'Selection sort', u'Self-balancing binary search tree', u'Shell sort', u'Shellsort', u'Shuffling algorithm', u'Smoothsort', u'Sort (C++)', u'Sorting', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Standard Template Library', u'Stooge sort', u'Strand sort', u'Swap (computer science)', u'Symposium on Foundations of Computer Science', u'Symposium on Theory of Computing', u'The Computer Journal', u'Thomas H. Cormen', u'Time-space tradeoff', u'Time complexity', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'Unstable sort', u'Upper and lower bounds', u'Virtual memory', u'Worst-case performance']"
Soundex,"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, Ingres, MS SQL Server and Oracle) and is often used (incorrectly) as a synonym for ""phonetic algorithm"". Improvements to Soundex are the basis for many modern phonetic algorithms.","[u'All articles with unsourced statements', u'Articles with unsourced statements from August 2007', u'Phonetic algorithms']","[u'Communications of the ACM', u'Consonant', u'Daitch\u2013Mokotoff Soundex', u'Donald Knuth', u'Double Metaphone', u'Encoding', u'Homophone', u'Index (publishing)', u'International Standard Book Number', u'Journal of the ACM', u'Labial consonant', u'Lawrence Philips', u'Letter (alphabet)', u'Match Rating Approach', u'Metaphone', u'Metonym', u'Microsoft SQL Server', u'MySQL', u'N-gram', u'National Archives and Records Administration', u'New York State Identification and Intelligence System', u'Numerical digit', u'OCLC', u'Oracle Database', u'Patent', u'Phonetic algorithm', u'Place of articulation', u'PostgreSQL', u'Pronunciation', u'Spelling', u'The Art of Computer Programming', u'The SoundEx', u'United States Census']"
Spaghetti sort,"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, by Alexander Dewdney in his column, Scientific American. This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. It requires a parallel processor.","[u'Accuracy disputes from July 2013', u'All Wikipedia articles needing clarification', u'All accuracy disputes', u'All articles with unsourced statements', u'Articles with unsourced statements from April 2015', u'Sorting algorithms', u'Wikipedia articles needing clarification from July 2013']","[u'Adaptive sort', u'Alexander Dewdney', u'Algorithm', u'American flag sort', u'Analog computer', u'Andrew Adamatzky', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Dietrich Stauffer', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'Introsort', u'JSort', u'Library sort', u'Linear-time', u'List (computing)', u'Luniver Press', u'Merge sort', u'Natural number', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel computing', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Scientific American', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort', u'World Scientific']"
Special number field sieve,"In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm. The general number field sieve (GNFS) was derived from it.
The special number field sieve is efficient for integers of the form re ± s, where r and s are small (for instance Mersenne numbers).
Heuristically, its complexity for factoring an integer  is of the form:

in O and L-notations.
The SNFS has been used extensively by NFSNet (a volunteer distributed computing effort), NFS@Home and others to factorise numbers of the Cunningham project; for some time the records for integer factorisation have been numbers factored by SNFS.",[u'Integer factorization algorithms'],"[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algebraic number field', u'Algorithm', u'Algorithmic efficiency', u'Ancient Egyptian multiplication', u'Arjen Lenstra', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Big O notation', u'Binary GCD algorithm', u'Carl Pomerance', u'Chakravala method', u""Cipolla's algorithm"", u'Computational complexity theory', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Cunningham project', u'Digital object identifier', u'Discrete logarithm', u'Distributed computing', u""Dixon's factorization method"", u'Elliptic curve method', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Fibonacci number', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Hendrik Lenstra', u'Heuristic', u'Index calculus algorithm', u'Integer', u'Integer factorization', u'Integer factorization records', u'Integer square root', u'International Standard Book Number', u'Irreducible polynomial', u'Karatsuba algorithm', u'L-notation', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Linear algebra', u'Long multiplication', u'Lucas number', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'MIT', u'Mathematics', u'Mersenne number', u'Miller\u2013Rabin primality test', u'Modular arithmetic', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u'Relatively prime', u'Ring (mathematics)', u'Ring homomorphism', u'Root of a function', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Smooth number', u'Solovay\u2013Strassen primality test', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Unique factorization domain', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Spectral layout,"Spectral layout is a class of algorithm for drawing graphs. The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.

","[u'All stub articles', u'Applied mathematics stubs', u'Graph algorithms', u'Graph drawing']","[u'Algorithm', u'Applied mathematics', u'Cartesian coordinate', u'Digital object identifier', u'Eigenvectors', u'Graph drawing', u'Laplace matrix', u'Mathematical Reviews']"
Spigot algorithm,"A spigot algorithm is an algorithm for computing the value of a mathematical constant such as π or e which generates output digits left to right, with limited intermediate storage.
The name comes from a ""spigot"", meaning a tap or valve controlling the flow of a liquid.
Interest in such algorithms was spurred in the early days of computational mathematics by extreme constraints on memory, and an algorithm for calculating the digits of e appears in a paper by Sale in 1968. The name ""Spigot algorithm"" appears to have been coined by Stanley Rabinowitz and Stan Wagon, whose algorithm for calculating the digits of π is sometimes referred to as ""the spigot algorithm for π"".
The spigot algorithm of Rabinowitz and Wagon is bounded, in the sense that the number of required digits must be specified in advance. Jeremy Gibbons (2004) uses the term ""streaming algorithm"" to mean one which can be run indefinitely, without a prior bound. A further refinement is an algorithm which can compute a single arbitrary digit, without first computing the preceding digits: an example is the Bailey-Borwein-Plouffe formula, a digit extraction algorithm for π which produces hexadecimal digits.

",[u'Computer arithmetic algorithms'],"[u'Algorithm', u'Bailey-Borwein-Plouffe formula', u'Digital object identifier', u'E (mathematical constant)', u'Eric W. Weisstein', u'Jeremy Gibbons', u'MathWorld', u'Modular arithmetic', u'Modular exponentiation', u'Natural logarithm', u'On-Line Encyclopedia of Integer Sequences', u'Pi', u'Precision (arithmetic)', u'Single precision', u'Tap (valve)']"
State-Action-Reward-State-Action,"State-Action-Reward-State-Action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was introduced in a technical note  where the alternative name SARSA was only mentioned as a footnote.
This name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent ""S1"", the action the agent chooses ""A1"", the reward ""R"" the agent gets for choosing this action, the state ""S2"" that the agent will now be in after taking that action, and finally the next action ""A2"" the agent will choose in its new state. Taking every letter in the quintuple (st, at, rt, st+1, at+1) yields the word SARSA.

",[u'Machine learning algorithms'],"[u'Algorithm', u'Digital object identifier', u'Machine learning', u'Markov decision process', u'PubMed Identifier', u'Q-learning', u'Reinforcement learning', u'Sarsa (disambiguation)', u'Temporal difference learning']"
Statistical classification,"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into ""spam"" or ""non-spam"" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).
In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a part word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. There is also some argument over whether classification methods that do not involve a statistical model can be considered ""statistical"". Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.","[u'All articles lacking in-text citations', u'All articles with unsourced statements', u'All pages needing cleanup', u'Articles lacking in-text citations from January 2010', u'Articles needing cleanup from May 2012', u'Articles with sections that need to be turned into prose from May 2012', u'Articles with unsourced statements from August 2014', u'Classification algorithms', u'Machine learning', u'Statistical classification']","[u'Accelerated failure time model', u'Accuracy', u'Actuarial science', u'Adaptive kernel density estimation', u'Akaike information criterion', u'Algorithm', u'Analysis of covariance', u'Analysis of variance', u'Anderson\u2013Darling test', u'Anomaly detection', u'Arithmetic mean', u'Artificial intelligence', u'Artificial neural network', u'Artificial neural networks', u'Association rule learning', u'Asymptotic theory (statistics)', u'Autocorrelation', u'Autoencoder', u'Autoregressive conditional heteroskedasticity', u'Autoregressive\u2013moving-average model', u'BIRCH', u'Bar chart', u'Bayes estimator', u'Bayes factor', u'Bayesian inference', u'Bayesian linear regression', u'Bayesian network', u'Bayesian probability', u'Bias-variance dilemma', u'Bias of an estimator', u'Binary classification', u'Binary data', u'Binomial regression', u'Bioinformatics', u'Biological classification', u'Biometric', u'Biometrika', u'Biostatistics', u'Biplot', u'Blocking (statistics)', u'Blood pressure', u'Blood type', u'Boosting (machine learning)', u'Boosting (meta-algorithm)', u'Bootstrap aggregating', u'Bootstrapping (statistics)', u'Box plot', u'Box\u2013Jenkins', u'Breusch\u2013Godfrey test', u'C. R. Rao', u'Canonical correlation analysis', u'Cartography', u'Categorical data', u'Categorical variable', u'Census', u'Chemometrics', u'Chi-square test', u'Class membership probabilities', u'Classification rule', u'Clinical study design', u'Clinical trial', u'Cluster analysis', u'Cluster sampling', u'Coefficient of determination', u'Coefficient of variation', u""Cohen's kappa"", u'Cointegration', u'Community ecology', u'Completeness (statistics)', u'Compound term processing', u'Computational learning theory', u'Computer vision', u'Conditional random field', u'Confidence interval', u'Confounding', u'Contingency table', u'Continuous probability distribution', u'Control chart', u'Convolutional neural network', u'Copula (probability theory)', u'Correlation and dependence', u'Correlogram', u'Count data', u'Credible interval', u'Credit scoring', u'Crime statistics', u'Cross-correlation', u'DBSCAN', u'Data collection', u'Data mining', u'Data warehouse', u'Decision tree learning', u'Decomposition of time series', u'Deep learning', u'Degrees of freedom (statistics)', u'Demographic statistics', u'Density estimation', u'Dependent variable', u'Descriptive statistics', u'Design of experiments', u'Dickey\u2013Fuller test', u'Digital object identifier', u'Dimensionality reduction', u'Discrete choice', u'Distance', u'Document classification', u'Dot product', u'Drug development', u'Drug discovery', u'Durbin\u2013Watson statistic', u'Econometrics', u'Effect size', u'Efficiency (statistics)', u'Email', u'Empirical distribution function', u'Empirical risk minimization', u'Engineering statistics', u'Ensemble learning', u'Environmental statistics', u'Epidemiology', u'Errors and residuals in statistics', u'Estimator', u'Expectation-maximization algorithm', u'Experiment', u'Explanatory variable', u'Explanatory variables', u'Exponential family', u'Exponential smoothing', u'F-test', u'Factor analysis', u'Factorial experiment', u'Failure rate', u'Fan chart (statistics)', u'Feature (pattern recognition)', u'Feature engineering', u'Feature learning', u'Feature space', u'Feature vector', u'First-hitting-time model', u""Fisher's linear discriminant"", u'Forest plot', u'Fourier analysis', u'Frequency distribution', u'Frequency domain', u'Frequentist inference', u'Function (mathematics)', u'Fuzzy logic', u'G-test', u'General linear model', u'Generalized linear model', u'Geographic information system', u'Geometric mean', u'Geostatistics', u'Glossary of statistics', u'Goodness of fit', u'Grammar induction', u'Granger causality', u'Graphical model', u'Grouped data', u'Handwriting recognition', u'Harmonic mean', u'Heteroscedasticity', u'Hidden Markov model', u'Hierarchical clustering', u'Histogram', u'Homoscedasticity', u'Independent component analysis', u'Independent variable', u'Index of dispersion', u'Information retrieval', u'Integer', u'International Standard Book Number', u'Interquartile range', u'Isotonic regression', u'Jarque\u2013Bera test', u'Johansen test', u'K-means clustering', u'K-nearest neighbor algorithm', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Kaplan\u2013Meier estimator', u'Kendall tau rank correlation coefficient', u'Kolmogorov\u2013Smirnov test', u'Kriging', u'Kruskal\u2013Wallis one-way analysis of variance', u'Kurtosis', u'L-moment', u'Learning to rank', u'Learning vector quantization', u'Least squares support vector machine', u'Likelihood-ratio test', u'Linear', u'Linear classifier', u'Linear combination', u'Linear discriminant analysis', u'Linear function', u'Linear predictor function', u'Linear regression', u'List of fields of application of statistics', u'List of statistics articles', u'Ljung\u2013Box test', u'Local outlier factor', u'Location parameter', u'Log-rank test', u'Logistic regression', u'Machine learning', u'Mahalanobis distance', u'Mann\u2013Whitney U test', u'Markov chain Monte Carlo', u'Maximum a posteriori estimation', u'Maximum likelihood', u""McNemar's test"", u'Mean', u'Mean-shift', u'Median', u'Median-unbiased estimator', u'Medical imaging', u'Medical statistics', u'Method of moments (statistics)', u'Methods engineering', u'Metric (mathematics)', u'Minimum-variance unbiased estimator', u'Minimum distance estimation', u'Mixed model', u'Mode (statistics)', u'Moment (mathematics)', u'Multiclass classification', u'Multilayer perceptron', u'Multinomial logistic regression', u'Multivariate adaptive regression splines', u'Multivariate analysis of variance', u'Multivariate normal distribution', u'Multivariate statistics', u'Naive Bayes classifier', u'National accounts', u'Natural experiment', u'Nelson\u2013Aalen estimator', u'No free lunch in search and optimization', u'Non-negative matrix factorization', u'Nonlinear', u'Nonlinear regression', u'Nonparametric regression', u'OPTICS algorithm', u'Observation', u'Observational study', u'Official statistics', u'Online machine learning', u'Opinion poll', u'Optical character recognition', u'Optimal design', u'Order statistic', u'Ordinal data', u'Ordinary least squares', u'Outline of statistics', u'Parametric statistics', u'Parse tree', u'Parsing', u'Part of speech', u'Part of speech tagging', u'Partial autocorrelation function', u'Partial correlation', u'Partition of sums of squares', u'Pattern recognition', u'Pearson product-moment correlation coefficient', u'Percentile', u'Perceptron', u'Permutation test', u'Pie chart', u'Poisson regression', u'Posterior probability', u'Power (statistics)', u'Precision and recall', u'Principal component analysis', u'Prior probability', u'Probabilistic classification', u'Probabilistic design', u'Probability', u'Probably approximately correct learning', u'Probit regression', u'Proportional hazards model', u'Psychometrics', u'Quadratic classifier', u'Quality control', u'Quantitative structure-activity relationship', u'Quasi-experiment', u'Questionnaire', u'Q\u2013Q plot', u'R. A. Fisher', u'Radar chart', u'Random assignment', u'Random forest', u'Randomization test', u'Randomized controlled trial', u'Randomized experiment', u'Range (statistics)', u'Rank correlation', u'Rank statistics', u'Real number', u'Receiver operating characteristic', u'Recommender system', u'Record value', u'Recurrent neural network', u'Regression analysis', u'Regression model validation', u'Reinforcement learning', u'Relevance vector machine', u'Reliability engineering', u'Replication (statistics)', u'Restricted Boltzmann machine', u'Robust regression', u'Robust statistics', u'Run chart', u'Sample size determination', u'Sampling (statistics)', u'Sampling distribution', u'Scan statistic', u'Scatter plot', u'Scientific control', u'Score test', u'Search engines', u'Seasonal adjustment', u'Self-organizing map', u'Semi-supervised learning', u'Semiparametric regression', u'Sequence labeling', u'Shape of the distribution', u'Shapiro\u2013Wilk test', u'Similarity function', u'Simple linear regression', u'Simultaneous equations model', u'Skewness', u'Social statistics', u'Spam filtering', u'Spatial analysis', u""Spearman's rank correlation coefficient"", u'Spectral density estimation', u'Speech recognition', u'Standard deviation', u'Standard error', u'Stationary process', u'Statistical dispersion', u'Statistical graphics', u'Statistical hypothesis testing', u'Statistical inference', u'Statistical learning theory', u'Statistical model', u'Statistical natural language processing', u'Statistical population', u'Statistical power', u'Statistical process control', u'Statistical theory', u'Statistically independent', u'Statistics', u'Stem-and-leaf display', u'Stratified sampling', u'Structural break', u'Structured prediction', u""Student's t-test"", u'Sufficient statistic', u'Supervised learning', u'Support vector machine', u'Survey methodology', u'Survival analysis', u'Survival function', u'Syntactic structure', u'System identification', u'T-distributed stochastic neighbor embedding', u'T. W. Anderson', u'Time domain', u'Time series', u'Toxicogenomics', u'Training set', u'Trend estimation', u'U-statistic', u'Uncertainty coefficient', u'University of Leicester', u'Unsupervised learning', u'Utility', u'Vapnik\u2013Chervonenkis theory', u'Variable kernel density estimation', u'Variance', u'Vector autoregression', u'Vector space', u'Video tracking', u'Wald test', u'Wavelet', u'Wilcoxon signed-rank test', u'Z-test']"
Steinhaus–Johnson–Trotter algorithm,"The Steinhaus–Johnson–Trotter algorithm or Johnson–Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter that generates all of the permutations of n elements. Each permutation in the sequence that it generates differs from the previous permutation by swapping two adjacent elements of the sequence. Equivalently, this algorithm finds a Hamiltonian path in the permutohedron.
This method was known already to 17th-century English change ringers, and Sedgewick (1977) calls it ""perhaps the most prominent permutation enumeration algorithm"". As well as being simple and computationally efficient, it has the advantage that subsequent computations on the permutations that it generates may be sped up because these permutations are so similar to each other.","[u'Combinatorial algorithms', u'Permutations']","[u'Algorithm', u'Carla Savage', u'Cayley graph', u'Change ringing', u'CiteSeer', u'Convex hull', u'Cut-the-knot', u'Digital object identifier', u'Donald Knuth', u'Edsger W. Dijkstra', u'Fabian Stedman', u'Factorial number system', u'Gray code', u'Hale F. Trotter', u'Hamiltonian path', u""Heap's algorithm"", u'Hugo Steinhaus', u'Inverse element', u'Inverse function', u'Inversion (discrete mathematics)', u'JSTOR', u'Loopless algorithm', u'Mathematical Reviews', u'Mixed radix', u'Nachum Dershowitz', u'Parity of a permutation', u'Permutation', u'Permutohedron', u'Polytope', u'Pseudocode', u'Radix', u'Recursive algorithm', u'Robert Sedgewick (computer scientist)', u'SIAM Review', u'Selmer M. Johnson', u'Shimon Even', u'Sourceforge', u'Symmetric group', u'The Art of Computer Programming', u'Truncated octahedron']"
Stochastic universal sampling,"Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination. It was introduced by James Baker.
SUS is a development of fitness proportionate selection (FPS) which exhibits no bias and minimal spread. Where FPS chooses several solutions from the population by repeated random sampling, SUS uses a single random value to sample all of the solutions by choosing them at evenly spaced intervals. This gives weaker members of the population (according to their fitness) a chance to be chosen and thus reduces the unfair nature of fitness-proportional selection methods.
Other methods like roulette wheel can have bad performance when a member of the population has a really large fitness in comparison with other members. Using a comb-like ruler, SUS starts from a small random number, and chooses the next candidates from the rest of population remaining, not allowing the fittest members to saturate the candidate space.
Described as an algorithm, pseudocode for SUS looks like:

SUS(Population, N)
    F := total fitness of Population
    N := number of offspring to keep
    P := distance between the pointers (F/N)
    Start := random number between 0 and P
    Pointers := [Start + i*P | i in [0..(N-1)]]
    return RWS(Population,Pointers)

RWS(Population, Points)
    Keep = []
    i := 0
    for P in Points
        while fitness sum of Population[0..i] < P
            i++
        add Population[i] to Keep
    return Keep

Where Population[0..i] is the set of individuals with array-index 0 to (and including) i.
Here RWS() describes the bulk of fitness proportionate selection (also known as ""roulette wheel selection"") - in true fitness proportional selection the parameter Points is always a (sorted) list of random numbers from 0 to F. The algorithm above is intended to be illustrative rather than canonical.","[u'Genetic algorithms', u'Pages using citations with accessdate and no URL', u'Stochastic algorithms']","[u'Fitness proportionate selection', u'Genetic algorithm', u'Reward-based selection', u'Roulette wheel']"
Stooge sort,"Stooge sort is a recursive sorting algorithm with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). The running time of the algorithm is thus slower compared to efficient sorting algorithms, such as Merge sort, and is even slower than Bubble sort, a canonical example of a fairly inefficient and simple sort.
The algorithm is defined as follows:
If the value at the end is smaller than the value at the start, swap them.
If there are 3 or more elements in the list, then:
Stooge sort the initial 2/3 of the list
Stooge sort the final 2/3 of the list
Stooge sort the initial 2/3 of the list again

else: exit the procedure
It is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data. However, if the code is written to end on a base case of size 1, rather than terminating on either size 1 or size 2, rounding the 2/3 of 2 upwards gives an infinite number of calls.
The algorithm gets its name from slapstick routines of The Three Stooges, in which each stooge hits the other two.","[u'All articles with unsourced statements', u'All stub articles', u'Articles with example pseudocode', u'Articles with unsourced statements from March 2010', u'Comparison sorts', u'Computer science stubs', u'Sorting algorithms', u'Use dmy dates from October 2010']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'JSort', u'Library sort', u'List (computing)', u'Merge sort', u'National Institute of Standards and Technology', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Recursion', u'Ron Rivest', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Slapstick', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Strand sort', u'The Three Stooges', u'Thomas H. Cormen', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Strand sort,"Strand sort is a sorting algorithm. It works by repeatedly pulling sorted sublists out of the list to be sorted and merging them with a result array. Each iteration through the unsorted list pulls out a series of elements which were already sorted, and merges those series together.
The name of the algorithm comes from the ""strands"" of sorted data within the unsorted list which are removed one at a time. It is a comparison sort due to its use of comparisons when removing strands and when merging them into the sorted array.
The strand sort algorithm is O(n2) in the average case. In the best case (a list which is already sorted) the algorithm is linear, or O(n). In the worst case (a list which is sorted in reverse order) the algorithm is O(n2).
Strand sort is most useful for data which is stored in a linked list, due to the frequent insertions and removals of data. Using another data structure, such as an array, would greatly increase the running time and complexity of the algorithm due to lengthy insertions and deletions. Strand sort is also useful for data which already has large amounts of sorted data, because such data can be removed in a single strand.","[u'Articles with example pseudocode', u'Comparison sorts', u'Sorting algorithms']","[u'Adaptive sort', u'American flag sort', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big-O notation', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Dictionary of Algorithms and Data Structures', u'Flashsort', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'Linked list', u'List (computing)', u'Merge sort', u'Mergesort', u'National Institute of Standards and Technology', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Pseudocode', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Strassen algorithm,"In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm used for matrix multiplication. It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices.

","[u'All articles needing additional references', u'Articles needing additional references from January 2015', u'Matrix multiplication algorithms']","[u'Abuse of notation', u'Algorithm', u'Basic Linear Algebra Subprograms', u'Bilinear map', u'Block matrix', u'CPU cache', u'Cache-oblivious algorithm', u'Charles E. Leiserson', u'Clifford Stein', u'Comparison of linear algebra libraries', u'Comparison of numerical analysis software', u'Computational complexity of mathematical operations', u'Coppersmith-Winograd algorithm', u'Coppersmith\u2013Winograd algorithm', u'Dot product', u'Dual space', u'Eric W. Weisstein', u'Floating point', u'Gauss\u2013Jordan elimination', u'Hadamard product (matrices)', u'International Standard Book Number', u'Introduction to Algorithms', u'Karatsuba algorithm', u'Linear algebra', u'MathWorld', u'Matrix decomposition', u'Matrix inversion', u'Matrix multiplication', u'Matrix multiplication algorithm', u'Multiplication algorithm', u'Multiprocessing', u'Numerical linear algebra', u'Numerical stability', u'Ring (mathematics)', u'Ronald L. Rivest', u'SIMD', u'Sch\xf6nhage\u2013Strassen algorithm', u'Sparse matrix', u'Springer-Verlag', u'Square matrix', u'Submatrices', u'System of linear equations', u'Tensor product', u'Thomas H. Cormen', u'Translation lookaside buffer', u'Volker Strassen', u'Z-order (curve)']"
Strongly polynomial,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm.","[u'All articles with unsourced statements', u'Analysis of algorithms', u'Articles with unsourced statements from July 2014', u'Computational complexity theory', u'Computational resources', u'Use dmy dates from September 2010']","[u'2-EXPTIME', u'3SAT', u'AKS primality test', u'Abstract machine', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Alan Cobham', u'Alexander Schrijver', u'Algorithm', u'Amortized time', u'Approximation algorithm', u'Approximation algorithms', u'ArXiv', u'Array data structure', u'Average-case complexity', u'Avi Wigderson', u'BPP (complexity)', u'BQP', u'Big O notation', u'Binary numeral system', u'Binary search', u'Binary search algorithm', u'Binary tree', u'Binary tree sort', u'Bounded-error probabilistic polynomial', u'Boyer\u2013Moore string search algorithm', u'Brute-force search', u'Bubble sort', u'Christos H. Papadimitriou', u""Cobham's thesis"", u'Cole-Vishkin algorithm', u'Comparison sort', u'Complexity Zoo', u'Complexity class', u'Computable function', u'Computation model', u'Computational complexity of mathematical operations', u'Computational complexity theory', u'Computer science', u'Conjunctive normal form', u'Content-addressable memory', u'Convolution theorem', u'DLOGTIME', u'DTIME', u'Decision problem', u'Deterministic Turing machine', u'Digital object identifier', u'Disjoint set data structure', u'Double exponential function', u'Dynamic programming', u'EXP', u'EXPTIME', u'E (complexity)', u'Element (math)', u'Elsevier', u'Euclidean algorithm', u'Exponential time hypothesis', u'Fast Fourier transform', u'Formal language', u'General number field sieve', u'Graph (mathematics)', u'Graph isomorphism problem', u'Greatest common divisor', u""Grover's algorithm"", u'Gr\xf6bner basis', u'Heapsort', u'In-place merge sort', u'Infra-exponential', u'Insertion sort', u'Instruction (computer science)', u'Integer factorization', u'International Standard Book Number', u'International Standard Serial Number', u'Introsort', u'Inverse Ackermann function', u'Iterated logarithm', u'Journal of Computer and System Sciences', u""Karmarkar's algorithm"", u'Kd-tree', u'L-notation', u'Lance Fortnow', u'Lecture Notes in Computer Science', u'Linear', u'Linear programming', u'Logarithm', u'Logarithmic growth', u'Logarithmic identities', u'L\xe1szl\xf3 Babai', u'L\xe1szl\xf3 Lov\xe1sz', u'Matrix chain multiplication', u'Maximum matching', u'Merge sort', u'Michael Sipser', u'Monge array', u'NP-complete', u'NP-hard', u'NP (complexity)', u'Noam Nisan', u'Non-deterministic Turing machine', u'Optimization (mathematics)', u'P (complexity)', u'P versus NP', u'P versus NP problem', u'P \u2260 NP', u'Parallel Random Access Machine', u'Parallel algorithm', u'Parallel computing', u'Parameterized complexity', u'Partial correlation', u'Patience sorting', u'Polygon triangulation', u'Polynomial expression', u'Presburger arithmetic', u'Priority queue', u'Probabilistic Turing machine', u'Product (mathematics)', u'Property testing', u'Pseudo-polynomial time', u'Quantifier elimination', u'Quantum Turing machine', u'Quantum algorithm', u'Quasilinear time', u'Quicksort', u'RP (complexity)', u'Raimund Seidel', u'Randomized algorithm', u'Real closed field', u'Recurrence relation', u'Reduction (complexity)', u'Repeated squaring', u'Robustness (computer science)', u'Running Time (film)', u'Running time', u'Russell Impagliazzo', u'Self-balancing binary search tree', u'Set cover', u'Shell sort', u'Smoothsort', u'Society for Industrial and Applied Mathematics', u'Sorting algorithm', u'Space complexity', u'Springer-Verlag', u'Steiner tree problem', u""Stirling's approximation"", u'String (computer science)', u'Traveling salesman problem', u'Travelling salesman problem', u'Turing machine', u""Ukkonen's Algorithm"", u'Upper bound', u'Worst-case complexity', u'ZPP (complexity)']"
Substring search,"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.
Let Σ be an alphabet (finite set). Formally, both the pattern and searched text are vectors of elements of Σ. The Σ may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (Σ = {0,1}) or DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.
In practice, how the string is encoded can affect the feasible string search algorithms. In particular if a variable width encoding is in use then it is slow (time proportional to N) to find the Nth character. This will significantly slow down many of the more advanced search algorithms. A possible solution is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.","[u'All articles needing additional references', u'Articles needing additional references from July 2013', u'String matching algorithms']","[u'Aho\u2013Corasick algorithm', u'Aho\u2013Corasick string matching algorithm', u'Algorithm', u'Alphabet (computer science)', u'Apostolico\u2013Giancarlo algorithm', u'Approximate string matching', u'Big O notation', u'Bioinformatics', u'Bitap algorithm', u'Boyer\u2013Moore string search algorithm', u'Boyer\u2013Moore\u2013Horspool algorithm', u'Charles E. Leiserson', u'Clifford Stein', u'Commentz-Walter algorithm', u'Comparison of regular expression engines', u'Compressed pattern matching', u'Computer science', u'Damerau\u2013Levenshtein distance', u'Depth-first search', u'Deterministic acyclic finite state automaton', u'Deterministic finite automaton', u'Digital object identifier', u'Directed acyclic word graph', u'Edit distance', u'Finite-state machine', u'Finite set', u'Fuzzy string searching', u'Generalized suffix tree', u'Hamming distance', u""Hirschberg's algorithm"", u'Introduction to Algorithms', u'Jaro\u2013Winkler distance', u'Knuth\u2013Morris\u2013Pratt algorithm', u'Lee distance', u'Levenshtein automaton', u'Levenshtein distance', u'List of regular expression software', u'Longest common subsequence', u'Longest common substring', u'Needleman\u2013Wunsch algorithm', u'Nondeterministic finite automaton', u'Parsing', u'Pattern', u'Pattern matching', u'Powerset construction', u'Rabin\u2013Karp algorithm', u'Rabin\u2013Karp string search algorithm', u'Regular expression', u'Regular grammar', u'Regular tree grammar', u'Ronald L. Rivest', u'Rope (data structure)', u'Sequence alignment', u'Sequential pattern mining', u'Smith\u2013Waterman algorithm', u'String (computer science)', u'String algorithm', u'String metric', u'Substring index', u'Suffix array', u'Suffix automaton', u'Suffix tree', u'Ternary search tree', u'Thomas H. Cormen', u""Thompson's construction"", u'Trie', u'Trigram search', u'Variable width encoding', u'Wagner\u2013Fischer algorithm']"
Sukhotin's algorithm,"Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in.","[u'Classification algorithms', u'Natural language processing']","[u'Algorithm', u'Computational linguistics', u'Consonant', u'Cryptanalysis', u'Statistical classification', u'Substitution cipher', u'Vowel', u'Voynich manuscript']"
Support Vector Machines,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.","[u'All articles with unsourced statements', u'Articles with unsourced statements from February 2015', u'Articles with unsourced statements from June 2013', u'Classification algorithms', u'Statistical classification', u'Support vector machines', u'Wikipedia articles with GND identifiers']","[u'Alexey Chervonenkis', u'Algorithm', u'Anomaly detection', u'ArXiv', u'Artificial neural network', u'Association for Computing Machinery', u'Association rule learning', u'Autoencoder', u'BIRCH', u'Bayesian network', u'Bayesian optimization', u'Bernhard E. Boser', u'Bias-variance dilemma', u'Bibcode', u'Binary classification', u'Binary classifier', u'Boosting (machine learning)', u'Bootstrap aggregating', u'Canonical correlation analysis', u'Class membership probabilities', u'Cluster analysis', u'Computational learning theory', u'Conditional random field', u'Convolutional neural network', u'Coordinate descent', u'Corinna Cortes', u'Cross-validation (statistics)', u'DBSCAN', u'Data mining', u'Decision tree learning', u'Deep learning', u'Digital object identifier', u'Dimensionality reduction', u'Directed acyclic graph', u'Dot product', u'Dual problem', u'Empirical risk minimization', u'Ensemble learning', u'Expectation-maximization algorithm', u'Factor analysis', u'Feature engineering', u'Feature learning', u'Feature space', u'Fisher kernel', u'Gaussian', u'Generalization error', u'Grammar induction', u'Graphical model', u'Grid search', u'Hidden Markov model', u'Hierarchical clustering', u'High-dimensional space', u'Hilbert space', u'Homogeneous polynomial', u'Hyperbolic function', u'Hyperplane', u'In situ adaptive tabulation', u'Independent component analysis', u'Integrated Authority File', u'Interior point method', u'International Standard Book Number', u'Isabelle M. Guyon', u'John Shawe-Taylor', u'Journal of Machine Learning Research', u'K-means clustering', u'K-nearest neighbors algorithm', u'K-nearest neighbors classification', u'Karush\u2013Kuhn\u2013Tucker conditions', u'Kernel (integral operator)', u'Kernel machines', u'Kernel trick', u'LIBSVM', u'Lagrange multipliers', u'Learning to rank', u'Least squares support vector machine', u'Lecture Notes in Computer Science', u'Linear classifier', u'Linear discriminant analysis', u'Linear regression', u'Linear separability', u'Linearly separable', u'Local outlier factor', u'Logistic regression', u'MATLAB', u'Machine Learning (journal)', u'Machine learning', u'Margin classifier', u'Maximum-margin hyperplane', u'Mean-shift', u'Minimum Message Length', u'Minimum message length', u'Multiclass problem', u'Multilayer perceptron', u'Multivariate adaptive regression splines', u'Naive Bayes classifier', u'Nello Cristianini', u""Newton's method"", u'Non-negative matrix factorization', u'Nonlinear', u'Normal (geometry)', u'OPTICS algorithm', u'Online machine learning', u'Optimization (mathematics)', u'Paris Kanellakis Award', u'Perceptron', u'Platt scaling', u'Polynomial kernel', u'Positive-definite kernel', u'Predictive analytics', u'Principal component analysis', u'Probabilistic classification', u'Probably approximately correct learning', u'Quadratic programming', u'Radial basis function', u'Radial basis function kernel', u'Random forest', u'Rate of convergence', u'Real number', u'Recurrent neural network', u'Regression analysis', u'Regularization (mathematics)', u'Regularization perspectives on support vector machines', u'Reinforcement learning', u'Relevance vector machine', u'Restricted Boltzmann machine', u'Saddle point', u'Scikit-learn', u'Secure Virtual Machine', u'Self-organizing map', u'Semi-supervised learning', u'Sequential Minimal Optimization', u'Sequential minimal optimization', u'Shogun (toolbox)', u'Space mapping', u'Statistical classification', u'Statistical learning theory', u'Stochastic gradient descent', u'Structured SVM', u'Structured prediction', u'Supervised learning', u'T-distributed stochastic neighbor embedding', u'Tikhonov regularization', u'Transduction (machine learning)', u'Unsupervised learning', u'Vapnik\u2013Chervonenkis theory', u'Vladimir N. Vapnik', u'Weka (machine learning)', u'Winnow (algorithm)']"
Swarm intelligence,"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.
SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of ""intelligent"" global behavior, unknown to the individual agents. Examples in natural systems of SI include ant colonies, bird flocking, animal herding, bacterial growth, fish schooling and microbial intelligence.
The application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms. 'Swarm prediction' has been used in the context of forecasting problems.

","[u'All articles needing additional references', u'All articles with unsourced statements', u'Articles needing additional references from August 2013', u'Articles with unsourced statements from September 2015', u'Behavioral and social facets of systemic risk', u'Collective intelligence', u'Intelligence by type', u'Multi-agent systems', u'Optimization algorithms and methods']","[u'Active matter', u'Agent-based model', u'Agent-based model in biology', u'Algorithm', u'Allee effect', u'Altitudinal migration', u'Animal cognition', u'Animal communication', u'Animal consciousness', u'Animal echolocation', u'Animal language', u'Animal migration', u'Animal migration tracking', u'Animal navigation', u'Ant-based routing', u'Ant colony', u'Ant colony optimization', u'Ant colony optimization algorithms', u'Ant robotics', u'Approximation algorithm', u'ArXiv', u'Artificial Ants', u'Artificial bee colony algorithm', u'Artificial immune systems', u'Artificial intelligence', u'Atlantis', u'Augmented Lagrangian method', u'Bacteria', u'Bait ball', u'Barrier function', u'Bat algorithm', u'Batman Returns', u'Bees algorithm', u'Bellman\u2013Ford algorithm', u'Bibcode', u'Bird intelligence', u'Bird migration', u'Boids', u""Bor\u016fvka's algorithm"", u'Brain-to-body mass ratio', u'Brain size', u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Bruce Sterling', u'Cat intelligence', u'Cell migration', u'Cellular automaton', u'Cephalopod intelligence', u'Cetacean intelligence', u'CiteSeer', u'Clustering of self-propelled particles', u'Coded wire tag', u'Cognitive bias in animals', u'Cognitive ethology', u'Collective animal behavior', u'Collective behavior', u'Collective intelligence', u'Collective motion', u'Combinatorial optimization', u'Comparative cognition', u'Comparison of optimization software', u'Complex systems', u'Convex minimization', u'Convex optimization', u'Craig Reynolds (computer graphics)', u'Criss-cross algorithm', u'Crowd simulation', u'Cutting-plane method', u'Daniel Suarez (author)', u'Davidon\u2013Fletcher\u2013Powell formula', u'Decentralised system', u'Decentralization', u'Decipher (novel)', u'Deleuze', u'Diel vertical migration', u'Differential evolution', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dinosaur intelligence', u'Dog intelligence', u'Douglas A. Lawson', u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Elephant cognition', u'Ellipsoid method', u'Emergence', u'Emergent behaviour', u'Emotion in animals', u'Encephalization quotient', u""Ender's Game"", u'European Space Agency', u'Eusociality', u'Evolution of human intelligence', u'Evolutionary algorithm', u'Evolutionary computation', u'Exchange algorithm', u'Face Dancer', u'Feeding frenzy', u'Firefly algorithm', u'Fish intelligence', u'Fish migration', u'Fitness (biology)', u'Flock (birds)', u'Flocking (behavior)', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Formics', u'Frank Sch\xe4tzing', u'Frank\u2013Wolfe algorithm', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'George A. Bekey', u'Gerardo Beni', u'Geth', u'Global brain', u'Global optimization', u'Glowworm', u'Glowworm swarm optimization', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Group mind (science fiction)', u'Group size measures', u'Hallucination (short story)', u'Halo (series)', u'Harmony search', u'Herd', u'Herd behavior', u'Herding', u'Hessian matrix', u'Heuristic algorithm', u'Hewlett Packard', u'Hill climbing', u'Homing (biology)', u'Honey bees', u'Hybrid algorithm', u'Hypothesis', u'Insect migration', u'Integer programming', u'Intelligent Small World Autonomous Robots for Micro-manipulation', u'Intelligent agent', u'International Standard Book Number', u'Isaac Asimov', u'Iterative method', u'J. Phys. A', u'James Kennedy (social psychologist)', u'Jean-Baptiste Waldner', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u'Khrone', u'Kill Decision', u""Kruskal's algorithm"", u'Last and First Men', u'Law of gravity', u'Legends of Dune', u""Lemke's algorithm"", u'Lepidoptera migration', u'Leptothorax acervorum', u'Lessepsian migration', u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'List of animals by number of neurons', u'Local convergence', u'Local minima', u'Local search (optimization)', u'Louis B. Rosenberg', u'Luca Maria Gambardella', u'Luciferin', u'Luminescence', u'M. Anthony Lewis (roboticist)', u'Marco Dorigo', u'Mars', u'Mass Effect', u'Massive (software)', u'Mathematical optimization', u'Matroid', u'Metaheuristic', u'Michael Crichton', u'Michael Ende', u'Microbat', u'Microbial intelligence', u'Microbotics', u'Minimum spanning tree', u'Mirror test', u'Mixed-species foraging flock', u'Mobbing (animal behavior)', u'Monarch butterfly migration', u'Multi-agent system', u'Multi-swarm optimization', u'Mutualism (biology)', u'Myrmecology', u'NASA', u'Nanobots', u'Natal homing', u'National Geographic Magazine', u'Nelder\u2013Mead method', u'Neuroethology', u'Neuroscience and intelligence', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Observational learning', u'Olaf Stapledon', u'Omnius', u'Optimization (mathematics)', u'Optimization algorithm', u'Otto-von-Guericke University of Magdeburg', u'Pack (canine)', u'Pack hunter', u'Pain in animals', u'Pain in crustaceans', u'Pain in fish', u'Pain in invertebrates', u'Parameter space', u'Particle swarm optimization', u'Patterns of self-organization in ants', u'Penalty method', u'Penumbra: Black Plague', u'Pheromone', u'Philopatry', u'Physica A', u'Physical Review Letters', u""Powell's method"", u'Predator satiation', u'Prey (novel)', u'Primate cognition', u'Probabilistic algorithm', u'Promise theory', u'PubMed Identifier', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quarians', u'Quasi-Newton method', u'Quorum sensing', u'Radio waves', u'Random selection', u'Reinforcement learning', u'Reverse migration (birds)', u'Revised simplex algorithm', u'Robot', u'Rudy Rucker', u'Rule 110', u'Russell C. Eberhart', u'Salmon run', u'Sandworms of Dune', u'Sardine run', u'Science fiction', u'Sea turtle migration', u'Self-organization', u'Self-organized criticality', u'Self-propelled particles', u'Sequential quadratic programming', u'Shoaling and schooling', u'Simplex algorithm', u'Simulated annealing', u'Sort sol (bird flock)', u'Southwest Airlines', u'Spatial organization', u'Stanis\u0142aw Lem', u'Stanley and Stella in: Breaking the Ice', u'Star Maker', u'Starcraft (series)', u'Stel Pavlou', u'Stigmergy', u'Stochastic Diffusion Search', u'Stochastic diffusion search', u'Stochastic optimization', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Swarm (novelette)', u'Swarm (simulation)', u'Swarm Development Group', u'Swarm behaviour', u'Swarm robotics', u'Swarming (honey bee)', u'Swarming (military)', u'Swarming motility', u'SwisTrack', u'Symbrion', u'Symmetric rank-one', u'Symmetry breaking of escaping ants', u'Tabu search', u'Talking bird', u'Tamas Vicsek', u'Task allocation and partitioning of social insects', u'Telecommunications network', u'The Andromeda Strain', u'The Hacker and the Ants', u'The Invincible', u'The Lord of the Rings (film series)', u'The Neverending Story', u'The New York Times', u'The Swarm (novel)', u'The Wisdom of Crowds', u'Tool use by animals', u'Truncated Newton method', u'Trust region', u'Unmanned combat air vehicle', u'Velocity', u'Vicsek model', u'Vocal learning', u'Waggle dance', u'Weaver ant', u'Wisdom of the crowd', u'Wolfe conditions', u'Ygramul', u'Yuhui Shi', u'Zerg']"
Sweep line algorithm,"In computational geometry, a sweep line algorithm or plane sweep algorithm is a type of algorithm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. It is one of the key techniques in computational geometry.
The idea behind algorithms of this type is to imagine that a line (often a vertical line) is swept or moved across the plane, stopping at some points. Geometric operations are restricted to geometric objects that either intersect or are in the immediate vicinity of the sweep line whenever it stops, and the complete solution is available once the line has passed over all objects.

","[u'All articles with unsourced statements', u'Articles with unsourced statements from May 2009', u'Geometric algorithms']","[u'Analysis of algorithms', u'Bentley\u2013Ottmann algorithm', u'Big O notation', u'Boolean operations on polygons', u'Computational geometry', u'Computer graphics', u'Delaunay triangulation', u'Diane Souvaine', u'Digital object identifier', u'Euclidean space', u""Fortune's algorithm"", u'Integrated circuit layout', u'Line segment intersection', u'Michael Ian Shamos', u'Projective dual', u'Rotating calipers', u'Scanline algorithm', u'Self-balancing binary search tree', u'Voronoi diagram']"
Symmetric key algorithm,"Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption.","[u'All articles needing additional references', u'All articles with unsourced statements', u'Articles needing additional references from April 2012', u'Articles with unsourced statements from April 2012', u'Cryptographic algorithms', u'Wikipedia articles with GND identifiers']","[u'3-Way', u'3-subset meet-in-the-middle attack', u'A5/1', u'A5/2', u'ARIA (cipher)', u'Achterbahn', u'Advanced Encryption Standard', u'Advanced Encryption Standard process', u'Akelarre (cipher)', u'Algorithm', u'Anubis (cipher)', u'Asymmetric-key cryptography', u'Avalanche effect', u'BATON', u'BEAR and LION ciphers', u'BaseKing', u'BassOmatic', u'Biclique attack', u'Block cipher', u'Block cipher mode of operation', u'Block cipher modes of operation', u'Block size (cryptography)', u'Blowfish (cipher)', u'Boomerang attack', u'Brute-force attack', u'CAST-128', u'CAST-256', u'CAST5', u'CBC-MAC', u'CIKS-1', u'CIPHERUNICORN-A', u'CIPHERUNICORN-E', u'CLEFIA', u'COCONUT98', u'CRYPTON', u'CRYPTREC', u'CS-Cipher', u'Camellia (cipher)', u'Cellular Message Encryption Algorithm', u'Chi-square test', u'Chiasmus (cipher)', u'Chosen plaintext attack', u'Cipher security summary', u'Ciphertext', u'CiteSeer', u'Cobra ciphers', u'Correlation attack', u'Correlation immunity', u'Crab (cipher)', u'Cryptanalysis', u'Cryptographic hash function', u'Cryptographic key', u'Cryptographic primitive', u'Cryptographically secure pseudorandom number generator', u'Cryptography', u'Cryptomeria cipher', u'DEAL', u'DES-X', u'DES supplementary material', u'DFC (cipher)', u'Data Encryption Standard', u""Davies' attack"", u'Differential-linear attack', u'Differential cryptanalysis', u'Distinguishing attack', u'E0 (cipher)', u'E2 (cipher)', u'EFF DES cracker', u'ESTREAM', u'Entropy (information theory)', u'F-FCSR', u'FEA-M', u'FEAL', u'FISH (cipher)', u'FROG', u'Feistel cipher', u'GDES', u'GOST (block cipher)', u'Grain (cipher)', u'Grand Cru (cipher)', u'HC-256', u'Hash function', u'Hasty Pudding cipher', u'Hierocrypt', u'Higher-order differential cryptanalysis', u'History of cryptography', u'ICE (cipher)', u'IDEA NXT', u'ISAAC (cipher)', u'Impossible differential cryptanalysis', u'Initialization vector', u'Integral cryptanalysis', u'Integrated Authority File', u'Intel Cascade Cipher', u'International Data Encryption Algorithm', u'International Standard Book Number', u'Interpolation attack', u'Iraqi block cipher', u'KASUMI', u'KHAZAD', u'KN-Cipher', u'KeeLoq', u'Kendall tau rank correlation coefficient', u'Key schedule', u'Key size', u'Key whitening', u'Khufu and Khafre', u'Known-key distinguishing attack', u'Known-plaintext attack', u'LOKI', u'LOKI97', u'Ladder-DES', u'Lai-Massey scheme', u'Libelle (cipher)', u'Linear cryptanalysis', u'Linear feedback shift register', u'Lucifer (cipher)', u'M6 (cipher)', u'M8 (cipher)', u'MAGENTA', u'MARS (cryptography)', u'MESH (cipher)', u'MICKEY', u'MISTY1', u'MMB', u'MUGI', u'MULTI2', u'MacGuffin (cipher)', u'Madryga', u'Meet-in-the-middle attack', u'Mercy (cipher)', u'Message authentication code', u'Mod n cryptanalysis', u'MultiSwap', u'NESSIE', u'NIST', u'NLFSR', u'NOEKEON', u'NUSH', u'NewDES', u'New Data Seal', u'Nimbus (cipher)', u'Non-repudiation', u'One-way compression function', u'Outline of cryptography', u'PRESENT (cipher)', u'Padding (cryptography)', u'Panama (cryptography)', u'Partitioning cryptanalysis', u'Permutation box', u'Phelix', u'Pike (cipher)', u'Piling-up lemma', u'Plaintext', u'Product cipher', u'Public-key cryptography', u'Public-key encryption', u'Py (cipher)', u'QUAD (cipher)', u'Q (cipher)', u'RC2', u'RC4', u'RC5', u'RC6', u'REDOC', u'Rabbit (cipher)', u'Rebound attack', u'Red Pike (cipher)', u'Related-key attack', u'Rotational cryptanalysis', u'S-1 block cipher', u'S-box', u'SAFER', u'SAVILLE', u'SC2000', u'SEAL (cipher)', u'SEED', u'SHACAL', u'SHARK', u'SMS4', u'SNOW', u'SOBER', u'SOBER-128', u'SOSEMANUK', u'SXAL/MBAL', u'Salsa20', u'Scream (cipher)', u'Serpent (cipher)', u'Shared secret', u'Shift register', u'Shrinking generator', u'Simon (cipher)', u'Skipjack (cipher)', u'Slide attack', u'Speck (cipher)', u'Spectr-H64', u'Square (cipher)', u'Steganography', u'Stream cipher', u'Substitution-permutation network', u'T-function', u'Threefish', u'Time/memory/data tradeoff attack', u'Timing attack', u'Tiny Encryption Algorithm', u'Treyfer', u'Triple DES', u'Trivium (cipher)', u'Truncated differential cryptanalysis', u'Twofish', u'UES (cipher)', u'VEST', u'WAKE (cipher)', u'Weak key', u'Whitening transformation', u'XSL attack', u'XTEA', u'XXTEA', u'Xenon (cipher)', u'Xmx', u'Zodiac (cipher)']"
Tabu search,"Tabu search, created by Fred W. Glover in 1986 and formalized in 1989, is a metaheuristic search method employing local search methods used for mathematical optimization.
Local (neighborhood) searches take a potential solution to a problem and check its immediate neighbors (that is, solutions that are similar except for one or two minor details) in the hope of finding an improved solution. Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit.
Tabu search enhances the performance of local search by relaxing its basic rule. First, at each step worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local mimimum). In addition, prohibitions (henceforth the term tabu) are introduced to discourage the search from coming back to previously-visited solutions.
The implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, it is marked as ""tabu"" (forbidden) so that the algorithm does not consider that possibility repeatedly.","[u'1989 introductions', u'Optimization algorithms and methods']","[u'Algorithm', u'Ant colony optimization algorithms', u'Approximation algorithm', u'Augmented Lagrangian method', u'Barrier function', u'Bellman\u2013Ford algorithm', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branch and cut', u'Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm', u'Combinatorial optimization', u'Comparison of optimization software', u'Convex minimization', u'Convex optimization', u'Criss-cross algorithm', u'Cutting-plane method', u'Davidon\u2013Fletcher\u2013Powell formula', u'Digital object identifier', u""Dijkstra's algorithm"", u""Dinic's algorithm"", u'Dynamic programming', u'Edmonds\u2013Karp algorithm', u'Ellipsoid method', u'Evolutionary algorithm', u'Exchange algorithm', u'Flow network', u'Floyd\u2013Warshall algorithm', u'Ford\u2013Fulkerson algorithm', u'Frank\u2013Wolfe algorithm', u'Fred W. Glover', u'Function (mathematics)', u'Gauss\u2013Newton algorithm', u'Genetic algorithm', u'Golden section search', u'Gradient', u'Gradient descent', u'Graph algorithm', u'Greedy algorithm', u'Greedy randomized adaptive search procedure', u'Guided Local Search', u'Hessian matrix', u'Heuristic algorithm', u'Hill climbing', u'Integer programming', u'Iterative method', u""Johnson's algorithm"", u""Karmarkar's algorithm"", u""Kruskal's algorithm"", u""Lemke's algorithm"", u'Levenberg\u2013Marquardt algorithm', u'Limited-memory BFGS', u'Line search', u'Linear programming', u'Local convergence', u'Local optima', u'Local search (optimization)', u'Mathematical optimization', u'Matroid', u'Maxima and minima', u'Metaheuristic', u'Minimum spanning tree', u'NP-hard', u'Nearest neighbor algorithm', u'Nelder\u2013Mead method', u""Newton's method in optimization"", u'Nonlinear conjugate gradient method', u'Nonlinear programming', u'Optimization (mathematics)', u'Optimization algorithm', u'Pattern classification', u'Penalty method', u'Polynesia', u""Powell's method"", u'Pseudocode', u'Push\u2013relabel maximum flow algorithm', u'Quadratic programming', u'Quasi-Newton method', u'Reactive search optimization', u'Resource planning', u'Revised simplex algorithm', u'Satisficing', u'Sequential quadratic programming', u'Simplex algorithm', u'Simulated annealing', u'Subgradient method', u'Subroutine', u'Successive linear programming', u'Successive parabolic interpolation', u'Symmetric rank-one', u'Taboo', u'Tonga', u'Traveling salesman problem', u'Truncated Newton method', u'Trust region', u'VLSI design', u'Wolfe conditions']"
Tarjan's off-line least common ancestors algorithm,"In computer science, Tarjan's off-line lowest common ancestors algorithm is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure. The lowest common ancestor of two nodes d and e in a rooted tree T is the node g that is an ancestor of both d and e and that has the greatest depth in T. It is named after Robert Tarjan, who discovered the technique in 1979. Tarjan's algorithm is an offline algorithm; that is, unlike other lowest common ancestor algorithms, it requires that all pairs of nodes for which the lowest common ancestor is desired must be specified in advance. The simplest version of the algorithm uses the union-find data structure, which unlike other lowest common ancestor data structures can take more than constant time per operation when the number of pairs of nodes is similar in magnitude to the number of nodes. A later refinement by Gabow & Tarjan (1983) speeds the algorithm up to linear time.",[u'Graph algorithms'],"[u'Algorithm', u'Computer science', u'Digital object identifier', u'Disjoint-set data structure', u'Journal of the ACM', u'Linear time', u'Lowest common ancestor', u'Robert Tarjan', u'Rooted tree', u""Tarjan's strongly connected components algorithm"", u'Union-find']"
Tarjan's strongly connected components algorithm,"Tarjan's Algorithm is an algorithm in graph theory for finding the strongly connected components of a graph. Although proposed earlier, it can be seen as an improved version of Kosaraju's algorithm, and is comparable in efficiency to the path-based strong component algorithm. Tarjan's Algorithm is named for its discoverer, Robert Tarjan.","[u'Articles with example pseudocode', u'Graph algorithms', u'Graph connectivity']","[u'Algorithm', u'Best, worst and average case', u'Digital object identifier', u'Directed acyclic graph', u'Directed graph', u'Graph (data structure)', u'Graph theory', u'Invariant (computer science)', u""Kosaraju's algorithm"", u'Partition of a set', u'Path-based strong component algorithm', u'Robert Tarjan', u'SIAM Journal on Computing', u'Spanning forest', u'Stack (data structure)', u'Strongly connected component', u'Topological sorting', u'Vertex (graph theory)']"
Temporal difference learning,"Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be ""a combination of Monte Carlo ideas and dynamic programming (DP) ideas."" TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning.
As a prediction method, TD learning takes into account the fact that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that we adjust predictions to match other, more accurate, predictions about the future. This procedure is a form of bootstrapping, as illustrated with the following example:
Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives.
Mathematically speaking, both in a standard and a TD approach, we would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach we in some sense assume E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the Bellman equation of the return.","[u'Computational neuroscience', u'Machine learning algorithms']","[u'Algorithm', u'Arthur Samuel', u'Backgammon', u'Bellman equation', u'Bootstrapping (machine learning)', u'Digital object identifier', u'Dopamine', u'Dynamic programming', u'Gerald Tesauro', u'International Standard Book Number', u'Machine learning', u'Monte Carlo method', u'Neurons', u'Neuroscience', u'PVLV', u'PubMed Identifier', u'Q-learning', u'Reinforcement learning', u'Rescorla-Wagner model', u'Reward system', u'Richard S. Sutton', u'Sampling (statistics)', u'Schizophrenia', u'State-Action-Reward-State-Action', u'Substantia nigra', u'TD-Gammon', u'Ventral tegmental area']"
Ternary search,"A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function. A ternary search determines either that the minimum or maximum cannot be in the first third of the domain or that it cannot be in the last third of the domain, then repeats on the remaining two-thirds. A ternary search is an example of a divide and conquer algorithm (see search algorithm).","[u'All articles lacking sources', u'Articles lacking sources from May 2007', u'Mathematical optimization', u'Search algorithms']","[u'Binary search algorithm', u'Computer science', u'Divide and conquer algorithm', u'Exponential search', u'Golden section search', u'Interpolation search', u'Linear search', u'Maxima and minima', u""Newton's method in optimization"", u'Search algorithm', u'Unimodal', u'Unimodality']"
Timsort,"Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was invented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsets of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subset, called a run, with existing runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, and in GNU Octave.","[u'All articles with dead external links', u'Articles with dead external links from June 2013', u'Comparison sorts', u'Sorting algorithms', u'Stable sorts']","[u'Adaptive sort', u'Algorithm', u'American flag sort', u'Android (operating system)', u'Array data structure', u'Array data type', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary search', u'Binary search algorithm', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'C++', u'CPU cache', u'CPython', u'C (programming language)', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Flashsort', u'Formal verification', u'Function call', u'GNU Octave', u'Gnome sort', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'In-place merge sort', u'Information theory', u'Insertion sort', u'Integer sorting', u'International Standard Book Number', u'Introsort', u'JSort', u'Java 7', u'KeY', u'Library sort', u'Linear search', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Python (programming language)', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Shellsort', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsort', u'Stack (data structure)', u'Stooge sort', u'Strand sort', u'Tim Peters software engineer', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree sort']"
Tomasulo algorithm,"Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution, designed to efficiently utilize multiple execution units. It was developed by Robert Tomasulo at IBM in 1967, and first implemented in the IBM System/360 Model 91’s floating point unit.
The major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.
Robert Tomasulo received the Eckert-Mauchly Award in 1997 for his work on the algorithm.","[u'Algorithms', u'Instruction processing', u'Wikipedia articles needing clarification from March 2015']","[u'Algorithm', u'Arithmetic logic unit', u'Computer Architecture: A Quantitative Approach', u'Computer architecture', u'Digital object identifier', u'Eckert-Mauchly Award', u'Elsevier', u'Exception (computing)', u'Floating point unit', u'Hazard (computer architecture)', u'IBM', u'IBM System/360', u'Instruction level parallelism', u'International Standard Book Number', u'International Standard Serial Number', u'Out-of-order execution', u'Parallel computing', u'Re-order buffer', u'Register renaming', u'Reservation station', u'Robert Tomasulo', u'Scoreboarding']"
Toom–Cook multiplication,"Toom–Cook, sometimes known as Toom-3, named after Andrei Toom, who introduced the new algorithm with its low complexity, and Stephen Cook, who cleaned the description of it, is a multiplication algorithm, a method of multiplying two large integers.
Given two large integers, a and b, Toom–Cook splits up a and b into k smaller parts each of length l, and performs operations on the parts. As k grows, one may combine many of the multiplication sub-operations, thus reducing the overall complexity of the algorithm. The multiplication sub-operations can then be computed recursively using Toom–Cook multiplication again, and so on. Although the terms ""Toom-3"" and ""Toom–Cook"" are sometimes incorrectly used interchangeably, Toom-3 is only a single instance of the Toom–Cook algorithm, where k = 3.
Toom-3 reduces 9 multiplications to 5, and runs in Θ(nlog(5)/log(3)), about Θ(n1.465). In general, Toom-k runs in Θ(c(k) ne), where e = log(2k − 1) / log(k), ne is the time spent on sub-multiplications, and c is the time spent on additions and multiplication by small constants. The Karatsuba algorithm is a special case of Toom–Cook, where the number is split into two smaller ones. It reduces 4 multiplications to 3 and so operates at Θ(nlog(3)/log(2)), which is about Θ(n1.585). Ordinary long multiplication is equivalent to Toom-1, with complexity Θ(n2).
Although the exponent e can be set arbitrarily close to 1 by increasing k, the function c unfortunately grows very rapidly. The growth rate for mixed-level Toom-Cook schemes was still an open research problem in 2005. An implementation described by Donald Knuth achieves the time complexity Θ(n 2√(2 log n) log n).
Due to its overhead, Toom–Cook is slower than long multiplication with small numbers, and it is therefore typically used for intermediate-size multiplications, before the asymptotically faster Schönhage–Strassen algorithm (with complexity Θ(n log n log log n)) becomes practical.
Toom first described this algorithm in 1963, and Cook published an improved (asymptotically equivalent) algorithm in his PhD thesis in 1966.

","[u'Computer arithmetic algorithms', u'Multiplication']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Andrei Toom', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'Chakravala method', u""Cipolla's algorithm"", u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Discrete logarithm', u""Dixon's factorization method"", u'Donald Knuth', u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'Gaussian elimination', u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'Karatsuba algorithm', u'Karatsuba multiplication', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Positional notation', u'Primality test', u""Proth's theorem"", u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'Rational sieve', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Stephen Cook', u'The Art of Computer Programming', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Trial division', u'Wheel factorization', u""Williams' p + 1 algorithm""]"
Top-nodes algorithm,"The top-nodes algorithm is an algorithm for managing a resource reservation calendar.
It is used when a resource is shared among lots of users (for example bandwidth in a telecommunication link, or disk capacity in a large data center).
The algorithm allows
to check if an amount of resource is available during a specific period of time,
to reserve an amount of resource for a specific period of time,
to delete a previous reservation,
to move the calendar forward (the calendar covers a defined duration, and it must be moved forward as time goes by).","[u'All articles covered by WikiProject Wikify', u'All articles with too few wikilinks', u'Articles covered by WikiProject Wikify from December 2012', u'Articles with French-language external links', u'Articles with too few wikilinks from December 2012', u'Calendar algorithms', u'Scheduling algorithms']","[u'Algorithm', u'Bandwidth (signal processing)', u'Binary tree', u'Data center', u'Disk capacity', u'Telecommunication']"
Topological sorting,"In the field of computer science, a topological sort (sometimes abbreviated toposort) or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.","[u'Articles with example pseudocode', u'Directed graphs', u'Graph algorithms', u'Sorting algorithms']","[u'Adaptive sort', u'Adjacency matrix', u'Algorithm', u'American flag sort', u'Analysis of algorithms', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Big O notation', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Charles E. Leiserson', u'Clifford Stein', u'Cocktail sort', u'Coffman\u2013Graham algorithm', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Computer science', u'Counting sort', u'Cycle sort', u'D. E. Knuth', u'Dependency (disambiguation)', u'Dependency graph', u'Dependency resolution', u'Depth-first search', u'Digital object identifier', u'Directed acyclic graph', u'Directed cycle', u'Directed graph', u'Directed path', u'Eric W. Weisstein', u'Feedback arc set', u'Flashsort', u'Gnome sort', u'Hamiltonian path', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Instruction scheduling', u'Integer sorting', u'International Standard Book Number', u'Introduction to Algorithms', u'Introsort', u'JSort', u'Job shop scheduling', u'Layered graph drawing', u'Lexicographic order', u'Library sort', u'Linear extension', u'Linear time', u'Linker (computing)', u'List (computing)', u'Logic synthesis', u'Longest path problem', u'Makefile', u'MathWorld', u'Mathematical Reviews', u'Merge sort', u'Min-plus matrix multiplication', u'NC (complexity)', u'NP-hard', u'Odd\u2013even sort', u'Online algorithm', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Parallel computer', u'Partial order', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Program Evaluation and Review Technique', u'Project management', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Reachability', u'Robert Tarjan', u'Ron Rivest', u'Ronald L. Rivest', u'Selection algorithm', u'Selection sort', u'Serialization', u'Shellsort', u'Shortest path problem', u'Smoothsort', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splaysort', u'Spreadsheet', u'Spreadsort', u'Stephen Cook', u'Stooge sort', u'Strand sort', u'The Art of Computer Programming', u'Thomas H. Cormen', u'Timsort', u'Total order', u'Tournament sort', u'Transitive reduction', u'Transitive relation', u'Tree sort', u'Tsort (Unix)', u'Vertex (graph theory)', u'Weighted graph']"
Tournament selection,"Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm. Tournament selection involves running several ""tournaments"" among a few individuals (or 'chromosomes') chosen at random from the population. The winner of each tournament (the one with the best fitness) is selected for crossover. Selection pressure is easily adjusted by changing the tournament size. If the tournament size is larger, weak individuals have a smaller chance to be selected.
The tournament selection method may be described in pseudo code:

choose k (the tournament size) individuals from the population at random
choose the best individual from pool/tournament with probability p
choose the second best individual with probability p*(1-p)
choose the third best individual with probability p*((1-p)^2)
and so on...

Deterministic tournament selection selects the best individual (when p = 1) in any tournament. A 1-way tournament (k = 1) selection is equivalent to random selection. The chosen individual can be removed from the population that the selection is made from if desired, otherwise individuals can be selected more than once for the next generation. In comparison with the (stochastic) fitness proportionate selection method, tournament selection is often implemented in practice due to its lack of stochastic noise.
Tournament selection has several benefits over alternative selection methods for genetic algorithms (for example, fitness proportionate selection and reward-based selection): it is efficient to code, works on parallel architectures and allows the selection pressure to be easily adjusted. Tournament selection has also been shown to be independent of the scaling of the genetic algorithm fitness function (or 'objective function') in some classifier systems.","[u'Genetic algorithms', u'Pages using citations with accessdate and no URL']","[u'Chromosome (genetic algorithm)', u'Crossover (genetic algorithm)', u'Digital object identifier', u'Fitness function', u'Fitness proportionate selection', u'Genetic algorithm', u'International Standard Book Number', u'Loss function', u'Reward-based selection']"
Traveling salesman problem,"The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.

TSP is a special case of the travelling purchaser problem and the Vehicle routing problem.
In the theory of computational complexity, the decision version of the TSP (where, given a length L, the task is to decide whether the graph has any tour shorter than L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (perhaps, specifically, exponentially) with the number of cities.
The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact methods are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.
The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimise the time spent slewing the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.","[u'Commons category with local link same as on Wikidata', u'Computational problems in graph theory', u'Graph algorithms', u'Hamiltonian paths and cycles', u'NP-complete problems', u'NP-hard problems', u'Operations research', u'Pages containing cite templates with deprecated parameters', u'Travelling salesman problem', u'Use dmy dates from July 2012']","[u'2-opt', u'APX', u'Alexander Schrijver', u'Alpha processor', u""Analyst's traveling salesman theorem"", u'Ant colony optimization', u'Ant colony optimization algorithms', u'Approximation algorithm', u'Artificial intelligence', u'Best, worst and average case', u'Bibcode', u'Bitonic tour', u'Bottleneck traveling salesman problem', u'Bounded convergence theorem', u'Branch-and-bound', u'Branch and bound', u'Branch and cut', u'Brian Kernighan', u'Brute force search', u'Canadian traveller problem', u'Charles E. Leiserson', u'Chemistry', u'Christofides algorithm', u'Christos Papadimitriou', u'Clifford Stein', u'Cognitive psychology', u'Combinatorial optimization', u'Complete graph', u'Complexity class', u'Computational complexity theory', u'Computer science', u'Concorde TSP Solver', u'Constructive heuristic', u'Cross entropy method', u'Cutting-plane method', u'Cutting plane', u'Cutting stock problem', u'D. R. Fulkerson', u'DNA sequencing', u'David S. Johnson', u'Decision problem', u'Delbert Ray Fulkerson', u'Digital object identifier', u'Directed graph', u'Distance function', u'Distance matrix', u'Drill', u'Dynamic programming', u'Edge (graph theory)', u'Electronic Colloquium on Computational Complexity', u'Emergence', u'Euclidean distance', u'Euclidean minimum spanning tree', u'Euclidean space', u'Eugene Lawler', u'Eulerian graph', u'Eulerian tour', u'Evolutionary computing', u'Exponential time hypothesis', u'Factorial', u'Function problem', u'Genetic algorithm', u'Geometric measure theory', u'George Dantzig', u'George Nemhauser', u'Gerhard J. Woeginger', u'Glossary of graph theory', u'Graph (mathematics)', u'Graph theory', u'Graph traversal', u'Greedy algorithm', u'G\xf6del Prize', u'Hamiltonian cycle', u'Hamiltonian path problem', u'Hassler Whitney', u'Heikki Mannila', u'Held\u2013Karp algorithm', u'Heuristic', u'Heuristic (computer science)', u'Heuristic algorithm', u'ILOG', u'Icosian Game', u'Integer linear program', u'Integer programming', u'Integrated circuit', u'International Standard Book Number', u'Introduction to Algorithms', u'JSTOR', u'Jan Karel Lenstra', u'Joseph S. B. Mitchell', u'Journal of the ACM', u'Karl Menger', u'Leonard Adleman', u'Linear programming', u'Lin\u2013Kernighan', u'Lin\u2013Kernighan\u2013Johnson', u'Local minimum', u'Logistics', u'Manhattan distance', u'Marco Dorigo', u'Marek Karpinski', u'Markov chain', u'Matching (graph theory)', u'Mathematical Reviews', u'Mathematics', u'Maximum metric', u'Metric (mathematics)', u'Metric space', u'Michael Held', u'Michael R. Garey', u'Minimum spanning tree', u'Monotone polygon', u'NP-complete', u'NP-hard', u'Nearest neighbour algorithm', u'One-way street', u'Operations Research Letters', u'Operations research', u'Optimization problem', u'P vs. NP', u'Perfect matching', u'Permutation', u'Pheromone', u'Physics', u'Planning', u'Polynomial-time approximation scheme', u'Princeton University', u'Printed circuit board', u'PubMed Identifier', u'RAND Corporation', u'Rectifiable curve', u'Rice University', u'Richard Karp', u'Richard M. Karp', u'River formation dynamics', u'Ronald L. Rivest', u'Route inspection problem', u'Running time', u'Sanjeev Arora', u'Santa Monica', u'Selmer M. Johnson', u'Semiconductor', u'Set TSP problem', u'Seven Bridges of K\xf6nigsberg', u'Shen Lin', u'Shortest path', u'Simulated annealing', u'Swarm intelligence', u'Symposium on Theory of Computing', u'Tabu search', u'Theoretical computer science', u'Thomas H. Cormen', u'Thomas Kirkman', u'Time complexity', u'Traffic collision', u'Traveling purchaser problem', u'Travelling Salesman (2012 film)', u'Triangle inequality', u'Triangular inequality', u'Tube Challenge', u'Undirected graph', u'University of Heidelberg', u'University of Waterloo', u'Va\u0161ek Chv\xe1tal', u'Vehicle routing problem', u'Vertex (graph theory)', u'William J. Cook', u'William Rowan Hamilton']"
Tree sort,"A tree sort is a sort algorithm that builds a binary search tree from the keys to be sorted, and then traverses the tree (in-order) so that the keys come out in sorted order. Its typical use is sorting elements adaptively: after each insertion, the set of elements seen so far is available in sorted order.","[u'All articles lacking sources', u'All articles with unsourced statements', u'Articles lacking sources from September 2014', u'Articles with unsourced statements from September 2014', u'Sorting algorithms']","[u'Adaptive sort', u'American flag sort', u'Array data structure', u'Batcher odd\u2013even mergesort', u'Bead sort', u'Best, worst and average case', u'Big O notation', u'Binary Tree', u'Binary heap', u'Binary search tree', u'Bitonic sorter', u'Block sort', u'Bogosort', u'Bubble sort', u'Bucket sort', u'Burstsort', u'Cartesian tree', u'Cascade merge sort', u'Cocktail sort', u'Comb sort', u'Comparison sort', u'Computational complexity theory', u'Counting sort', u'Cycle sort', u'Flashsort', u'Functional programming', u'Gnome sort', u'Haskell (programming language)', u'Heapsort', u'Hybrid algorithm', u'In-place algorithm', u'Insertion sort', u'Integer sorting', u'Introsort', u'JSort', u'Library sort', u'Linked list', u'List (computing)', u'Merge sort', u'Odd\u2013even sort', u'Oscillating merge sort', u'Pairwise sorting network', u'Pancake sorting', u'Patience sorting', u'Pigeonhole sort', u'Polyphase merge sort', u'Proxmap sort', u'Quicksort', u'Radix sort', u'Selection algorithm', u'Selection sort', u'Self-balancing binary search tree', u'Shellsort', u'Smoothsort', u'Sort algorithm', u'Sorting algorithm', u'Sorting network', u'Spaghetti sort', u'Splay tree', u'Splaysort', u'Spreadsort', u'Stooge sort', u'Strand sort', u'Timsort', u'Topological sorting', u'Total order', u'Tournament sort', u'Tree traversal']"
Tree traversal,"In computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (examining and/or updating) each node in a tree data structure, exactly once, in a systematic way. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree, but they may be generalized to other trees as well.","[u'All articles needing additional references', u'Articles needing additional references from June 2013', u'Articles needing additional references from May 2009', u'Articles with example Haskell code', u'Articles with example Java code', u'Articles with example pseudocode', u'Graph algorithms', u'Iteration in programming', u'Recursion', u'Trees (data structures)']","[u'A* search algorithm', u'Alpha\u2013beta pruning', u'Array data structure', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best-first search', u'Bidirectional search', u'Bijection', u'Binary search tree', u'Binary tree', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Branching factor', u'Breadth-first search', u'British Museum algorithm', u'Call stack', u'Chess', u'Composition (number theory)', u'Computer science', u'Corecursion', u'D*', u'Depth-first search', u'Depth-limited search', u'Diagonal argument (disambiguation)', u'Digital object identifier', u""Dijkstra's algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Floyd\u2013Warshall algorithm', u'Fringe search', u'Functional programming', u'Game tree', u'Go (game)', u'Graph traversal', u'Hill climbing', u'Information Processing Letters', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Lazy evaluation', u'Lexicographic breadth-first search', u'Lexicographic order', u'Linked list', u'List of algorithms', u'List of data structures', u'Monte Carlo method', u'Monte Carlo tree search', u'Ordinal number', u'Parse tree', u'Polish notation', u""Prim's algorithm"", u'Queue (abstract data type)', u'Queue (data structure)', u'Recursion', u'Reverse Polish notation', u'Rosetta Code', u'SMA*', u'Search game', u'Search tree', u'Stack (abstract data type)', u'Threaded binary tree', u'Tree (data structure)']"
Trial division,"Trial division is the most laborious but easiest to understand of the integer factorization algorithms. The essential idea behind trial division tests to see if an integer n, the integer to be factored, can be divided by each number in turn that is less than n. For example, for the integer n = 12, the only numbers that divide it are 1,2,3,4,6,12. Selecting only the largest powers of primes in this list gives that 12 = 3 × 4.","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from March 2014', u'Articles with example Python code', u'Division (mathematics)', u'Integer factorization algorithms']","[u'AKS primality test', u'Adleman\u2013Pomerance\u2013Rumely primality test', u'Algorithm', u'Ancient Egyptian multiplication', u'Baby-step giant-step', u'Baillie\u2013PSW primality test', u'Binary GCD algorithm', u'Carl Pomerance', u'Chakravala method', u""Cipolla's algorithm"", u'Composite number', u'Continued fraction factorization', u""Cornacchia's algorithm"", u'Discrete logarithm', u""Dixon's factorization method"", u'Elliptic curve primality', u'Elliptic curve primality proving', u'Euclidean algorithm', u""Euler's factorization method"", u'Extended Euclidean algorithm', u""Fermat's factorization method"", u'Fermat primality test', u'Function field sieve', u""F\xfcrer's algorithm"", u'General number field sieve', u'Generating primes', u'Greatest common divisor', u'Grid computing', u'Index calculus algorithm', u'Integer factorization', u'Integer square root', u'International Standard Book Number', u'Judges of the International Criminal Court', u'Karatsuba algorithm', u""Lehmer's GCD algorithm"", u'Lenstra elliptic curve factorization', u'Lenstra\u2013Lenstra\u2013Lov\xe1sz lattice basis reduction algorithm', u'Long multiplication', u'Lucas primality test', u'Lucas\u2013Lehmer primality test', u'Lucas\u2013Lehmer\u2013Riesel test', u'Miller\u2013Rabin primality test', u'Modular exponentiation', u'Multiplication algorithm', u'Number theory', u""Pocklington's algorithm"", u'Pocklington primality test', u'Pohlig\u2013Hellman algorithm', u""Pollard's kangaroo algorithm"", u""Pollard's p \u2212 1 algorithm"", u""Pollard's rho algorithm"", u""Pollard's rho algorithm for logarithms"", u'Primality test', u'Primality testing', u'Prime-counting function', u'Prime number', u""Proth's theorem"", u'Public key cryptography', u'Python (programming language)', u""P\xe9pin's test"", u'Quadratic Frobenius test', u'Quadratic residue', u'Quadratic sieve', u'RSA-768', u'Rational sieve', u'Richard Crandall', u""Schoof's algorithm"", u'Sch\xf6nhage\u2013Strassen algorithm', u""Shanks' square forms factorization"", u""Shor's algorithm"", u'Sieve of Atkin', u'Sieve of Eratosthenes', u'Sieve of Sundaram', u'Solovay\u2013Strassen primality test', u'Special number field sieve', u'Springer-Verlag', u'Square number', u'Supercomputer', u'Tonelli\u2013Shanks algorithm', u'Toom\u2013Cook multiplication', u'Undergraduate Texts in Mathematics', u'Wheel factorization', u""Williams' p + 1 algorithm"", u'Worst case', u'Zentralblatt MATH']"
Truncated binary exponential backoff,"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.","[u'Ethernet', u'Networking algorithms', u'Scheduling algorithms', u'Use dmy dates from July 2013', u'Wikipedia articles incorporating text from the Federal Standard 1037C']","[u'Algorithm', u'Carrier sense multiple access with collision avoidance', u'Carrier sense multiple access with collision detection', u'Computer networks', u'Control theory', u'Copyright status of work by the U.S. government', u'Data', u'Data frame', u'Ethernet', u'Expected value', u'Exponential growth', u'Feedback', u'General Services Administration', u'IEEE 802.3', u'Interval (mathematics)', u'Media access control', u'Network congestion avoidance', u'Retransmission (data networks)', u'Slot time', u'Time', u'Triangular number', u'Uniform distribution (discrete)']"
Truncation selection,"Truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination.
In truncation selection the candidate solutions are ordered by fitness, and some proportion, p, (e.g. p = 1/2, 1/3, etc.), of the fittest individuals are selected and reproduced 1/p times. Truncation selection is less sophisticated than many other selection methods, and is not often used in practice. It is used in Muhlenbein's Breeder Genetic Algorithm.","[u'All stub articles', u'Artificial intelligence stubs', u'Bioinformatics stubs', u'Computer science stubs', u'Genetic algorithms']","[u'Artificial intelligence', u'Breeder Genetic Algorithm', u'Computer science', u'Genetic algorithm', u'Selection (genetic algorithm)']"
UPGMA,"UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. It is one of the most popular methods in ecology for the classification of sampling units (such as vegetation plots) on the basis of their pairwise similarities in relevant descriptor variables (such as species composition). In bioinformatics, UPGMA is used for the creation of phenetic trees (phenograms). In a phylogenetic context, UPGMA assumes a constant rate of evolution (molecular clock hypothesis), and is not a well-regarded method for inferring relationships unless this assumption has been tested and justified for the data set being used. UPGMA was initially designed for use in protein electrophoresis studies, but is currently most often used to produce guide trees for more sophisticated phylogenetic reconstruction algorithms.
The UPGMA algorithm constructs a rooted tree (dendrogram) that reflects the structure present in a pairwise similarity matrix (or a dissimilarity matrix).
At each step, the nearest two clusters are combined into a higher-level cluster. The distance between any two clusters A and B is taken to be the average of all distances between pairs of objects ""x"" in A and ""y"" in B, that is, the mean distance between elements of each cluster:

The method is generally attributed to Sokal and Michener. Fionn Murtagh found a time optimal  time algorithm to construct the UPGMA tree.","[u'All stub articles', u'Bioinformatics', u'Bioinformatics algorithms', u'Bioinformatics stubs', u'Computational phylogenetics', u'Data clustering algorithms', u'Phylogenetics']","[u'Autapomorphy', u'Bayesian inference in phylogeny', u'Bioinformatics', u'Clade', u'Cladistics', u'Cluster analysis', u'Complete-linkage clustering', u'Computational phylogenetics', u'DNA barcoding', u'Dendrogram', u'Distance matrices in phylogeny', u'Distance matrix', u'Ecology', u'Evolutionary biology', u'Evolutionary grade', u'Evolutionary taxonomy', u'Ghost lineage', u'Hierarchical clustering', u'Index of evolutionary biology articles', u'Least squares inference in phylogeny', u'List of phylogenetics software', u'Long branch attraction', u'Maximum likelihood', u'Maximum parsimony (phylogenetics)', u'Models of DNA evolution', u'Molecular clock', u'Molecular clock hypothesis', u'Molecular phylogenetics', u'Monophyly', u'Neighbor-joining', u'Paraphyly', u'Phenetic', u'PhyloCode', u'Phylogenetic comparative methods', u'Phylogenetic network', u'Phylogenetic niche conservatism', u'Phylogenetic tree', u'Phylogenetic trees', u'Phylogenetics', u'Phylogenomics', u'Phylogeography', u'Polyphyly', u'Protein electrophoresis', u'Similarity matrix', u'Single-linkage clustering', u'Symplesiomorphy', u'Synapomorphy', u'Three-taxon analysis']"
Ukkonen's algorithm,"In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995.
The algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its ""on-line"" property. The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix. A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix.
The naive implementation for generating a suffix tree going forward requires O(n2) or even O(n3) time complexity in big O notation, where n is the length of the string. By exploiting a number of algorithmic techniques, Ukkonen reduced this to O(n) (linear) time, for constant-size alphabets, and O(n log n) in general, matching the runtime performance of the earlier two algorithms.","[u'Algorithms on strings', u'All stub articles', u'Bioinformatics algorithms', u'Computer science stubs', u'Substring indices']","[u'Big O notation', u'Computer science', u'Digital object identifier', u'Edward M. McCreight', u'Esko Ukkonen', u'Journal of the ACM', u'Online algorithm', u'Peter Weiner', u'Suffix tree', u'Symposium on Switching and Automata Theory']"
Unicode Collation Algorithm,"The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings. These comparisons can then be used to collate or sort text in any writing system and language that can be represented with Unicode.
Unicode Technical Report #10 also specifies the Default Unicode Collation Element Table (DUCET). This datafile specifies the default collation ordering. The DUCET is customizable for different languages. Some such customisations can be found in Common Locale Data Repository (CLDR).
An important open source implementation of UCA is included with the International Components for Unicode, ICU. ICU also supports tailoring and the collation tailorings from CLDR are included in ICU. You can see the effects of tailoring and a large number of language specific tailorings in the on-line ICU Locale Explorer.","[u'Algorithms and data structures stubs', u'All stub articles', u'Collation', u'Computer science stubs', u'Standards and measurement stubs', u'String collation algorithms', u'Unicode algorithms']","[u""'Phags-pa script"", u'Ahom alphabet', u'Algorithm', u'Anatolian hieroglyphs', u'Ancient North Arabian', u'Arabic diacritics', u'Arabic script', u'Aramaic alphabet', u'Armenian alphabet', u'Avestan alphabet', u'Balinese alphabet', u'Bamum script', u'Bassa alphabet', u'Batak alphabet', u'Baybayin', u'Bengali alphabet', u'Bi-directional text', u'Binary Ordered Compression for Unicode', u'Bopomofo', u'Brahmi script', u'Braille', u'Buhid alphabet', u'Burmese alphabet', u'Byte order mark', u'CESU-8', u'CJK Unified Ideographs', u'Canadian Aboriginal syllabics', u'Carian alphabets', u'Caucasian Albanian alphabet', u'Chakma alphabet', u'Cham alphabet', u'Cherokee syllabary', u'Code point', u'Collate', u'Collation', u'Combining Grapheme Joiner', u'Combining character', u'Common Locale Data Repository', u'Comparison of Unicode encodings', u'ConScript Unicode Registry', u'Coptic alphabet', u'Cuneiform', u'Currency symbol', u'Cypriot syllabary', u'Cyrillic script', u'Data structure', u'Deseret alphabet', u'Devanagari', u'Diacritic', u'Duplicate characters in Unicode', u'Duployan shorthand', u'Egyptian hieroglyphs', u'Elbasan', u'Emoji', u'European ordering rules', u'Fraser alphabet', u'GB 18030', u""Ge'ez script"", u'Georgian scripts', u'Glagolitic alphabet', u'Gothic alphabet', u'Grantha alphabet', u'Greek alphabet', u'Gujarati alphabet', u'Gurmukh\u012b alphabet', u'Halfwidth and fullwidth forms', u'Han unification', u'Hangul', u'Hanja', u""Hanun\xf3'o alphabet"", u'Hatran alphabet', u'Hebrew alphabet', u'Hebrew diacritics', u'Hiragana', u'Homoglyph', u'ISO/IEC 8859', u'ISO 14651', u'ISO 15924', u'Ideographic Rapporteur Group', u'International Components for Unicode', u'Internationalized domain name', u'Javanese script', u'Kaithi', u'Kanji', u'Kannada alphabet', u'Katakana', u'Kayah Li alphabet', u'Kharosthi', u'Khmer alphabet', u'Khojki', u'Khudabadi script', u'Language', u'Lao alphabet', u'Latin script in Unicode', u'Left-to-right mark', u'Lepcha alphabet', u'Limbu alphabet', u'Linear A', u'Linear B', u'List of Unicode characters', u'List of XML and HTML character entity references', u'List of precomposed Latin characters in Unicode', u'Lontara alphabet', u'Lycian alphabet', u'Lydian alphabet', u'Mahajani', u'Malayalam script', u'Mandaic alphabet', u'Manichaean alphabet', u'Mathematical operators and symbols in Unicode', u'Measurement', u'Meithei script', u'Mende Kikakui script', u'Meroitic alphabet', u'Modi alphabet', u'Mongolian script', u'Mro people', u'Multani alphabet', u""N'Ko alphabet"", u'Nabataean alphabet', u'New Tai Lue alphabet', u'Numerals in Unicode', u'Numeric character reference', u'Ogham', u'Ol Chiki alphabet', u'Old Hungarian alphabet', u'Old Italic script', u'Old Permic alphabet', u'Old Persian cuneiform', u'Old Turkic alphabet', u'Oriya alphabet', u'Osmanya alphabet', u'Pahawh Hmong', u'Pahlavi scripts', u'Palmyrene alphabet', u'Parthian language', u'Pau Cin Hau', u'Phoenician alphabet', u'Phonetic symbols in Unicode', u'Plane (Unicode)', u'Pollard script', u'Precomposed character', u'Private Use Areas', u'Punctuation', u'Punycode', u'Rejang alphabet', u'Religious and political symbols in Unicode', u'Right-to-left mark', u'Runes', u'Samaritan alphabet', u'Saurashtra alphabet', u'Script (Unicode)', u'Shavian alphabet', u'Siddha\u1e43 script', u'SignWriting', u'Sinhala alphabet', u'Soft hyphen', u'Sorang Sompeng alphabet', u'South Arabian alphabet', u'Space (punctuation)', u'Standard Compression Scheme for Unicode', u'Standardization', u'String (computer science)', u'Sundanese alphabet', u'Sylheti Nagari', u'Syriac alphabet', u'Tagbanwa alphabet', u'Tai Dam language', u'Tai Le alphabet', u'Tai Tham alphabet', u'Takri alphabet', u'Tamil script', u'Telugu script', u'Thaana', u'Thai alphabet', u'Tibetan alphabet', u'Tifinagh', u'Tirhuta', u'UTF-1', u'UTF-16', u'UTF-32', u'UTF-7', u'UTF-8', u'UTF-9 and UTF-18', u'UTF-EBCDIC', u'Ugaritic alphabet', u'Unicode', u'Unicode Consortium', u'Unicode and HTML', u'Unicode and email', u'Unicode anomaly', u'Unicode block', u'Unicode character property', u'Unicode compatibility characters', u'Unicode equivalence', u'Unicode font', u'Unicode input', u'Unicode symbols', u'Universal Character Set characters', u'Universal Coded Character Set', u'Vai syllabary', u'Varang Kshiti', u'Word joiner', u'Writing system', u'Yi script', u'Z-variant', u'Zero-width joiner', u'Zero-width non-joiner', u'Zero-width space', u'\u015a\u0101rad\u0101 script']"
Uniform-cost search,"Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.
The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Dijkstra's original algorithm does not use a min-priority queue and runs in time  (where  is the number of nodes). The idea of this algorithm is also given in (Leyzorek et al. 1957). The implementation based on a min-priority queue implemented by a Fibonacci heap and running in  (where  is the number of edges) is due to (Fredman & Tarjan 1984). This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights.
In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search.","[u'1959 in computer science', u'Articles with example pseudocode', u'Combinatorial optimization', u'Commons category with local link same as on Wikidata', u'Dutch inventions', u'Graph algorithms', u'Routing algorithms', u'Search algorithms', u'Use dmy dates from February 2011']","[u'A* algorithm', u'A* search algorithm', u'A-star algorithm', u'Adjacency list', u'Admissible heuristic', u'Algorithm', u'Alpha\u2013beta pruning', u'Artificial Intelligence: A Modern Approach', u'Artificial intelligence', u'Asymptotic computational complexity', u'B*', u'Backtracking', u'Beam search', u'Bellman\u2013Ford algorithm', u'Best, worst and average case', u'Best-first search', u'Bidirectional search', u'Big O notation', u'Binary heap', u""Bor\u016fvka's algorithm"", u'Branch and bound', u'Breadth-first search', u'British Museum algorithm', u'Brodal queue', u'Charles Babbage Institute', u'Charles E. Leiserson', u'Clifford Stein', u'Computer scientist', u'Consistent heuristic', u'D*', u'Depth-first search', u'Depth-limited search', u'Digital object identifier', u'Directed acyclic graph', u'Directed graph', u'Donald Knuth', u'Dover Publications', u'Dual linear program', u""Dykstra's projection algorithm"", u'Dynamic programming', u""Edmonds' algorithm"", u'Edsger W. Dijkstra', u'Euclidean shortest path', u'Fast marching method', u'Fibonacci heap', u'Flood fill', u'Floyd\u2013Warshall algorithm', u'Francis & Taylor', u'Fringe search', u'Graph (abstract data type)', u'Graph (data structure)', u'Graph labeling', u'Graph traversal', u'Greedy algorithm', u'Hill climbing', u'IEEE', u'IS-IS', u'Information Processing Letters', u'International Standard Book Number', u'Intersection (road)', u'Introduction to Algorithms', u'Iterative deepening A*', u'Iterative deepening depth-first search', u""Johnson's algorithm"", u'Jump point search', u""Kruskal's algorithm"", u'Kurt Mehlhorn', u'Lexicographic breadth-first search', u'Linear programming', u'Link-state routing protocol', u'List of algorithms', u'Longest path problem', u'MIT Press', u'McGraw\u2013Hill', u'Michael Fredman', u'Min-priority queue', u'Minimum spanning tree', u'Motion planning', u'Negative cycle', u'Neighbourhood (graph theory)', u'OSPF', u'PDF', u'Pairing heap', u'Peter Norvig', u'Peter Sanders (computer scientist)', u""Prim's algorithm"", u'Principle of Optimality', u'Priority queue', u'Probability distribution', u'Radix heap', u'Reduced cost', u'Richard Bellman', u'Robert Endre Tarjan', u'Robert Tarjan', u'Robotics', u'Ronald L. Rivest', u'Routing protocol', u'SMA*', u'Search algorithm', u'Search game', u'Self-balancing binary search tree', u'Shortest path problem', u'Shortest path tree', u'Sparse graph', u'Stuart J. Russell', u'Subroutine', u'Thomas H. Cormen', u'Time complexity', u'Transportation Science', u'Tree traversal', u'Vertex (graph theory)', u'Wavefront']"
Uniform binary search,"Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming. It uses a lookup table to update a single array index, rather than taking the midpoint of an upper and a lower bound on each iteration; therefore, it is optimized for architectures (such as Knuth's MIX) on which
a table lookup is generally faster than an addition and a shift, and
many searches will be performed on the same array, or on several arrays of the same length","[u'All orphaned articles', u'Articles with example C code', u'Orphaned articles from February 2009', u'Search algorithms']","[u'Binary search', u'Binary search algorithm', u'C (programming language)', u'Donald Knuth', u'Lookup table', u'MIX', u'Pascal (programming language)', u'The Art of Computer Programming']"
VEGAS algorithm,"The VEGAS algorithm, due to G. P. Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral.
The VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function , so that the points are concentrated in the regions that make the largest contribution to the integral.
In general, if the Monte Carlo integral of  is sampled with points distributed according to a probability distribution described by the function , we obtain an estimate ,
.
The variance of the new estimate is then

where  is the variance of the original estimate, .
If the probability distribution is chosen as  then it can be shown that the variance  vanishes, and the error in the estimate will be zero. In practice it is not possible to sample from the exact distribution g for an arbitrary function, so importance sampling algorithms aim to produce efficient approximations to the desired distribution.
The VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region while histogramming the function f. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like  with dimension d the probability distribution is approximated by a separable function:  so that the number of bins required is only Kd. This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS.","[u'Computational physics', u'Monte Carlo methods', u'Statistical algorithms', u'Variance reduction']","[u'Digital object identifier', u'Histogram', u'Importance sampling', u'Integral', u'Integrand', u'Las Vegas algorithm', u'Monte Carlo integration', u'Monte Carlo simulation', u'Probability distribution', u'Projection (mathematics)', u'Variance', u'Variance reduction']"
Vector clocks,"Vector clocks is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations. Just as in Lamport timestamps, interprocess messages contain the state of the sending process's logical clock. A vector clock of a system of N processes is an array/vector of N logical clocks, one clock per process; a local ""smallest possible values"" copy of the global clock-array is kept in each process, with the following rules for clock updates:

Initially all clocks are zero.
Each time a process experiences an internal event, it increments its own logical clock in the vector by one.
Each time a process prepares to send a message, it sends its entire vector along with the message being sent.
Each time a process receives a message, it increments its own logical clock in the vector by one and updates each element in its vector by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element).
The vector clocks algorithm was independently developed by Colin Fidge and Friedemann Mattern in 1988.",[u'Distributed algorithms'],"[u'Algorithm', u'Antisymmetric relation', u'Array data structure', u'Causality', u'Digital object identifier', u'Distributed system', u'Friedemann Mattern', u'International Standard Book Number', u'Lamport timestamps', u'Logical clock', u'Matrix clock', u'Partial ordering', u'Transitive relation', u'Version vector']"
Vector quantization,"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.
The density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.
Vector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.","[u'Articles to be expanded from February 2009', u'Incomplete lists from August 2008', u'Lossy compression algorithms']","[u'.VQA', u'AMR-WB+', u'Adaptive resonance theory', u'Apple Video', u'Autoencoder', u'Bink video', u'CELP', u'Centroid', u'Centroidal Voronoi tessellation', u'Cinepak', u'Cluster analysis', u'Codebook', u'Competitive learning', u'Coordinate vector', u'DTS Coherent Acoustics', u'Daala', u'Data clustering', u'Data compression', u'Deep Learning', u'Deep learning', u'Density estimation', u'Digital Video Interactive', u'Digital object identifier', u'Dynamic time warping', u'Entropy code', u'Free On-line Dictionary of Computing', u'G.729', u'Hidden Markov model', u'ILBC', u'Image segmentation', u'Indeo', u'Internet Engineering Task Force', u'K-means', u'K-means clustering', u'Learning vector quantization', u'Linde\u2013Buzo\u2013Gray algorithm', u'Linear subspace', u""Lloyd's algorithm"", u'Lossy data compression', u'MPEG', u'MPEG-4', u'Microsoft Video 1', u'Motion compensation', u'Neural gas', u'Ogg Vorbis', u'Opus (codec)', u'Pattern recognition', u'Prefix code', u'Projection (mathematics)', u'Quantization (signal processing)', u'QuickTime', u'QuickTime Graphics Codec', u'Rate-distortion function', u'Self-organizing map', u'Signal processing', u'Simulated annealing', u'Smacker video', u'Sorenson codec', u'Sparse coding', u'Speech coding', u'Transform coding', u'TwinVQ', u'Vector space', u'Voronoi diagram']"
Velvet (algorithm),"Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions. Velvet has also been implemented inside of commercial packages, such as Geneious, MacVector and BioNumerics.","[u'Bioinformatics algorithms', u'Bioinformatics software', u'DNA sequencing', u'Metagenomics software']","[u'BioNumerics', u'Breadth-first search', u'C language', u'Complementarity (molecular biology)', u'Contig', u'DNA sequencing', u'De Bruijn graph', u'Digital object identifier', u""Dijkstra's algorithm"", u'European Bioinformatics Institute', u'Ewan Birney', u'GNU General Public License', u'Genome', u'Genome assembly', u'International Standard Book Number', u'K-mer', u'List of software categories', u'MacVector', u'Mammal', u'N50 statistic', u'Next-generation sequencing', u'Operating system', u'Polymorphism (biology)', u'Prokaryotes', u'PubMed Central', u'PubMed Identifier', u'SPAdes (software)', u'Sequence alignment', u'Software developer', u'Software license', u'Software release life cycle']"
Verhoeff algorithm,"The Verhoeff algorithm is a checksum formula for error detection developed by the Dutch mathematician Jacobus Verhoeff and was first published in 1969. It was the first decimal check digit algorithm which detects all single-digit errors, and all transposition errors involving two adjacent digits, which was at the time thought impossible with such a code.","[u'Checksum algorithms', u'Error detection and correction', u'Modular arithmetic', u'Wikipedia articles needing clarification from April 2014']","[u'CPAN', u'Cayley table', u'Checksum', u'Commutative', u'Damm algorithm', u'Digital object identifier', u'Dihedral group', u'Error detection', u'International Standard Book Number', u'Jacobus Verhoeff', u'Library of Congress Control Number', u'Lookup table', u'Permutation']"
Wang and Landau algorithm,"The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau, is a Monte Carlo method designed to calculate the density of states of a system. The method performs a non-markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.
The Wang–Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. For instance, it has been applied to the solution of numerical integrals and the folding of proteins. The Wang-Landau Sampling is related to the Metadynamics algorithm.","[u'Articles with example Python code', u'Computational physics', u'Markov chain Monte Carlo', u'Statistical algorithms']","[u'ArXiv', u'Bibcode', u'David P. Landau', u'Density of states', u'Digital object identifier', u'Harmonic oscillator', u'Metadynamics', u'Metropolis-Hastings algorithm', u'Metropolis\u2013Hastings algorithm', u'Monte Carlo method', u'Multicanonical ensemble', u'PubMed Central', u'PubMed Identifier', u'Python (programming language)', u'Stochastic process']"
Warnock algorithm,"The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics. It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute. In other words, if the scene is simple enough to compute efficiently then it is rendered; otherwise it is divided into smaller parts which are likewise tested for simplicity.
This is a divide and conquer algorithm with run-time of , where n is the number of polygons and p is the number of pixels in the viewport.
The inputs are a list of polygons and a viewport. The best case is that if the list of polygons is simple, then draw the polygons in the viewport. Simple is defined as one polygon (then the polygon or its part is drawn in appropriate part of a viewport) or a viewport that is one pixel in size (then that pixel gets a color of the polygon closest to the observer). The continuous step is to split the viewport into 4 equally sized quadrants and to recursively call the algorithm for each quadrant, with a polygon list modified such that it only contains polygons that are visible in that quadrant.","[u'All stub articles', u'Computer graphics algorithms', u'Computer programming stubs']","[u'Analysis of algorithms', u'Computer graphics', u'Computer programming', u'Divide and conquer algorithm', u'Hidden surface determination', u'International Standard Book Number', u'John Warnock']"
Winnow algorithm,"The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples. It is very similar to the perceptron algorithm. However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative. The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated.",[u'Classification algorithms'],"[u'Boolean-valued', u'Features (pattern recognition)', u'Hyperplane', u'Linear classifier', u'Machine learning', u'Multi-label classification', u'Online machine learning', u'Perceptron', u'Upper and lower bounds']"
Xiaolin Wu's line algorithm,"Xiaolin Wu's line algorithm is an algorithm for line antialiasing, which was presented in the article An Efficient Antialiasing Technique in the July 1991 issue of Computer Graphics, as well as in the article Fast Antialiasing in the June 1992 issue of Dr. Dobb's Journal.
Bresenham's algorithm draws lines extremely quickly, but it does not perform anti-aliasing. In addition, it cannot handle any cases where the line endpoints do not lie exactly on integer points of the pixel grid. A naive approach to anti-aliasing the line would take an extremely long time. Wu's algorithm is comparatively fast, but is still slower than Bresenham's algorithm. The algorithm consists of drawing pairs of pixels straddling the line, each coloured according to its distance from the line. Pixels at the line ends are handled separately. Lines less than one pixel long are handled as a special case.
An extension to the algorithm for circle drawing was presented by Xiaolin Wu in the book Graphics Gems II. Just like the line drawing algorithm is a replacement for Bresenham's line drawing algorithm, the circle drawing algorithm is a replacement for Bresenham's circle drawing algorithm.","[u'All articles lacking in-text citations', u'Articles lacking in-text citations from January 2013', u'Articles with example pseudocode', u'Computer graphics algorithms']","[u'Algorithm', u""Bresenham's line algorithm"", u'Computer Graphics', u'Computer Graphics (publication)', u'Digital object identifier', u""Dr. Dobb's Journal"", u'Graphics Gems', u'International Standard Book Number', u'Spatial anti-aliasing']"
Xor swap algorithm,"In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. ""Distinct"" means that the variables are stored at different memory addresses; the actual values of the variables do not have to be different.","[u'Algorithms', u'All articles needing additional references', u'Articles needing additional references from February 2012', u'Articles with example C code', u'Binary arithmetic']","[u'Abelian group', u'Algorithm', u'Aliasing (computing)', u'Amiga CD32', u'Associativity', u'Bignum', u'Binary operation', u'Bitwise operation', u'Block matrices', u'CPU architecture', u'C (programming language)', u'Call by name', u'Commutative operation', u'Computer programming', u'Cyclic group', u'Data type', u'Elementary matrix', u'Exclusive disjunction', u'Feistel cipher', u'Identity element', u'Instruction-level parallelism', u'Instruction pipeline', u'Integer overflow', u'Inverse element', u""Jensen's Device"", u'Machine code', u'Microcontrollers', u'Modular arithmetic', u'Nibble', u'Processor register', u'Register allocator', u'Register pressure', u'Shear mapping', u'Swap (computer science)', u'Symmetric difference', u'System/370', u'Variable (programming)', u'XOR linked list']"
Yamartino method,"The Yamartino method (introduced by Robert J. Yamartino in 1984) is an algorithm for calculating an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data. The standard deviation of wind direction is a measure of lateral turbulence, and is used in a method for estimating the Pasquill stability category.
The simple method for calculating standard deviation requires two passes through the list of values. The first pass determines the average of those values; the second pass determines the sum of the squares of the differences between the values and the average. This double-pass method requires access to all values. A single-pass method can be used for normal data but is unsuitable for angular data such as wind direction where the 0°/360° (or +180°/-180°) discontinuity forces special consideration. For example, the directions 1°, 0°, and 359° (or -1°) should not average to the direction 120°!
The Yamartino method solves both problems. The United States Environmental Protection Agency (EPA) has chosen it as the preferred way to compute the standard deviation of wind direction. A further discussion of the Yamartino method, along with other methods of estimating the standard deviation of wind direction can be found in Farrugia & Micallef.
It should be mentioned that it is also possible to calculate the exact standard deviation in one pass. However, that method needs slightly more calculation effort.","[u'Atmospheric dispersion modeling', u'Boundary layer meteorology', u'Directional statistics', u'Statistical algorithms']","[u'Air pollution dispersion terminology', u'Algorithms for calculating variance', u'Arcsine', u'Bibcode', u'Digital object identifier', u'Directional statistics', u'Polar coordinate system', u'Right angle', u'Standard deviation', u'Turbulence', u'United States Environmental Protection Agency', u'Wind direction']"
Zeller's congruence,Zeller's congruence is an algorithm devised by Christian Zeller to calculate the day of the week for any Julian or Gregorian calendar date. It can be considered to be based on the conversion between Julian day and the calendar date.,"[u'CS1 German-language sources (de)', u'CS1 Latin-language sources (la)', u'Calendar algorithms', u'Gregorian calendar', u'Julian calendar', u'Modular arithmetic']","[u'Algorithm', u'Calculate the day of the week', u'Christian Zeller', u'Determination of the day of the week', u'Dictionary of Algorithms and Data Structures', u'Digital object identifier', u'Doomsday rule', u'Gregorian calendar', u'ISO week date', u'Julian calendar', u'Julian day', u'Modulo operation', u'National Institute of Standards and Technology', u'Proleptic calendar', u'Zero-based']"
Zhu–Takaoka string matching algorithm,"In computer science, the Zhu–Takaoka string matching algorithm is a variant of the Boyer–Moore string search algorithm. It uses two consecutive text characters to compute the bad character shift. It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase.",[u'String matching algorithms'],"[u'Boyer\u2013Moore string search algorithm', u'Computer science', u'Dictionary of Algorithms and Data Structures', u'National Institute of Standards and Technology', u'Pre-processing', u'Skip table']"
Ziggurat algorithm,"The ziggurat algorithm is an algorithm for pseudo-random number sampling. Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables. The algorithm is used to generate values from a monotone decreasing probability distribution. It can also be applied to symmetric unimodal distributions, such as the normal distribution, by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from. It was developed by George Marsaglia and others in the 1960s.
A typical value produced by the algorithm only requires the generation of one random floating-point value and one random table index, followed by one table lookup, one multiply operation and one comparison. Sometimes (2.5% of the time, in the case of a normal or exponential distribution when using typical table sizes) more computations are required. Nevertheless, the algorithm is computationally much faster than the two most commonly used methods of generating normally distributed random numbers, the Marsaglia polar method and the Box–Muller transform, which require at least one logarithm and one square root calculation for each pair of generated values. However, since the ziggurat algorithm is more complex to implement it is best used when large quantities of random numbers are required.
The term ziggurat algorithm dates from Marsaglia's paper with Wai Wan Tsang in 2000; it is so named because it is conceptually based on covering the probability distribution with rectangular segments stacked in decreasing order of size, resulting in a figure that resembles a ziggurat.","[u'All articles with unsourced statements', u'Articles with unsourced statements from September 2011', u'Non-uniform random numbers', u'Pages using citations with format and no URL', u'Pseudorandom number generators', u'Statistical algorithms']","[u'Algorithm', u'ArXiv', u'Bisection method', u'Box\u2013Muller transform', u'Digital object identifier', u'Error function', u'Gaussian distribution', u'George Marsaglia', u'IEEE 754', u'Inline function', u'International Standard Serial Number', u'MATLAB', u'Marsaglia polar method', u'Monotonic function', u'Normal distribution', u'Normalizing constant', u'Numerical integration', u'Probability distribution', u'Pseudo-random number generator', u'Pseudo-random number sampling', u'Recursion', u'Rejection sampling', u'Root-finding algorithm', u'Round-off error', u'Sanity test', u'Symmetric function', u'The Journal of Business', u'Unimodal distribution', u'Ziggurat']"
